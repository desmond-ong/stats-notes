# The Linear Model II: Logistic Regression

<span class="badge badge-bt"> BT1101 </span>

In the previous chapter, we introduced the linear model, and showed how we can use it to model continuous dependent variables ($Y$), using a combination of both continuous and categorical indepdent variables ($X$).

In this chapter we will discuss expanding our toolkit to use a different type of regression, **logistic** regression, to model categorical dependent variables ($Y$). For now, we **only consider binary dependent variables** (such as `Yes`/`No` Decisions, `True`/`False` classifications, etc), although there are also extensions to categorical dependent variable with multiple levels (e.g., multinomial regression).



The learning objectives for this chapter are:

- Readers should understand and be able to use logistic regression to estimate categorical dependent variables (e.g., to perform classification).



```{r lmii-setup, echo=T, warning=F}
# Load the libraries we'll use in this chapter
library(ggplot2) # for plotting
```


## Basics of Logistic Regression

<span class="badge badge-bt"> BT1101 </span>

Let's imagine that we have a dataset with individual yes/no decisions, that we want to predict. For example, we might have a dataset of individual consumer purchases on an e-commerce platform, where we want to predict a consumer’s decision to purchase a product based upon other variables such as how much they spend on the platform, what their demographics are^[Another common example: we might want to predict people's voting behaviour or willingness to support certain policies, based upon certain characteristics.].

Here, our dependent variable is whether or not the customer purchased the product, so just "Yes" or "No", and we can write the model with this Purchased variable on the left hand side. Can we build a linear model to predict purchasing behaviour?

$$\text{Purchased} \sim \text{Spending} + \text{Demographics} + \ldots$$





Unfortunately, here we cannot use linear regression, because our response variable is a binary (yes/no) variable, while the linear regression model assumes a variable with normally distributed errors. But we can use the `generalized linear model`, which, as its name suggests, generalizes the linear model to other types of variables. The idea is that the GLM introduces a _link_ function that maps the actual response variable `Y` to what the linear model predicts.

Here we'll focus on `logistic regression`, which uses the `logit` function as its link function. 

Let $p$ be the probability that Purchase = 1 (i.e., "Yes")^[In general, $p$ is the probability that the dependent variable takes on a particular value of interest (e.g., "success"). R treats binary outcome variables as factors, where the first level (e.g. `0`, `FALSE`, `No`) is the base, comparison group and the second level (e.g., `1`, `TRUE`, `Yes`) is the "success" group. As with categorical independent variables, you can use `levels(df$var)` to check which level R will use as the base group by default.]. Then the logistic regression equation becomes:


$$\text{logit}(p) = b_0 + b_1 X_1 + \ldots$$

Thus, instead of predicting a continuous outcome variable $Y$ directly, we instead predict the log-odds of an event occurring:

$$ \text{logit}(p) \equiv \log \frac{p}{1-p}$$

This term is called the log-odds, where $\frac{p}{1-p}$ is called the "odds" or "odds-ratio". To be clear, the logit is defined using the natural (base-$e$) logarithm, not the base-10 logarithm.


The difference between the equation for linear regression (in the previous chapter) and logistic regression is summarized in the following table:

Name | Equation
--- | ---
Linear Regression | $$Y = b_0 + b_1 X_1 + \ldots$$
Logistic Regression | $$\text{logit}(p) = b_0 + b_1 X_1 + \ldots$$

In fact, linear regression is a special case of the general linear model with the identity function as the link function.



## Running a Logistic Regression

<span class="badge badge-bt"> BT1101 </span>

The syntax for running a logistic regression is almost the same as a linear regression, just that the call is `glm()` for **g**eneral **l**inear **m**odel, with an additional specification of `family = binomial`, which tells `glm` to run logistic regression. (Other `family` options produce other types of general linear regression, such as probit regression, etc.)

Now, the good news is that R handles a lot of this complication for you, when it can. For example, we **do not** have to manually calculate the odds ourselves. All we have to do is make sure our variable is a binary factor, then we can just call `glm()`. 


> Note that, just like categorical IVs, when we do logistic regression with a categorical DV, we also have a **reference group**, so do use the `levels()` function to check.

Let's generate a simple dataset with two independent variables $X_1$ and $X_2$, and use them to predict $\text{Purchase}$, a binary yes/no variable.

```{r lmii-generate-logistic, echo=T, eval=T}
# logistic
set.seed(1)
df2 = data.frame(x1=rnorm(20,0,5) + seq(20,1),
                 x2=rnorm(20,5,3),
                 Purchase = factor(rep(c("Yes", "No"), each=10)),
                                   levels=c("No", "Yes"))
levels(df2$Purchase) 
## This means that "No" is the `base group`, and `p` is the probability of "Yes".
```



```{r lmii-run-logistic, echo=T, eval=T}
# next, running a logistic regression via a general linear model
fit_log1 <- glm(Purchase ~ x1 + x2, family="binomial", df2)
summary(fit_log1)
```



The summary output looks almost the same too as a `lm()` call. Let’s focus on the coefficient table, and reproduce the regression equation to help us in the interpretation.



$$\text{logit}(p) \equiv \log\frac{p}{1-p} = b_0 + b_1 X_1 + b_2X_2$$

Just like in linear regression, $b_0$ is the mean value of the left-hand-side when all the $X$ on the right hand side are zero. Hence, $b_0$ is the log-odds of the event occurring when $X_1$ and $X_2$ are both zero (this part is the same as what we’ve covered previously). So this means that when $X_1$ and $X_2$ are both zero, the log-odds of purchasing an item is `r round(summary(fit_log1)$coeff["(Intercept)", "Estimate"], 3)`. Conversely, we can also say that the odds of purchasing this item is `exp`(`r round(summary(fit_log1)$coeff["(Intercept)", "Estimate"], 3)`), or `r round(exp(summary(fit_log1)$coeff["(Intercept)", "Estimate"]), 3)`. 
This means that $\frac{p}{1-p}$ is `r round(exp(summary(fit_log1)$coeff["(Intercept)", "Estimate"]), 3)`.

Next, let's move onto $b_1$. $b_1$ is the expected increase in log-odds per unit increase of $X_1$, holding $X_2$ constant. This is the same as linear regression. 

And similarly, $b_2$ is the expected increase in log-odds per unit increase of $X_2$, holding $X_1$ constant. Note that $b_2$ is negative, so increasing $X_2$ will decrease the log-odds. (But in this case, it's not significant anyway)

Now, let's take some numbers to build intuition. Every unit-increase of $X_1$ increases the log-odds by 0.3954. Conversely, every unit-increase of $X_1$ multiplies the odds by exp(0.3954) = 1.48. i.e., the odds increase by 48%

- Check: When $X_1$ and $X_2$ are 0, the odds are `exp`(`r round(summary(fit_log1)$coeff["(Intercept)", "Estimate"], 3)`) = `r round(exp(summary(fit_log1)$coeff["(Intercept)", "Estimate"]), 4)`. 
- If we now increase $X_1$ to 1, the odds are now `exp`(`r round(summary(fit_log1)$coeff["(Intercept)", "Estimate"], 3)` + `r round(summary(fit_log1)$coeff["x1", "Estimate"], 3)`), or `r round(exp(summary(fit_log1)$coeff["(Intercept)", "Estimate"] + summary(fit_log1)$coeff["x1", "Estimate"]), 4)`.
- The odds have increased by (0.0271-0.0182)/0.0182 ~ 48\% (if we kept more decimal places)

For example, if $X_1$ is "Number of A-list celebrities endorsing your product", then getting one additional celebrity endorsement would, in expectation, increase each customer’s odds of purchasing your product by 48\%. (not increasing probability, but odds)




This table summarizes the interpretations:

Coefficient | Interpretation
--- | ---
($b_0$) | Log-odds when $X_1$ and $X_2$ are both zero. Odds of purchasing = exp(-4.0054) = 0.018
($b_1$) | Expected increase in log-odds of event per unit-increase of $X_1$, holding $X_2$ constant.
($b_2$) | Expected increase in log-odds of event per unit-increase of $X_2$, holding $X_1$ constant.






The rest of the coefficient table is the similar to the `lm()`. However, instead of these coefficients following a $t$ distribution, they follow a $z$ distribution. The interpretation of the standard error, $z$ values, and $p$ values are similar. Thus, in this table, the coefficient $X_1$ is statistically significant at the $\alpha=0.05$ confidence level ($p<.05$)



## [Not Done:] Exercises: Linear Model II

<span class="badge badge-bt"> BT1101 </span>



