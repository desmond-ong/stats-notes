[
["index.html", "Some notes on Statistics and Data Analytics Preface", " Some notes on Statistics and Data Analytics Desmond Ong Draft Dated: 2019-12-16 Preface This is a collection of notes on statistics and data analytics that I am compiling, primarily to supplement a course that I teach at the National University of Singapore (BT1101: Introduction to Business Analytics), which is a statistics course in R targeted at first-year undergraduate students and aspiring data scientists / researchers. Some material will go beyond this level, though, and should be useful to graduate-level researchers as well. I hope to cover quite broadly several important and useful statistical concepts (e.g. regression, simulations), as well as discuss issues like data visualization best practices. If I have time, I would like to transition to teaching statistics in a more Bayesian tradition. As some background, I am a computational cognitive psychologist, with a little bit of training in econometrics, so I tend to favor regression and simulation approaches, and my examples may default to examples common in the social sciences. This is a work in progress that is inspired by Russ Poldrack’s Psych10 book here: http://statsthinking21.org/. This set of notes is hosted on GitHub and built using Bookdown. Feedback can be sent to dco (at) comp.nus.edu.sg. "],
["outline-of-notes.html", "Outline of notes", " Outline of notes [In Progress] Getting Started - R, RStudio, R Markdown. Coding Best Practices. Analysis Best Practices. [In Progress] Introduction - detective analogy [In Progress] Descriptive Statistics - Mean / Median / Mode, Types of Variables, Variance / Covariance / Correlation [In Progress] Probability ? - Normal distribution? [In Progress] Handling Data [In Progress] Data Visualization The Linear Model Linear Regression Logistic Regression [In Progress] Interactions [In Progress] Model Selection [In Progress] Mixed-effects linear models [In Progress] Data Mining [In Progress] Simulations Monte Carlo Simulations The Bootstrap Optimization Linear Optimization [In Progress] Integer-valued Optimization … "],
["not-done-getting-started.html", "[Not Done:] Getting Started", " [Not Done:] Getting Started How to use R How to use R Markdown Coding Best Practices "],
["intro.html", "Chapter 1 [Not Done:] Introduction", " Chapter 1 [Not Done:] Introduction Introduction Models are always wrong. "],
["not-done-descriptive-statistics.html", "Chapter 2 [Not Done:] Descriptive Statistics", " Chapter 2 [Not Done:] Descriptive Statistics Descriptive Statistics "],
["not-done-handling-data.html", "Chapter 3 [Not Done:] Handling Data", " Chapter 3 [Not Done:] Handling Data Wide Form, Long Form, when to choose each… "],
["not-done-data-visualization.html", "Chapter 4 [Not Done:] Data Visualization", " Chapter 4 [Not Done:] Data Visualization Different types of visualizations, when to choose each when bar, when line ggplot jittering "],
["the-linear-model-i-linear-regression.html", "Chapter 5 The Linear Model I: Linear Regression", " Chapter 5 The Linear Model I: Linear Regression In the next few Chapters, we will learning about a workhorse tool of analytics: the linear model, which allows us to run linear regression models that we can use to estimate simple trends and look at how some variables in our data may affect other variables. The linear model is a key tool used in many fields, from psychology, economics, linguistics, business, as well as in some physical sciences like ecology. Thus, it is an essential part of the data scientist’s toolkit. In this Chapter, we will be going over linear regression with one or more independent variables, to predict a continuous dependent variable. We will also discuss how to interpret the output of a simple regression model, and discuss the case of categorical independent variables. The learning objectives for this chapter are: Readers should be able to understand and be able to use both simple and multiple regression to estimate simple trends. Readers should be able to interpret the output of a regression model, including regression coefficients, confidence intervals, hypothesis testing about coefficients, and goodness-of-fit statistics. Readers should be able to interpret the meaning of dummy-coded variables used to model categorical independent variables. # Load the libraries we&#39;ll use in this chapter library(ggplot2) # for plotting I’ve also made up some simulated data just for the next few sections that we will use to illustrate linear regression. Below is the code I used to generate df1: \\(X\\) is just a vector from 0 to 10, and \\(Y\\) is an affine transformation of \\(X\\) with some random noise added. set.seed(1) df1 = data.frame(X=c(seq(0,10), NA)) df1$Y = -2 + df1$X*2 + rnorm(n=nrow(df1), mean=0, sd=1) "],
["basics-of-linear-regression.html", "5.1 Basics of Linear Regression", " 5.1 Basics of Linear Regression Linear Regression really boils down to this simple equation. \\[ Y = b_0 + b_1 X \\] We want to predict \\(Y\\), our outcome variable or dependent variable (DV), using \\(X\\), which can have several names, like independent variable (IV), predictor or regressor. \\(b_0\\) and \\(b_1\\) are parameters that we have to estimate in the model: \\(b_0\\) is the Constant or Intercept term, and \\(b_1\\) is generally called a regression coefficient, or the slope. Below, we have a simple graphical illustration. Let’s say I have a dataset of \\(X\\) and \\(Y\\) values, and I plot a scatterplot of \\(Y\\) on the vertical axis and \\(X\\) on the horizontal. It’s convention that the dependent variable is always on the vertical axis. On the right I’ve also drawn the “best-fit” line to the data. Graphically, \\(b_0\\) is where the line crosses the vertical axis (i.e., when \\(X\\) is 0), and \\(b_1\\) is the slope of the line. Some readers may have learnt this in high school, where the slope of the line is the “rise” over the “run”, so how many units of “Y” do we increase (‘rise’) as we increase “X”. And that’s the basic idea. Our linear model is one way of trying to explain \\(Y\\) using \\(X\\), which is by multiplying \\(X\\) by the regression coefficient \\(b_1\\) and adding a constant \\(b_0\\). It’s so simple, yet, it is a very powerful and widely used tool, and we shall see more over the rest of this chapter and the next few chapters. Our dependent variable \\(Y\\) is always the quantity that we are interested in and that we are trying to predict. Our independent variables \\(X\\) are variables that we use to predict \\(Y\\). Here are some examples: Dependent Variable Y Independent Variable X Income Years of Education Quartly Revenue Expenditure on Marketing Quantity of Products Sold Month of Year Click-through rate Colour of advertisement (Note that this is an experiment!) Perhaps we are interested in predicting an individual’s income based on their years of education. Or for a company, we might want to predict Quarterly Revenue using the amount we spent on Marketing, or predict sales at in different months of the year. We can also use regression to model the results of experiments. For example, we might be interested in whether changing the colour of an advertisement will affect how effective it is, measured by how many people click on the advertisement (called the click-through rate, measuring how many people click on our ad). So we may show some of our participants the ad in blue, some in green, etc, and then see how that affects our click-through rate. So linear modelling can and is frequently used to model experiments as well. Independent Variables can be continuous (e.g., Years; Expenditure), or they can also be discrete/categorical (e.g., Month, Colour, Ethnicity). And we shall see examples of both in this chapter Similarly, Dependent Variables can either be continuous or categorical. In this Chapter we shall only cover continuous DVs, and we shall learn about categorical DVs in the next Chapter on Logistic Regression. "],
["running-a-regression.html", "5.2 Running a regression", " 5.2 Running a regression The first step to running a regression is to be clear about what is your dependent variable of interest, and what are your independent variables. Often, this is clear from the context: As a researcher we have an objective to model or predict a certain variable — that will be the dependent variable, \\(Y\\). And we have variables that we think would predict that, and those will be our \\(X\\)’s. (Later we’ll discuss the differences between predictors, covariates, and confounders, which could all statistically affect the depenent variable.) The second step is to structure your data. For most Linear Regression (at least for the examples in this Chapter), we almost always want wide-form data, discussed in the earlier Chapter on data handling, where you have each row of the data frame be one observation, and you have one column for \\(Y\\) and one column for \\(X\\). (In later Chapters we shall see when we may need long-form data for other types of regression.) For example, the mtcars dataset that comes with R is an example of wide-form data. Each row is one observation (i.e., one car), and each column is an attribute/variable associated with that observation (e.g., fuel economy, number of cylinders, horsepower, etc). head(mtcars,3) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 The third step is to visualize your data, discussed also in the earlier Chapter on Data Visualization. For regression analyses, visualization of your data allows you to see whether there may be linear trends or non-linear trends (or no trends). Linear models assume that there exists a linear trend between \\(Y\\) and \\(X\\). If you have a non-linear trend, like the quadratic and exponential ones shown here, you may want to think about transforming some of your variables to see if you can get a linear trend before running the linear model. If you visually no trend, like the last plot above, you can confirm this lack of trend just by running a linear model. Plotting can also help with troubleshooting. For example, you’ll be able to immediately see if you accidentally have a factor instead of a numeric variable, or if you have possible outliers (like the graph on the left) or possibly missing data (like the graph on the right). Let’s plot the two variables in our toy dataset df1: Looks very linear! So we should expect to see a strong linear relationship between df$X and df$Y. Finally, we’re ready to run the model. And in fact, it’s one line of code. lm for linear model, and then you provide an “equation”, y~x, which is R syntax for “Y depends upon X”. The final argument is the dataframe in which the data is stored. # running a Linear Model fit1 &lt;- lm(Y~X, df1) TIP! Best analysis practice: If you store all your Y and X variables in your dataframe as wide form data, you can just write lm(y~x, df1), which is very neat syntax. It’s not as clean to write lm(df1$y~df1$x), and I discourage this. After fitting the model, we can just call summary() on our lm object, in order to view the output of the model. # examining the output summary(fit1) ## ## Call: ## lm(formula = Y ~ X, data = df1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0781 -0.5736 0.1260 0.3071 1.5452 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.26117 0.46171 -4.897 0.000851 *** ## X 2.10376 0.07804 26.956 6.44e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8185 on 9 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.9878, Adjusted R-squared: 0.9864 ## F-statistic: 726.6 on 1 and 9 DF, p-value: 6.442e-10 There’s a lot of information here, which we will break down in the next few sections, after a short digression to discuss how (Ordinary Least Squares) regression is solved. "],
["ordinary-least-squares-regression.html", "5.3 Ordinary Least Squares Regression", " 5.3 Ordinary Least Squares Regression In this section we’ll be taking a quick peek behind what the model is doing, and we’ll discuss the formulation of Ordinary Least Squares regression. In linear regression, we assume that the “true” model is such that \\(Y\\) is \\(b_0\\) + \\(b_1\\)*\\(X\\) plus some “errors”, which could be due to other, unmeasured factors (omitted variables), or maybe just random noise. \\[ ``\\text{True&quot; model: } Y = b_0 + b_1 X + \\epsilon \\] In fact, within regression, we make the further assumption that the errors are normally distributed around zero with some variance. (So \\(\\epsilon \\sim \\mathscr{N}(0, \\sigma)\\)). Since we don’t know the “true” \\(b_0\\) and \\(b_1\\), we can only choose \\(\\hat{b_0}\\) and \\(\\hat{b_1}\\); using this we can compute the prediction of our model, \\(\\hat{Y}\\). We want our \\(\\hat{b_0}\\) and \\(\\hat{b_1}\\) to be as close to the “true” \\(b_0\\) and \\(b_1\\), which will also make our predictions \\(\\hat{Y}\\) as close to the actual \\(Y\\). To do this, we define the Residual or Residual Error of our model. For the \\(i\\)-th data point, the residual is the difference between the actual \\(Y_i\\) and our model predicted, \\(\\hat{Y_i}\\). \\[ \\text{Residual Error: } e_i = Y_i - \\hat{Y_i} \\] Here’s an illustration. Let’s say I start off just by drawing a green line through the origin with some upward slope. Here, the red lines illustrate the residual error; the difference between the actual value and our prediction. And to make our model better, we want to minimise the red bars. Some red bars are lower, some are higher, so let’s pivot the slope upwards. Now, we have this yellow line. It looks better, overall the bars are smaller. Now we note that all the red bars are below, so instead of pivoting, let’s move the whole line down. And finally we get the blue line here, which is the best solution to minimising the red bars. We want to minimise the residuals. How is this done? 5.3.1 Ordinary Least Squares Derivation The residuals can be positive or negative, so if we simply add the residuals up we might be cancelling out some of them. So instead of minimising the sum of the residuals, we usually choose to square the residuals and minimise the sum of squares of the residuals. (Mathematically it becomes easier to work with the square than the absolute value). So here, we have the Ordinary Least Squares Regression, where the goal is to choose \\(\\hat{b_0}\\) and \\(\\hat{b_1}\\) to minimise the sum of squares of the residuals \\(\\sum_{i} e_i^2 = \\sum_i \\left( Y_i - \\hat{Y_i} \\right)^2\\). We can do this by taking the partial derivative with respect to \\(\\hat{b_0}\\) and \\(\\hat{b_1}\\), and setting them both to 0. First, we define the following variables to simplify notation: \\[\\begin{align} \\text{Define } \\bar{Y} &amp;\\equiv \\frac{1}{n}\\sum_i^n Y_i \\\\ \\text{Define } \\bar{X} &amp;\\equiv \\frac{1}{n}\\sum_i^n X_i \\\\ \\text{Define } Z &amp;\\equiv \\sum_i \\left( Y_i - \\hat{Y_i} \\right)^2 \\\\ &amp;= \\left( Y_i - \\hat{b_0} - \\hat{b_1} X \\right)^2 \\\\ \\end{align}\\] Then we take the partial derivative with respect to \\(\\hat{b_0}\\), solve for this \\(\\hat{b_0}\\), then substitute it into the partial deriative with respect to \\(\\hat{b_1}\\): \\[\\begin{align} \\text{Partial deriative w.r.t. } \\hat{b_0} : \\; \\; \\frac{\\partial Z}{\\partial \\hat{b_0}} &amp;= \\sum_i^n -2 \\left(Y_i - \\hat{b_0} - \\hat{b_1}X_i \\right) \\\\ \\text{Setting the derivative to 0 and solving, we have: } \\; \\; \\hat{b_0} &amp;= \\frac{1}{n}\\sum_i^n Y_i - \\frac{1}{n}\\sum_i^n\\hat{b_1}X_i \\\\ \\implies \\hat{b_0} &amp;= \\bar{Y} - \\hat{b_1} \\bar{X} \\\\ \\text{Partial deriative w.r.t. } \\hat{b_1} : \\; \\;\\frac{\\partial Z}{\\partial \\hat{b_1}} &amp;= \\sum_i^n -2X_i \\left( Y_i - \\hat{b_0} - \\hat{b_1}X_i \\right) \\end{align}\\] \\[\\begin{align} \\text{Setting the derivative to 0 and substituting $\\hat{b_1}$, we have: } &amp; \\\\ \\sum_i^n X_i Y_i - \\sum_i^n (\\bar{Y}-\\hat{b_1}\\bar{X})X_i - \\sum_i^n\\hat{b_1}X_i^2 &amp;= 0 \\\\ \\sum_i^n X_i Y_i - \\bar{Y} \\sum_i^n X_i + \\hat{b_1} \\left(\\bar{X} \\sum_i^n X_i - \\sum_i^n X_i^2 \\right) &amp;= 0 \\\\ \\hat{b_1} &amp;= \\frac{\\sum_i^n X_i Y_i - \\bar{Y}\\sum_i^n X_i }{ \\sum_i^n X_i^2 - \\bar{X} \\sum_i^n X_i } \\\\ &amp;= \\frac{\\sum_i^n X_i Y_i - n\\bar{X}\\bar{Y}}{ \\sum_i^n X_i^2 - n\\bar{X}^2} \\\\ \\text{simplifying: } \\; \\; \\hat{b_1} &amp;= \\frac{\\sum_i^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{ \\sum_i^n (X_i - \\bar{X})^2 } \\end{align}\\] And we end up with the final OLS solution: \\[\\begin{align} \\hat{b_0} &amp;= \\bar{Y} - \\hat{b_1} \\bar{X} \\\\ \\hat{b_1} &amp;= \\frac{\\sum_i^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{ \\sum_i^n (X_i - \\bar{X})^2 } = \\frac{Cov(X,Y)}{Var(X)} \\end{align}\\] The good news is that \\(R\\) already does this for you. Let’s check this solution with the lm() model we fit on the previous page! Let’s do \\(\\hat{b_1}\\) first: in R, we can calculate the covariance of \\(X\\) and \\(Y\\), and divide that by the variance of \\(X\\), and save that into b1-hat: for this dataset we get 2.10. b1hat = cov(df1$X, df1$Y) / var(df1$X) b1hat ## [1] NA Following the equation for \\(\\hat{b_0}\\), we can take the mean of \\(Y\\), and subtract \\(\\hat{b_1}\\) times the mean of \\(X\\), and we get -2.26. b0hat = mean(df1$Y) - b1hat * mean(df1$X) b0hat ## [1] NA Finally let’s go back to our regression output table, which we can summon using summary(...)$coeff. fit1 &lt;- lm(Y~X, df1) # verifying the OLS solution summary(fit1)$coeff ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.261167 0.46170984 -4.897376 8.50721e-04 ## X 2.103757 0.07804321 26.956313 6.44250e-10 We can see that the Estimate of the Intercept, i.e., \\(\\hat{b_0}\\), is -2.26, and the Estimate of the Coefficient on \\(X\\), i.e., \\(\\hat{b_1}\\), is 2.10. They agree exactly! Excellent. So our lm() is really doing OLS regression. Again, since \\(R\\) does all the calculations for you, it’s not necessary to know how to derive the OLS solutions (especially with more than one independent variable \\(X\\)), but it is handy to know the intuition behind it, especially when we get to more complicated regression. "],
["interpreting-the-output-of-a-regression-model.html", "5.4 Interpreting the output of a regression model 5.4.2 Multiple R-squared: 0.9878", " 5.4 Interpreting the output of a regression model In this section we’ll be going over the different parts of the linear model output. First, we’ll talk about the coefficient table, then we’ll talk about goodness-of-fit statistics. Let’s re-run the same model from before: fit1 &lt;- lm(Y~X, df1) summary(fit1) ## ## Call: ## lm(formula = Y ~ X, data = df1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0781 -0.5736 0.1260 0.3071 1.5452 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.26117 0.46171 -4.897 0.000851 *** ## X 2.10376 0.07804 26.956 6.44e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8185 on 9 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.9878, Adjusted R-squared: 0.9864 ## F-statistic: 726.6 on 1 and 9 DF, p-value: 6.442e-10 First, summary() helpfully reiterates the formula that you put in. This is useful to check that it’s running what you thought it ran. Call: lm(formula = Y ~ X, data = df1) It also tells you the minimum, 1st quantile (25%-ile), median, 3rd quantile (75%-ile), and maximum of the residuals (\\(e_i = Y_i - \\hat{Y_i}\\)). That is, the minimum residual error of this model is -1.0781, the median residual error is 0.1260, and the maximum is 1.5452. Residuals: Min 1Q Median 3Q Max -1.0781 -0.5736 0.1260 0.3071 1.5452 5.4.1 The coefficient table Let’s turn next to the coefficient table. summary(fit1)$coeff ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.261167 0.46170984 -4.897376 8.50721e-04 ## X 2.103757 0.07804321 26.956313 6.44250e-10 Let’s focus on the “Estimate” column. These are the point estimate of \\(b_0\\) and \\(b_1\\), for the equation \\[Y= b_0 + b_1 X\\] What do these numbers mean? \\(b_0\\): The mean value of \\(Y\\) when \\(X\\) is zero The meaning of the intercept, \\(b_0\\), is pretty straightforward. It is the average value of the dependent variable \\(Y\\) when the independent variable \\(X\\) is set to 0. (Graphically, it is the vertical intercept: the point at which the line crosses the vertical axis.) \\(b_1\\): According to the model, a one-unit change in \\(X\\) results in a \\(b_1\\)-unit change in \\(Y\\) The coefficient on \\(X\\), \\(b_1\\), captures the magnitude of change in \\(Y\\), per unit-change in \\(X\\). Graphically, this is the slope of the regression line; if \\(b_1\\) is larger, the line will have a steeper slope. Conversely, if \\(b_1\\) is smaller in magnitude, the line will have a more shallow slope. If \\(b_1\\) is positive, the slope will slope upwards /, otherwise if \\(b_1\\) is negative, the slope will go downwards \\. Example: Interpreting Simple Regression Coefficients Let’s go through an example. Let’s say we fit a model to predict our monthly profit given the amount that we spent on advertising. Both Profit and Expenditure are measured in $. \\[\\text{Profit} = -2500 + 3.21* \\text{ExpenditureOnAdvertising}\\] Coefficient Interpretation (\\(b_0\\)) Monthly profit is -$2500 without any money spent on advertising. (\\(b_1\\)) For every dollar spent on Advertising, Profit increases by $3.21 Q: Why could profit be negative here? Negative (or otherwise unusual) intercepts arise all the time in linear regression. In this example, this just means that, if we spent $0 on advertising, we would still incur a negative profit of $2,500, which could be due to omitted variables such as the amount we have to spent on rent, wages, and other upkeep. Note that it is very important to be aware of the units that each of the variables, both \\(Y\\) and \\(X\\), are measured in. This will ensure accurate interpretation of the coefficients! The rest of the coefficient table The estimated value of \\(b_0\\) and \\(b_1\\) are given in the first column (Estimate) of the coefficient table. Next to the estimates, we have the standard error of \\(b_0\\) and \\(b_1\\), which gives us a sense of the error associated with our estimate. In the third column, we have the t value. This is the t-statistic for a one-sample t-test comparing this coefficient to zero. That is, it is the one-sample t-test for the null hypothesis that the coefficient is zero, against the alternative, two-sided hypothesis that it is not zero: \\[ H_0: b_j = 0 \\\\ H_1: b_j \\neq 0 \\] In fact, the t value here, is simply the Estimate divided by the Standard Error. (You can check it yourself!) So with this t value, and the degrees of freedom of the model, we can actually calculate the p value for such a t test. R helpfully does this for you, and this is given in the fourth column, Pr(&gt;|t|). We can see that these numbers in this example are quite small, so both \\(b_0\\) and \\(b_1\\) are statistically different from zero. To the right of the Pr(&gt;|t|) column, R will helpfully print out certain significance codes. If \\(p\\) is between 0.1 and 0.05, R will print a .. If \\(p\\) is less than 0.05 (\\(\\alpha\\)=5% level of significance) but greater than 0.01 (1%), R will print a single *. If \\(p\\) is less than 0.01 but greater than 0.001, R will print out two asteriks, **. Finally, if \\(p\\) is less than 0.001, R will print out three asterisks, ***. 5.4.2 Goodness-of-fit statistics Finally we’ll look at the last part of the summary output. ## Residual standard error: 0.8185 on 9 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.9878, Adjusted R-squared: 0.9864 ## F-statistic: 726.6 on 1 and 9 DF, p-value: 6.442e-10 First, note that R will helpfully print out whether or not there were observations missing in our data. (1 observation deleted due to missingness) If, for any data point, either the \\(X\\) value, or the \\(Y\\) value (or both) are missing, then R will remove that observation from the linear model, and report it in the output. This is always something useful to check: do we have an abnormally large number of missing observations that we not expect? For example, perhaps one of the variables has a large number of missing observations? Or maybe when we were calculating new variables, we did not consider certain situations, and so end up with a lot of missing variables. (Or maybe we made a typo in our code!). This is always a good safety check before proceeding further. (Note that if there are no missing observations, R will omit this line). Next, we’ll discuss a very important statistic, called the coefficient of determination, or \\(R^2\\) (“R-squared”), which is a measure of the proportion of variance explained by the model. \\(R^2\\) is a number that always lies between 0 and 1. An \\(R^2\\) of 1 means it’s a perfect model, it explains all of the variance (all the data points lie on the line. Alternatively, all the residuals are 0). The total amount of variability of the data is captured in something called the Total Sum of Squares, which is the sum of the difference between each data point \\(Y_i\\) and the mean \\(\\bar{Y}\\) (this is also related to the variance of \\(Y\\)): \\[\\begin{align} \\text{Total Sum of Squares} \\equiv \\sum_i \\left(Y_i - \\bar{Y} \\right)^2 \\end{align}\\] The amount of variability that is explained by our model (which predicts \\(\\hat{Y}\\)) is given by the Regression Sum of Squares, which is the sum of the squared error between our model predictions and the mean \\(\\bar{Y}\\): \\[\\begin{align} \\text{Regression Sum of Squares} \\equiv \\sum_i \\left(\\hat{Y_i} - \\bar{Y} \\right)^2 \\end{align}\\] And finally, the leftover amount of variability, also called the Residual Sum of Squares, is basically the difference between our model predictions \\(\\hat{Y}\\) and the actual data points \\(Y\\). This was the term that Ordinary Least Squares regression tries to minimize, which we saw in the last Section. \\[\\begin{align} \\text{Residual Sum of Squares} \\equiv \\sum_i \\left(Y_i - \\hat{Y_i} \\right)^2 \\end{align}\\] As it turns out, the Total Sum of Squares is made up of these two parts: the Regression Sum of Squares (or “Explained” Sum of Squares), and the Residual Sum of Squares (or the “Unexplained” Sum of Squares). \\[\\begin{align} \\text{Total Sum of Squares} \\equiv \\text{Regression Sum of Squares} + \\text{Residual Sum of Squares} \\end{align}\\] \\(R^2\\) basically measures the proportion of explained variance over the total variance. In other words: \\[\\begin{align} R^2 &amp;\\equiv \\frac{\\text{Regression Sum of Squares}}{\\text{Total Sum of Squares}} \\\\ &amp;\\equiv 1 - \\frac{\\text{Residual Sum of Squares}}{\\text{Total Sum of Squares}} \\end{align}\\] In the output above, the \\(R^2\\), in 5.4.2 Multiple R-squared: 0.9878 is 0.9878; this means that this model explains 98.8% of the variance. That’s really high! Now, how good is a good \\(R^2\\)? Unfortunately there’s no good answer, because it really depends on contexts. In some fields and in some contexts, even an \\(R^2\\) of .10 to .20 could be really good. In other fields, maybe we would expect \\(R^2\\)s of .80 or .90! "],
["regression-example1.html", "5.5 Examples: Simple Regression", " 5.5 Examples: Simple Regression Here’s a simple example to illustrate what we’ve discussed so far, by using a dataset that comes bundled with R, the mtcars dataset. We can load the dataset using data(mtcars). First, let’s see what the data looks like, using head(mtcars), which prints out the first 6 rows of mtcars. data(mtcars) head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 We’re interested in particular in predicting the fuel economy (mtcars$mpg, measured in miles per gallon) of certain vehicles using its horsepower (mtcars$hp). Let’s plot what they look like on a scatterplot. ggplot(mtcars, aes(x=hp, y=mpg)) + geom_point() + theme_bw() It looks like there is a linear trend! We can see visually that as horsepower increases, fuel economy drops; large cars tend to guzzle more fuel than smaller ones. And finally, we can run the linear model using using lm(mpg ~ hp, mtcars): summary(lm(mpg ~ hp, mtcars)) ## ## Call: ## lm(formula = mpg ~ hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7121 -2.1122 -0.8854 1.5819 8.2360 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.09886 1.63392 18.421 &lt; 2e-16 *** ## hp -0.06823 0.01012 -6.742 1.79e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.863 on 30 degrees of freedom ## Multiple R-squared: 0.6024, Adjusted R-squared: 0.5892 ## F-statistic: 45.46 on 1 and 30 DF, p-value: 1.788e-07 Looking at the coefficient table, we can see that \\(b_0\\), the intercept term, suggests suggests that, according to our model, a car with zero horsepower (if such a vehicle exists) would have a mean fuel economy of 30 mpg. We can look at the coefficient on \\(X\\), which is \\(b_1 = -0.068\\). Now this means that every unit-increase in horsepower would be associated with a decrease in fuel efficiency by -0.068 miles per gallon. Now, these are quite strange units to think about: for one, we do not think about horsepower in 1’s and 2’s; we think of horsepower in the tens or hundreds, and typical cars have horsepowers around 120 (the median in our dataset is 123). Secondly, -0.068 miles per gallon sounds pretty little, given that we see in the data that the mean fuel consumption is about 20 mpg. What we can do to help our interpretation is to multiply our coefficient to get more human-understandable values. This means that, according to the model, an increase of horsepower by 100 units, will be associated with a decrease in fuel efficiency by 6.8 miles per gallon. Ahh these numbers make more sense now. Finally, we note that the \\(R^2\\) of this model is 0.6024. This means that the model explains about 60% of the variance in the data. Again, it’s hard to objectively say whether this is amazing or still needs more work, because this really depends on our desired context. One takeaway from this example is that the linear model just calculates the coefficients based upon the numbers that we, as the analyst, provide it. It is up to us to then read the numbers that are output from the model, and make sense of those numbers, and interpret the meaning of those numbers. So this means being aware of units, and also not hesitating to wonder, “hmm this isn’t right, did I code everything correctly?” "],
["not-done-assumptions-behind-linear-regression.html", "5.6 [Not Done:] Assumptions behind Linear Regression", " 5.6 [Not Done:] Assumptions behind Linear Regression "],
["multiple-linear-regression.html", "5.7 Multiple Linear Regression", " 5.7 Multiple Linear Regression We’ve covered Simple Linear Regression, whereby “Simple” means just one independent variable. Next we’ll talk about Multiple Linear Regression, where “Multiple” just means multiple independent variables. \\[Y = b_0 + b_1X_1 + b_2X_2 + b_3X_3 + \\ldots\\] This is actually quite a straightforward extension, once we get the hang of the interpretation. We can simply extend our linear model to also include \\(X_2\\), \\(X_3\\) and so forth, and now our \\(b_1\\) is called the coefficient on \\(X_1\\) or the partial coefficient on \\(X_1\\). Here’s the most difficult part. When we interpret each partial coefficient, the value is interpreted holding all the other IVs constant. So \\(b_1\\) represents the expected change in Y when \\(X_1\\) increases by one unit, holding constant all the other variables. This is so important that I’ll say it twice more. If you really understand this point, then you know how to do multiple regression. The partial regression coefficients represent the expected change in the dependent variable when the associated independent variable is increased by one unit while the values of all other independent variables are held constant. And a third time, in different words: Each coefficient \\(b_i\\) estimates the mean change in the dependent variable (\\(Y\\)) per unit increase in \\(X_i\\), when all other predictors are held constant. Here’s an example \\[\\text{Profit} = -2000 + 2.5* \\text{ExpenditureOnAdvertising} +32*\\text{NumberOfProductsSold}\\] Coefficient Intepretation: (b0) Monthly profit is -$2000 without any money spent on advertising and with zero products sold. (b1) Holding the number of products sold constant, every dollar spent on advertising increases profit by $2.50 (b2) Keeping advertising expenditure constant, every product sold increases profit by $32 "],
["standardized-coefficients.html", "5.8 Standardized Coefficients", " 5.8 Standardized Coefficients Let’s take a short digression to discuss standardised coefficients. In all the examples in this Chapter, we’ve seen that it’s very important to be clear about what the units of measurement are, as this affects how we interpret the numbers. For example, in \\(\\text{Income} = b_0 + b_1 \\text{YearsOfEducation}\\), we can say for a 1 unit increase in \\(X_1\\), so for one additional year of education, there is a \\(b_1\\) unit increase in \\(Y\\); so there is a $\\(b_1\\) increase in income. Unfortunately, sometimes we may have difficulty comparing \\(X\\)’s. Perhaps I have a dataset of American students and their standardized test scores on the SATs, and a corresponding dataset of Singaporean students and their standardized test scores on the O-levels1. I want to compare the datasets to predict how test scores (\\(X\\)) affect income (\\(Y\\)), but my \\(X\\)s here are on different scales. How should we compare them? One way is by using standardised coefficients2. To do so, we “standardize” each variable by subtracting its mean and dividing by its standard deviation. Then we just re-run the regression. Now, notice that I’ve replaced \\(b\\)’s with \\(\\beta\\)s, and now these \\(\\beta\\)s are unit-less. Or, to put it another way, they are in “standardized” units. By convention (although not everyone follows this), \\(b\\) are used to refer to unstandardized regression coefficients while \\(\\beta\\)s are used to refer to standardized regression coefficients. \\[\\left[ \\frac{Y-\\bar{Y}}{\\sigma_{Y}} \\right] = \\beta_0 + \\beta_1 \\left[ \\frac{X_1 - \\bar{X_1}}{\\sigma_{X_1}} \\right] + \\beta_2 \\left[ \\frac{X_2 - \\bar{X_2}}{\\sigma_{X_2}} \\right] + \\ldots\\] Note: We can choose to standardise only the IVs, or only some of the IVs. The usual convention is that all the IVs and sometimes the DV are standardised. Now, the interpretation is: When \\(X_i\\) increases by one standard deviation, there is a change in \\(Y\\) of \\(\\beta_i\\) standard deviations Example: if we run: \\[\\text{Income} = \\beta_0 + \\beta_1 \\text{Education} + \\beta_2 \\text{Working Experience}\\] as a standardized regression, and we find that \\(\\beta_1\\) = 0.5, then the interpretation is: “For every increase in education level by one standard deviation, holding working experience constant, there is an average increase in income by 0.5 standard deviations.” With standardised coefficients, the interpretation changes, everything is now in standard deviations. Sometimes, standardised coefficients makes it easier to interpret when the underlying unit is quite difficult to interpret, for example: IQ, the intelligence quotient, is actually a standardised variable itself such that 100 is the mean of the population and 15 is the standard deviation. It’s hard to come up with units of “absolute” intelligence, and so IQ is actually measured relative to others in the population. Coming back to the example of comparing the results in an American sample with the SAT and a Singaporean sample with the O-levels; even if I cannot compare 1 point on the SAT with 1 point on the O-levels, with standardized coefficients I can still ask: does a 1 standard deviation increase in SAT score have the same effect (on whatever \\(Y\\)), as a 1 standard deviation increase in O-level score? Here is some R code to ‘manually’ standardize variables and use them in a model # unstandardized lm(Y ~ X, df1) # standardized df1$X_standardized = scale(df1$X, center=TRUE, scale=TRUE) df1$Y_standardized = scale(df1$Y, center=TRUE, scale=TRUE) lm(Y_standardized ~ X_standardized, df1) ignore the fact that the SAT and O-levels are at different levels↩ Confusingly, when we use standardised coefficients, it is the variables that get standardised, not the coefficients.↩ "],
["categorical-independent-variables.html", "5.9 Categorical Independent Variables", " 5.9 Categorical Independent Variables So far we have been dealing with continuous independent variables (\\(X\\)), (e.g. Expenditure, Years, Age, Numbers, …). In this section, we consider categorical independent variables (e.g., Gender, Ethnicity, MaritalStatus, Color-Of-Search-Button, …). Let’s consider an example modelling how Umbrella Sales depend upon Weather. \\[\\text{UmbrellaSales} = b_0 + b_1 \\text{Weather}\\] These categorical variables take on one of a small set of fixed values. Let’s assume in this simple world that weather is only Sunny or Rainy. 5.9.1 Dummy Coding Dummy Coding (the default method in R) is a method by which we create and use dummy variables in our regression model3. In this example, we can define a variable: Rainy that is 1 if Weather==Rainy, and 0 if Weather==Sunny. Rainy is called a dummy variable (sometimes called an indicator variable) We can replace Weather with the dummy variable Rainy: \\[\\text{UmbrellaSales} = b_0 + b_1 \\text{Weather} \\; \\rightarrow \\; \\color{brown}{\\text{UmbrellaSales} = b_0 + b_1 \\text{Rainy}}\\] Thus, this breaks down into two equations (technically, a piecewise equation): If Sunny, \\(\\text{UmbrellaSales} = b_0 + b_1(0) = b_0\\) If Rainy, \\(\\text{UmbrellaSales} = b_0 + b_1(1) = b_0 + b_1\\) Now we can interpret the value of these coefficients. Looking at the first equation, we can see that \\(b_0\\) is simply the average umbrella sales when it is sunny. And similarly, from the second equation, we see that \\(b_0\\) PLUS \\(b_1\\) is the average umbrella sales when it is rainy. This means that \\(b_1\\) is the difference between these equations: It is the average difference in umbrella sales when it is rainy, compared to when it is sunny. The following table summarizes these interpretations: Coefficient Intepretation: (\\(b_0\\)) Average Umbrella sales when it is Sunny (\\(b_0+b_1\\)) Average Umbrella sales when it is Rainy (\\(b_1\\)) Average difference in Umbrella sales when it is Rainy, compared to when it is Sunny (Sales when Rainy - Sales when Sunny) Q: Would you predict $b_1$ to be greater than 0 or less than 0? 5.9.2 Dummy Coding with 3 levels Let’s now consider a more complicated world in which Weather can take one of three values: Sunny, Rainy, or Cloudy. Then we can define two dummy variables, Rainy and Cloudy, Rainy = 1 if weather is rainy, 0 otherwise Cloudy = 1 if weather is cloudy, 0 otherwise We say that Sunny is the Reference Group for the categorical variable Weather. \\[\\text{UmbrellaSales} = b_0 + b_1 \\text{Rainy} + b_2\\text{Cloudy}\\] This breaks down into three equations: If Sunny, \\(\\text{UmbrellaSales} = b_0 + b_1(0) + b_2(0) = b_0\\) If Rainy, \\(\\text{UmbrellaSales} = b_0 + b_1(1) + b_2(0) = b_0 + b_1\\) If Cloudy, \\(\\text{UmbrellaSales} = b_0 + b_1(0) + b_2(1) = b_0 + b_2\\) Just like above, we can interpret the meaning of the coefficients in the following table: Coefficient Intepretation: (\\(b_0\\)) Average Umbrella sales when it is Sunny (\\(b_0+b_1\\)) Average Umbrella sales when it is Rainy (\\(b_1\\)) Average difference in Umbrella sales when it is Rainy, compared to when it is Sunny (Sales when Rainy - Sales when Sunny) (\\(b_0+b_2\\)) Average Umbrella sales when it is Cloudy (\\(b_2\\)) Average difference in Umbrella sales when it is Cloudy, compared to when it is Sunny (Sales when Cloudy - Sales when Sunny) Thus, in general, a categorical variable with \\(n\\) levels will have \\((n-1)\\) dummy variables. And the general interpretation of these \\(i\\) dummy variables are: Coefficient Intepretation: (\\(b_0\\)) Average Value of Y for the reference group. (\\(b_i\\)) Average Difference in Y for Dummy Group i compared to the reference group. 5.9.3 The Reference Group Now, when we do dummy coding, one of the groups will automatically be the reference group. The choice of reference group is not fixed. And choosing your reference group well (depending on your goals) will make your analyses more convenient and interpretable. For example, for the Umbrella Sales example (Sunny, Rainy, Cloudy), I think “Sunny” is a good reference group. Why? Or let’s say I want to see how well people react to the color of the button on my webpage. So I run an experiment with the following four buttons4: Current Button Button A Button B Button C Which should I choose to be my reference group? I think that “Current Button” should be the reference group, since that is the status quo and I am interesting in how changing the buttons would affect click-through, relative to my current button. The good news: R handles dummy coding for you using factors. You do not need to create your own dummy variables. Just run: lm(sales ~ weather, df) and if weather is a factor with n levels, R will default to creating n-1 dummy variables The bad news: R does not know your hypotheses, so it uses a heuristic for choosing the reference group. If you do not specify, R defaults to ranking the groups by alphabetical order. Thus, in the weather example, it would choose &quot;Cloudy&quot;, and in the button example, &quot;Button A&quot; as the reference group. If your variable (df$var) is a factor, you can check by using levels(df$var). The first level will be the reference group. Use relevel(df$var, &quot;desiredReferenceLevel&quot;) to adjust the reference group. (if df$var is a character string, levels() will return NULL, but if you put it into a lm(), R will treat it as categorical variable, with the alphabetically smallest string as the reference group) 5.9.4 Interpreting categorical and continuous independent variables Whenever we have categorical independent variables in a model, interpreting the coefficients has to be done with respect to the reference group, even for other continuous independent variables. Let’s add a continuous variable to our 3-weather model. UmbrellaSales is a continuous variable measured in $. Rainy and Cloudy are dummy variables (“Sunny” is the reference group) ExpenditureOnAdvertising is also a continuous variable measured in $. Let’s say we fit the following model and obtain the following coefficients: \\[\\text{UmbrellaSales} = 10 + 50 \\text{Rainy} + 20 \\text{Cloudy} + 2.5\\text{ExpenditureOnAdvertising}\\] Here’s how we interpret each of these coefficients: Coefficient Intepretation: 10 Average umbrella sales when it is sunny and $0 spent on advertising. 50 Average umbrella sales when it is rainy compared to sunny and $0 spent on advertising. 20 Average umbrella sales when it is cloudy compared to sunny and $0 spent on advertising. 2.5 When it is sunny, every dollar spent on advertising increases sales by $2.50 Aside from dummy coding (R’s default), there are other coding schemes which can be used to test more specific hypothesis, e.g. effect coding, difference coding, etc. But here we shall focus on Dummy Coding.↩ This may seem frivolous, but Google actually ran a lot of these tests back in the day, with different shades of blue/green/red to settle on their current “Google colors”.↩ "],
["not-done-exercises-linear-model-i.html", "5.10 [Not Done:] Exercises: Linear Model I", " 5.10 [Not Done:] Exercises: Linear Model I 1a) For this simple linear regression, \\[ \\text{Income} = b_0 + b_1 \\text{Years of Education} \\] We find \\(b_0\\)=-4500, \\(b_1\\)=500 (in units of $/year). What does this mean? [Negative income?!] 1b) For this multiple linear regression, \\[\\text{Income} = b_0 + b_1 \\text{Years of Education} + b_2 \\text{Years of Experience}\\] We find \\(b_0\\)=-3700, \\(b_1\\)=450, \\(b_2\\)=350 (in $/year). What do these mean? 1c) For this simple linear regression, \\[\\text{Income} = b_0 + b_1 \\text{Gender}\\] Let’s say Gender = 1 if Female, 0 if Male, and we find that \\(b_0\\)=3500, \\(b_1\\)=5 (in $). What does this mean? "],
["todo-discuss-transformations.html", "5.11 [todo:] Discuss Transformations?", " 5.11 [todo:] Discuss Transformations? "],
["the-linear-model-ii-logistic-regression.html", "Chapter 6 The Linear Model II: Logistic Regression", " Chapter 6 The Linear Model II: Logistic Regression In the previous chapter, we introduced the general linear model, and showed how we can use it to model continuous dependent variables (\\(Y\\)), using a combination of both continuous and categorical indepdent variables (\\(X\\)). In this chapter we will discuss expanding our toolkit to use a different type of regression, logistic regression, to model categorical dependent variables (\\(Y\\)). For now, we only consider binary dependent variables (such as Yes/No Decisions), although there are also extensions to categorical dependent variable with multiple levels (e.g., multinomial regression). The learning objectives for this chapter are: Readers should understand and be able to use logistic regression to estimate categorical dependent variables (e.g., to perform classification). # Load the libraries we&#39;ll use in this chapter library(ggplot2) # for plotting "],
["basics-of-logistic-regression.html", "6.1 Basics of Logistic Regression", " 6.1 Basics of Logistic Regression Let’s imagine that we have a dataset with individual yes/no decisions, that we want to predict. For example, we might have a dataset of individual consumer purchases on an e-commerce platform, where we want to predict a consumer’s decision to purchase a product based upon other variables such as how much they spend on the platform, what their demographics are5. Here, our dependent variable is whether or not the customer purchased the product, so just “Yes” or “No”, and we can write the model with this Purchased variable on the left hand side. Can we build a linear model to predict purchasing behaviour? \\[\\text{Purchased} \\sim \\text{Spending} + \\text{Demographics} + \\ldots\\] Unfortunately, here we cannot use linear regression, but we can use the more general version of the model, called the general linear model. The general idea is that the GLM introduces a link function that maps the actual response variable Y to what the linear model predicts. Here we’ll focus on logistic regression, which uses the logit function as its link function. Let \\(p\\) be the probability that Purchase = 1 (i.e., “Yes”)6. Then the logistic regression equation becomes: \\[\\text{logit}(p) = b_0 + b_1 X_1 + \\ldots\\] Thus, instead of predicting a continuous outcome variable \\(Y\\) directly, we instead predict the log-odds of an event occurring: \\[ \\text{logit}(p) \\equiv \\log \\frac{p}{1-p}\\] This term is called the log-odds, where \\(\\frac{p}{1-p}\\) is called the “odds” or “odds-ratio”. To be clear, the logit is defined using the natural (base-\\(e\\)) logarithm, not the base-10 logarithm. The difference between the equation for linear regression (in the previous chapter) and logistic regression is summarized in the following table: Name Equation Linear Regression \\[Y = b_0 + b_1 X_1 + \\ldots\\] Logistic Regression \\[\\text{logit}(p) = b_0 + b_1 X_1 + \\ldots\\] In fact, linear regression is a special case of the general linear model with the identity function as the link function. Another common example: we might want to predict people’s voting behaviour or willingness to support certain policies, based upon certain characteristics.↩ In general, \\(p\\) is the probability that the dependent variable takes on a particular value of interest (e.g., “success”). R treats binary outcome variables as factors, where the first level (e.g. 0, FALSE, No) is the base, comparison group and the second level (e.g., 1, TRUE, Yes) is the “success” group. As with categorical independent variables, you can use levels(df$var) to check which level R will use as the base group by default.↩ "],
["running-a-logistic-regression.html", "6.2 Running a logistic regression", " 6.2 Running a logistic regression The syntax for running a logistic regression is almost the same as a linear regression, just that the call is glm() for general linear model, with an additional specification of family = binomial, which tells glm to run logistic regression. (Other family options produce other types of general linear regression, such as probit regression, etc.) Now, the good news is that R handles a lot of this complication for you, when it can. For example, we do not have to manually calculate the odds ourselves. All we have to do is make sure our variable is a binary factor, then we can just call glm(). Note that, just like categorical IVs, when we do logistic regression with a categorical DV, we also have a reference group, so do use the levels() function to check. Let’s generate a simple dataset with two independent variables \\(X_1\\) and \\(X_2\\), and use them to predict \\(\\text{Purchase}\\), a binary yes/no variable. # logistic set.seed(1) df2 = data.frame(x1=rnorm(20,0,5) + seq(20,1), x2=rnorm(20,5,3), Purchase = rep(c(&quot;Yes&quot;, &quot;No&quot;), each=10)) levels(df2$Purchase) ## [1] &quot;No&quot; &quot;Yes&quot; ## This means that &quot;No&quot; is the `base group`, and `p` is the probability of &quot;Yes&quot;. # running a logistic regression via a general linear model fit_log1 &lt;- glm(Purchase ~ x1 + x2, family=&quot;binomial&quot;, df2) summary(fit_log1) ## ## Call: ## glm(formula = Purchase ~ x1 + x2, family = &quot;binomial&quot;, data = df2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.96587 -0.37136 0.00399 0.54011 1.64780 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.0054 2.4320 -1.647 0.0996 . ## x1 0.3954 0.1653 2.393 0.0167 * ## x2 -0.1281 0.3062 -0.418 0.6758 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 27.726 on 19 degrees of freedom ## Residual deviance: 14.597 on 17 degrees of freedom ## AIC: 20.597 ## ## Number of Fisher Scoring iterations: 6 The summary output looks almost the same too. Let’s focus on the coefficient table, and reproduce the equation to help us interpret. \\[\\text{logit}(p) \\equiv \\log\\frac{p}{1-p} = b_0 + b_1 X_1 + b_2X_2\\] Just like in linear regression, \\(b_0\\) is the mean value of the left-hand-side when all the \\(X\\) on the right hand side are zero. Hence, \\(b_0\\) is the log-odds of the event occurring when \\(X_1\\) and \\(X_2\\) are both zero (this part is the same as what we’ve covered previously). So this means that when \\(X_1\\) and \\(X_2\\) are both zero, the log-odds of purchasing an item is -4.005. Conversely, we can also say that the odds of purchasing this item is exp(-4.005), or 0.018. This means that \\(\\frac{p}{1-p}\\) is 0.018. Next, let’s move onto \\(b_1\\). \\(b_1\\) is the expected increase in log-odds per unit increase of \\(X_1\\), holding \\(X_2\\) constant. This is the same as linear regression. And similarly, \\(b_2\\) is the expected increase in log-odds per unit increase of \\(X_2\\), holding \\(X_1\\) constant. Note that \\(b_2\\) is negative, so increasing \\(X_2\\) will decrease the log-odds. (But in this case, it’s not significant anyway) Now, let’s take some numbers to build intuition. Every unit-increase of \\(X_1\\) increases the log-odds by 0.3954. Conversely, every unit-increase of \\(X_1\\) multiplies the odds by exp(0.3954) = 1.48. i.e., the odds increase by 48% Check: When \\(X_1\\) and \\(X_2\\) are 0, the odds are exp(-4.005) = 0.0182. If we now increase \\(X_1\\) to 1, the odds are now exp(-4.005 + 0.395), or 0.0271. The odds have increased by (0.0271-0.0182)/0.0182 ~ 48% (if we kept more decimal places) For example, if \\(X_1\\) is “Number of A-list celebrities endorsing your product”, then getting one additional celebrity endorsement would, in expectation, increase each customer’s odds of purchasing your product by 48%. (not increasing probability, but odds) This table summarizes the interpretations: Coefficient Interpretation (\\(b_0\\)) Log-odds when \\(X_1\\) and \\(X_2\\) are both zero. Odds of purchasing = exp(-4.0054) = 0.018 (\\(b_1\\)) Expected increase in log-odds of event per unit-increase of \\(X_1\\), holding \\(X_2\\) constant. (\\(b_2\\)) Expected increase in log-odds of event per unit-increase of \\(X_2\\), holding \\(X_1\\) constant. The rest of the coefficient table is the similar to the lm(). However, instead of these coefficients following a \\(t\\) distribution, they follow a \\(z\\) distribution. The interpretation of the standard error, \\(z\\) values, and \\(p\\) values are similar. Thus, in this table, the coefficient \\(X_1\\) is statistically significant at the \\(\\alpha=0.05\\) confidence level (\\(p&lt;.05\\)) "],
["not-done-exercises-linear-model-ii.html", "6.3 [Not Done:] Exercises: Linear Model II", " 6.3 [Not Done:] Exercises: Linear Model II "],
["not-done-the-linear-model-iii-interactions.html", "Chapter 7 [Not Done:] The Linear Model III: Interactions", " Chapter 7 [Not Done:] The Linear Model III: Interactions Cover interpretation of interactions. Concept of moderation simple effect vs main effect The learning objectives for this chapter are: Readers should be able to interpret basic interactions in a regression model, including different slopes, simple effects vs. main effects. # Load the libraries we&#39;ll use in this chapter library(ggplot2) # for plotting "],
["not-done-the-linear-model-iv-model-selection.html", "Chapter 8 [Not Done:] The Linear Model IV: Model Selection", " Chapter 8 [Not Done:] The Linear Model IV: Model Selection The learning objectives for this chapter are: Readers should be able to discuss best practices for model building and selection, and identify potential practical issues with building regression models, such as multicollinearity. # Load the libraries we&#39;ll use in this chapter library(ggplot2) # for plotting Pairwise model comparisons "],
["not-done-the-linear-model-v-mixed-effects-linear-models.html", "Chapter 9 [Not Done:] The Linear Model V: Mixed Effects Linear Models", " Chapter 9 [Not Done:] The Linear Model V: Mixed Effects Linear Models Mixed-effects linear modelling / Multilevel modelling lmer # Load the libraries we&#39;ll use in this chapter library(ggplot2) # for plotting library(lme4) ## Loading required package: Matrix Deciding which is a fixed effect and which is a random effect Do not put the same grouping factor as both a fixed and a random effect7. (You could put a continuous variable as both a fixed and random effect if you have enough data) https://biologyforfun.wordpress.com/2015/08/31/two-little-annoying-stats-detail/↩ "],
["not-done-data-mining.html", "Chapter 10 [Not Done:] Data Mining", " Chapter 10 [Not Done:] Data Mining Data Mining "],
["not-done-simulations.html", "Chapter 11 [Not Done:] Simulations", " Chapter 11 [Not Done:] Simulations Simulations Monte Carlo simulations Randomness The Bootstrap Permutation Tests Simulating p values Power analyses Central Limit Theorem in simulations "],
["optimization-i-linear-optimization.html", "Chapter 12 Optimization I: Linear Optimization", " Chapter 12 Optimization I: Linear Optimization In this chapter we will switch gears slightly to start talking about Prescriptive Analytics: that is, how to use analytics to make better decisions using data. We’ll be discussing how to define certain problems as simple linear optimization problems: optimizing a given objective function, subject to certain linear constraints. We’ll cover how to use graphs to visualise and gain more intuition as to how to solve this problem. And finally, we will cover how to use R to solve the problem, as well as do additional sensitivity analyses. The learning objectives for this chapter are: Given an optimisation problem statement, readers should be able to properly identify an objective function and relevant constraints. Readers should be familiar with linear optimisation concepts like the feasible region and finding optimal solutions as corner points of the feasible region. Readers should be able to solve simple linear optimisation problems. Readers should be comfortable in interpreting the results of the optimisation and of sensitivity analyses, and preparing a recommendation memo with justifications. Readers should understand how and when integer constraints apply to optimisation problems. Readers should know how to use Linear-Program Relaxation (LP Relaxation) to gain insight into integer-constrained problems. Readers should understand how to solve integer-constrained optimisation problems. # Load the libraries we&#39;ll use in this chapter library(ggplot2) # for plotting library(lpSolve) # for solving linear optimization problems (&#39;linear programming&#39;) "],
["what-is-linear-optimization.html", "12.1 What is Linear Optimization", " 12.1 What is Linear Optimization Optimisation is used in every aspect of business: from operations, to finance, to HR, to marketing. Let’s imagine that you run a little bakery, and you have to decide how many of each type of product to make. You can, of course, decide your product line by saying “I like making cupcakes”, or “People really like my croissants”. But if you really want to optimize your sales and scale up product, you would have to do your own research and consider a range of factors, such as: what are the demand for each type of product, what are the costs (in terms of time and materials) associated with each type of product For example, from your “market research”, you may discover that mille-crepes are the rage now (in 2019) but they’re really effortful to make compared to normal cupcakes. So is it worth it to start branching out into this new product? When companies have factories that make thousands (or more) of products a day, one really has to use data to optimize the product mix. Note that this optimization isn’t about making the manufacturing processes faster or cheaper (that’s also a type of optimisation). Here we are interested in how much we need to produce. Some common use cases for linear optimization are: Product Mix: Deciding how much of each product to produce and sell. Product Line Planning: Deciding how much of each product to produce now vs later (e.g. targeting holiday sales season, using forecasts of supply/demand) Choosing an Investment Portfolio: we might be interested in choosing a mix of investment options that in this case might maximise our return, or perhaps minimise our risk exposure, subject to certain constraints. Optimising labor allocation: How best to schedule employees’ work shifts? Optimising transportation and supply chain: How to route deliveries to minimize wait time? How to get materials from suppliers to warehouse to distributors—very important especially for perishable goods. And many of us in today’s world use on-demand gig apps for transportation, food delivery, and so forth. These apps require lots of optimisation, and employ very complicated techniques, so the linear optimization techniques in this chapter will provide but a glimpse of this. Here are the basic steps in Linear Optimization, which we’ll go over in the next few Sections. Identify the Objective Function &amp; Decision Variables Identify the Constraints Write down the Optimization model Either: Solve graphically and/or manually Use R to solve Conduct Sensitivity Analysis Interpret results and make recommendations. "],
["objective-functions-decision-variables.html", "12.2 Objective Functions &amp; Decision Variables", " 12.2 Objective Functions &amp; Decision Variables Your goal (or objective) in linear optimisation is to optimise an objective function. This will be in the form of maximising or minimising some quantity of interest, by choosing the values of \\(X\\), our Decision Variables. Decision variables (\\(X\\)) are the variables that you get to pick. How many croissants to make? How many stocks of company A should I hold? In the objective function, we choose \\(X\\) to maximize or minimize some quantity of interest. Some common examples are to choose \\(X\\) to: maximise profits. minimise costs / time spent minimise risk exposure Where profits, costs, time spent, risk, etc, are given by some model of the world, that you as an analyst have come up with. I should note that we have already seen similar “optimization” in earlier chapters. For example, in Ordinary Least Squares Regression, we want to choose our regression coefficients (\\(b_0\\), \\(b_1\\), \\(b_2\\)…) to minimise the sum of squared errors. Or, we may want to choose model hyper parameters that maximises our model’s accuracy (e.g., on the Validation Set) Here, the difference is that we often formulate the objective function in terms of a real-world quantity (i.e., relevant to a business context) that we want to maximise or minimise, by choosing our X’s. Jean the Farmer: Objective Function Let’s get introduced to Farmer Jean, who runs a small farm in a little valley8, who will be with us throughout this chapter. Farmer Jean has two types of crops, Parsnips and Kale. Parsnips cost $0.20 per plot to grow, and sell for $0.35 per plot. Kale cost $0.70 per plot to grow, and sell for $1.10 per plot. She has 200 plots of land, and a $100 budget. How many of each crop should she plant to maximise her profits? Here, the decision variables are quite straightforward. We are just choosing how many plots of parsnips and plots of kale to grow. Let’s call them \\(X_1\\) and \\(X_2\\) respectively. Define: X_1 = number of plots of parsnips grown X_2 = number of plots of kale grown The objective function is just maximising profits, which is the selling price - cost price, multiplied by how many of each crop she grows. So Profits \\(= (0.35 - 0.20) X_1 + (1.10 - 0.70) X_2 = 0.15X_1 + 0.40X_2\\) Objective function: Maximize Profits = 0.15 X_1 + 0.40 X_2 Sprites and the costs/profits of Parsnips and Kale are borrowed from Stardew Valley, and this example is an optimization problem that could actually arise in the game.↩ "],
["constraints.html", "12.3 Constraints", " 12.3 Constraints In Farmer Jean’s case, obviously if we just wanted to maximise profits, we should just grow more and more of the profit maximising plant. But this is often not possible because of limited constraints. For example, you may not have unlimited budget, or unlimited time, or you may also need to fulfil certain criteria as part of your contract with your distributor. Formally, we represent these constraints as mathematical inequalities or equations. The inequalities can be less than, greater than, or strictly equal. On the left hand side we put a function of our decision variables, and on the right hand side we put the value of the constraint. So for example, if you have to “deliver within budget”, this means that your total cost of your decisions must be less than/equal to your budget. If you have to “deliver at least 50 units of product”, this means that number of product produced must be greater than or equal to 50. You may have to allocate working shifts to give exactly 40 hours of work per staff. So basically these constraints can be greater or less than or equal to some number. Examples Translates to: Deliver within Budget Total Cost \\(\\leq\\) Budget Deliver at least 50 units of product Product \\(\\geq\\) 50 Allocation must contain exactly 40 hours of work Working Hours \\(=\\) 40 Implicit: Products, hours of work must be non-negative Product \\(\\geq\\) 0 Hours \\(\\geq\\) 0 We also often have constraint called non-negativity constraints. For many of these decision variables that have real world meaning, they often cannot be negative. So we often specify that each of the decision variables must be greater than or equal to 0. These are important to specify explicitly, and in fact R assumes this. Jean the Farmer: Constraints Coming back to Farmer Jean, she has two types of constraints. First, she doesn’t have unlimited budget, with which she needs to pay the cost of growing each crop. So we can write this as the cost of producing parsnips times the amount of parsnips (\\(0.20X_1\\)), and the cost of producing kale times the amount of kale (\\(0.70X_2\\)), the sum of these two must be less than equal to 100. The second constraint she has is that she only has 200 plots of land. So the total amount of crops she grows has to fit into these plots. So \\(X_1\\) + \\(X_2\\) must be less than or equal to 200. And finally we have the non-negativity constraints. Name Constraints Budget Constraints \\(0.20X_1 + 0.70 X_2 \\leq 100\\) Space Constraints \\(X_1 + X_2 \\leq 200\\) Non-negativity \\(X_1 \\geq 0\\) \\(X_2 \\geq 0\\) "],
["solving-the-optimization-model.html", "12.4 Solving the Optimization Model", " 12.4 Solving the Optimization Model Before we start, it is good practice to formally write down the optimisation model that you want to solve. This summarises all the information in the problem into one system of equations to solve. So for the Farming Example, we have: Decision Variables: \\(X_1\\) = number of plots of parsnips grown \\(X_2\\) = number of plots of kale grown Maximize Profits = 0.15 \\(X_1\\) + 0.40 \\(X_2\\) subject to: - Budget Constraints \\(0.20X_1 + 0.70 X_2 \\leq 100\\) Space Constraints \\(X_1 + X_2 \\leq 200\\) Non-negativity \\(X_1 \\geq 0\\) \\(X_2 \\geq 0\\) Any solution that satisfies all the constraints is a feasible solution. In optimization, we want to find the best / optimal feasible solution. Let us understand this problem in more detail, by graphing our constraints as regions on an \\(X_1\\)-\\(X_2\\) graph, with \\(X_1\\) on the horizontal axis and \\(X_2\\) on the vertical axis. Below we’ve plotted the two constraints: \\(0.20X_1 + 0.70 X_2 \\leq 100\\), on the left, in magenta \\(X_1 + X_2 \\leq 200\\), on the right, in orange Since these are linear constraints, we can easily plot them by defining two points for each line, and then connecting them. For example, in the left graph, we need to plot: \\(0.20X_1 + 0.70 X_2 \\leq 100\\). The first point that will be handy is where it crosses the vertical axis; in this case, \\(X_1 = 0\\), and so we can solve \\(X_2 \\leq (100/0.70)\\), or \\(X_2 \\leq 142\\) (rounding down). The second point is where it crosses the horizontal axis. Now, \\(X_2 = 0\\), and so \\(X_1 \\leq (100/0.20)\\) or \\(X_1 \\leq 500\\). Thus, we can connect the point \\(X_1=0, X_2=142\\) on the vertical axis, with \\(X_1=500, X_2=0\\) on the horizontal axis with a straight line. Then we just read the constraint direction and reason that it should include the area down and to the left of this line! Make sure you understand the right graph as well, and understand how to reproduce it! Question: What do the non-negativity constraint regions look like? Feasible Regions The Feasible Region is the intersection of all the constraint regions. So let’s plot those two regions above on the same plot. A feasible solution is an allocation of decision variables (like \\(X_1\\) = 10, \\(X_2\\) = 10), that satisfies all the constraints. It also follows, that every feasible solution will lie in the feasible region. That means that every point within the feasible region (shaded in green above) could be a solution to this problem. If there is no feasible region, then there is no feasible solution, and we say that the problem is infeasible. In other words, there is no way to solve this problem while satisfying all the constraints. Corner Points While any point in the feasible region is a possible solution, it turns out that the optimal solution, if it exists, lies at a “corner” of the feasible region. Indeed, there are mathematical theorems proving this. In the plot below we’ve indicated the four corner points for this problem: Why must an optimal solution lie at a corner? Imagine that you have a solution in the center of the feasible region, not near a boundary. From here, you can still increase or decrease \\(X_1\\); or you can increase or decrease \\(X_2\\). These changes would either increase or decrease your profit (“better” or “worse”). So the best (and worst) solutions are when you cannot increase or decrease your decision variables any more! Let’s check our profits at each of the corner points. For three of them it’s simple; we just set X1 and X2 to 0 (Bottom-Left), or we set X1 to 0 (Top-Left), or we set X2 to 0 (Bottom-Right). \\[\\begin{align} \\text{Bottom-Left }:&amp; \\; X_1 = 0 ; \\;\\;\\;\\; X_2 = 0 ; \\;\\;\\;\\; \\text{Profit} = 0 \\quad [\\textbf{Worst}] \\\\ \\text{Top-Left }:&amp; \\; X_1 = 0 ; \\;\\;\\;\\; X_2 = 142 ; \\; \\text{Profit} = 56.8 \\\\ \\text{Bottom-Right }:&amp; \\; X_1 = 200 ; \\; X_2 = 0 ; \\;\\;\\;\\; \\text{Profit} = 30 \\\\ \\end{align}\\] Note, the WORST solution is also at a corner point. Can you reason out why? In this case, the bottom-left point happens to be the worst! At the fourth corner point (the top-right point), this point satisfies both the inequalities exactly. So they become equalities, which we can solve as a pair of simultaneous equations: \\[\\begin{align} X_1 + X_2 &amp;= 200 \\quad (1) \\\\ 0.20X_1 + 0.70X_2 &amp;= 100 \\quad (2) \\\\ (1) \\implies X_2 &amp;= 200 - X_1 \\\\ \\text{sub into } (2): 0.20X_1 + 0.70 (200 - X_1) &amp;= 100 \\\\ (0.20 - 0.70) X_1 &amp;= 100 - 140 \\\\ X_1 &amp;= 80 \\\\ \\implies X_2 &amp;= 120 \\end{align}\\] Hence, the solution is \\(X_1 = 80\\), \\(X_2 = 120\\), which gives us a profit of \\(0.15X_1 + 0.40X_2 = 60\\). This happens to be the maximum profit, so (80,120) gives us the optimal solution with a maximum profit of $60. Level Sets An alternative way to visualise this solution is to draw the level set of the objective function (i.e., profits). The level set is the set of all the points that give the same profit. We indicate the direction in which we are optimising (i.e., direction of increasing profit), and we “shift” the level set as we increase profit. The optimal solution is the last point before the level set leaves the feasible region as we optimise profits. (If the level set is parallel to a constraint line, we could also get a set of optimal solutions) Solution Types There are 4 possible types of solutions: There exists a unique optimal solution. There exists multiple optimal solutions. Graphically, this occurs when the set of optimal solutions (i.e., the level set of the optimal solution) is parallel to / lies along a constraint. Thus, every point along that constraint will give an optimal solution. The solution is unbounded There exists no feasible solutions Finding corner points in higher dimensions So we’ve seen that the optimal solution must lie on a corner point. This is true even if we have more than two variables and we cannot plot this on a 2D graph. A ten decision-variable problem will have a feasible region be a region in a ten dimensional space, also called a 10-dimensional polytope. The solution will lie on a corner, or vertex, of this polytope. Image of a high-dimensional polytope and a path that Simplex Algorithm might take, from Wikipedia One algorithm to solve these optimization problems is called the Simplex Algorithm, which systematically checks through these corner vertices to find the optimal one. The algorithm starts at a vertex (perhaps chosen randomly or through some initialization) It will then move along an edge (“side of polytope”) to another vertex only if the objective function value along that edge is increasing. It repeats this until it reaches a maximum, i.e., no such edges are found. The algorithm terminates at the optimal solution, or reports that none are found (infeasible, or unbounded). For this chapter, we do not cover how the Simplex algorithm works, but we just discuss the high-level intuition behind it. We will let R implement this algorithm and solve more difficult optimization problems for us. "],
["using-r-to-solve-linear-optimization.html", "12.5 Using R to solve Linear Optimization", " 12.5 Using R to solve Linear Optimization The most difficult part about using R to solve a linear optimization problem is to translate the optimization problem into code. Let’s reproduce the table with all the necessary information for the example of Farmer Jean: Decision Variables: \\(X_1\\) = number of plots of parsnips grown \\(X_2\\) = number of plots of kale grown Maximize Profits = 0.15 \\(X_1\\) + 0.40 \\(X_2\\) subject to: - Budget Constraints \\(0.20X_1 + 0.70 X_2 \\leq 100\\) Space Constraints \\(X_1 + X_2 \\leq 200\\) Non-negativity \\(X_1 \\geq 0\\) \\(X_2 \\geq 0\\) Here’s how you translate it into code. First, we define the objective function parameters, which are just the coefficients of \\(X_1\\) and \\(X_2\\) in the object function: Profits = 0.15 \\(X_1\\) + 0.40 \\(X_2\\) objective.fn &lt;- c(0.15, 0.40) Next, we define the constraints, which are broken up into 3 variables: the constraints matrix, the constraint directions, and the constraint values (or constraint RHS for right-hand-side). \\[0.20X_1 + 0.70 X_2 \\leq 100\\] \\[X_1 + X_2 \\leq 200\\] The constraint matrix is simply concatenating all the coefficients here into a matrix. For simplicity, using the convention below, we just read off the matrix from top to bottom, then within each line, from left to right. So we have: (0.20, 0.70, 1, 1). When I construct the matrix, you have to specify the number of columns (ncol) which is simply the number of decision variables we have; in this case it’s 2. The constraint directions are just a vector that corresponds to each constraint (\\(\\leq\\), \\(\\leq\\)), and the constraint right-hand-side values are just (100, 200) const.mat &lt;- matrix(c(0.20, 0.70, 1, 1) , ncol=2 , byrow=TRUE) const.dir &lt;- c(&quot;&lt;=&quot;, &quot;&lt;=&quot;) const.rhs &lt;- c(100, 200) The important thing to note is to get the order of all the constraints correct. In particular, if you have certain constraints that do not include ALL of the decision variables, to include 0s whenever appropriate. TIP! For example, if you have an additional constraint that you can only produce a maximum of 500 \\(X_1\\), this constraint translates to: \\(X_1 \\leq 500\\) but to be even more helpful to yourself, write it out as: \\[ 1 X_1 + 0 X_2 \\leq 500 \\] This will help you remember to put … 1,0 … into the relevant row of the constraints matrix when you are reading the matrix off from your table. Finally, we put all of these into a call to the lp function within the lpSolve package. We specify max for maximizing the objective function, pass in the rest of the parameters we just defined, and finally we also ask it to compute.sens=TRUE: we need this for the sensitivity analysis in the next section. lp.solution &lt;- lp(&quot;max&quot;, objective.fn, const.mat, const.dir, const.rhs, compute.sens=TRUE) Putting it all together, and looking at the solution (lp.solution$solution) and objective function value (lp.solution$objval) # defining parameters objective.fn &lt;- c(0.15, 0.40) const.mat &lt;- matrix(c(0.20, 0.70, 1, 1) , ncol=2 , byrow=TRUE) const.dir &lt;- c(&quot;&lt;=&quot;, &quot;&lt;=&quot;) const.rhs &lt;- c(100, 200) # solving model lp.solution &lt;- lp(&quot;max&quot;, objective.fn, const.mat, const.dir, const.rhs, compute.sens=TRUE) # check if it was successful; it also prints out the objective fn value lp.solution ## Success: the objective function is 60 # optimal solution (decision variables values) lp.solution$solution ## [1] 80 120 # Printing it out: cat(&quot;The optimal solution is:&quot;, lp.solution$solution, &quot;\\nAnd the optimal objective function value is:&quot;, lp.solution$objval) ## The optimal solution is: 80 120 ## And the optimal objective function value is: 60 Thus, the optimal solution is \\(X_1\\) = 80, \\(X_2\\) = 120, and the optimal profit is 60, which is what we found manually in the previous section. "],
["sensitivity-analysis.html", "12.6 Sensitivity Analysis", " 12.6 Sensitivity Analysis When we get an optimal solution to a linear optimisation problem, oftentimes we may want to ask: what if we change the objective by a little, or what if we change the constraints by a little, what would happen? i.e., how sensitive is the solution to changes in the problem? Sensitivity analysis allows us to ask this question systematically. Here we shall cover two types of sensitivity analyses – varying objective function coefficients, and varying constraint values. Varying objective function coefficients Recall our farming example: our optimal solution is \\(X_1\\) = 80, \\(X_2\\) = 120, and our objective function is: \\[ \\text{Profit} = 0.15 X_1 + 0.40 X_2 \\] What if Farmer Jean’s customer decides to reduce the amount they are willing to pay for Kale (\\(X_2\\)) to $1.00 (reducing Jean’s profits from 0.40 to 0.30 per unit of \\(X_2\\)). \\[ \\text{Profit} = 0.15 X_1 + \\color{red}{0.30} X_2 \\] Will that change our optimal solution? Turns out, no, it doesn’t. (Try running the code again to verify that the optimal solution is still (80,120)!) But now if the price that Farmer Jean can sell Parsnips (\\(X_1\\)) drops from 0.35 to 0.30, i.e., reducing her profits per unit-parsnips from 0.15 to 0.10, \\[ \\text{Profit} = \\color{red}{0.10} X_1 + 0.40 X_2 \\] Then the optimal solution actually changes (to \\(X_1\\) = 0, \\(X_2\\) = 142)! Basically, Parsnips are now not profitable, and Jean should plant plant all Kale. Graphically the optimal solution now moves from the blue vertex to the green vertex. The optimal solution is to produce as much Kale as she can afford. Indeed in the first case, reducing the price of \\(X_2\\) by 10 cents did not change the solution, but in the second case, reducing the price of \\(X_1\\) by 5 cents did! So this solution is sensitive to some changes but not others. Varying objective function coefficients in R R’s lpSolve::lp() function helps you to calculate the range of coefficient values for which the given solution is still optimal. So again, recall that the original objective function is: \\[ \\text{Profit} = 0.15 X_1 + 0.40 X_2 \\] We use lp.solution$sens.coef.from and lp.solution$sens.coef.to to get the range of coefficients. # sensitivity analysis on coefficients lp.solution$sens.coef.from ## [1] 0.1142857 0.1500000 lp.solution$sens.coef.to ## [1] 0.400 0.525 The way you read this is that “from” is the lower bound of the coefficients (\\(X_1\\) and \\(X_2\\) respectively), while “to” gives the upper bound of the coefficients. This means that as long as the coefficient on \\(X_1\\) lies between [0.1143, 0.400], this solution is still optimal. Or if the coefficient on \\(X_2\\) lies between [0.150, 0.525], this solution is still optimal. If the coefficients shift outside these ranges, then the solution will move to a different vertex. (How will they move?) Note that all of these sensitivity calculations assume you only adjust one coefficient at a time. This means that if you decide to reduce \\(X_1\\) to 0.12 and increase \\(X_2\\) to 0.50, the optimal solution might change. These ranges assume that you only change that one coefficient while holding the rest constant. If you want to change two or more coefficients, you should re-run the model. Now let’s think through and try to predict what happens when these coefficients go out of these bounds. We just saw that if the coefficient on \\(X_1\\) goes to 0.10, which is less than 0.1143, then the solution moves to: produce no \\(X_1\\) and produce maximum \\(X_2\\). What if the coefficient on \\(X_1\\) goes above 0.400? What do you think will happen? Increasing the profit of \\(X_1\\) beyond this range should mean that it has become more profitable to produce \\(X_1\\), and we know that the solution must lie on a vertex, so we move down the edge of the feasible region to the solution on the horizontal axis, \\(X_1 = 200, X_2 = 0\\). Similarly, we can think about what will happen when the coefficient on \\(X_2\\) drops below 0.15: it becomes not profitable to produce Kale, so we’ll also get the point \\(X_1 = 200, X_2 = 0\\). But when the coefficient on \\(X_2\\) rises above 0.525, then it becomes much more profitable to produce Kale, so we’ll come to the solution \\(X_1 = 0, X_2 = 142\\). Varying Constraint Values (Shadow Prices) The next type of sensitivity analysis we shall look at is what happens when we vary the constraint values. We’ll introduce a new term called shadow prices. The Shadow Price of a constraint is the change in the objective function value per unit-increase in the right-hand-side value of that constraint (holding all else equal). Let’s break this down. In the Farming example, we have two constraints. Let’s consider the first constraint. What if we increased the RHS of the first constraint from 100 to 101? \\[0.20X_1 + 0.70X_2 \\leq 100 \\quad \\rightarrow \\quad 0.20X_1 + 0.70X_2 \\leq \\color{green}{101}\\] The feasible region was pushed outwards a little, by adding this green segment. Doing this will move the optimal vertex up and to the left. The new solution is (\\(X_1\\) = 78, \\(X_2\\) = 122), which gives a profit of $60.50 – an increase of $0.50 Thus, increasing the budgetary constraint by 1 unit (100 to 101) increases the profit by $0.50 (from $60 to $60.50). Hence, (by definition), the Shadow Price of the first constraint is $0.50. What this means is that if Farmer Jean increased her operating budget from $100 to $101, then she can increase her profits by $0.50. That’s a pretty good return-on-investment! (Note that the optimal solution also changes, but here the shadow price indicates the effect of changing the constraint value on the PROFIT). Next, let’s consider the second constraint. What if we increased the RHS of the second constraint from 200 to 201? \\[X_1 + X_2 \\leq 200 \\quad \\rightarrow \\quad X_1 + X_2 \\leq \\color{cyan}{201}\\] The feasible region was pushed out a little, by adding this cyan segment, which moved the optimal vertex down and to the right. The new solution is (\\(X_1\\) = 81.4, \\(X_2\\) = 119.6), which gives a profit of $60.05 – an increase by $0.05 Thus, increasing the space constraint by 1 unit (200 to 201) increases the profit by $0.05 (from $60 to $60.05). The Shadow Price of the space constraint is $0.05. What this means is that if Farmer Jean increased her land plot size from 200 to 201, then she can increase her profits by $0.05. That’s not as high as the budgetary constraints, so this translates to a good recommendation: if Jean wants to increase her profit, she gets more bang of the buck if she gets more operating budget than land space. Varying Constraint Values in R Shadow prices are also known as duals in other fields (e.g. computer science), so we use lp.solution$duals to get them. # Shadow prices lp.solution$duals ## [1] 0.50 0.05 0.00 0.00 Notice that there are four values here. The first two are the two constraints we put in, in the order they were specified in the constraints matrix (so \\(0.20X_1 + 0.70X_2 \\leq 100\\) then \\(X_1 + X_2 \\leq 200\\)). So we can see, as per our calculations above, that the shadow price of the first constraint is $0.50, and that of the second constraint is $0.05. The last two are the shadow prices of the non-negativity constraints \\(X_1 \\geq 0\\), and \\(X_2 \\geq 0\\). This means that the change in the optimal profit, if we were to increase the right hand side values from 0 to 1, are both 0. Note that in some fields, the shadow prices of the non-negativity constraints have a special name, reduced costs. But the interpretation is the same. Why would we have a shadow price of zero for these non-negativity constraints? In this particular case, in the optimal solution we are already producing non-zero quantities of \\(X_1\\) and \\(X_2\\). Thus, increasing the right-hand-side value (i.e., forcing us to produce at least 1 \\(X_1\\) or at least 1 \\(X_2\\)) will not change our optimal solution. The shadow prices are zero. Binding vs non-binding constraints In general, constraints can either be binding or non-binding for the optimal solution. Constraints that are binding ‘restrict’ the optimal solution; so in the Parsnips/Kale example, both the Budget and Space constraints are binding; if we increase the right-hand-side of the constraints, we can do better and increase our profit. Hence, they have non-zero shadow prices. Conversely, non-binding constraints do not restrict or bind the optimal solution. Thus, even if we change the right-hand-side value of the constraint by 1, we will not affect the optimal solution. Thus, shadow prices are zero for non-binding constraints. In certain cases, we might even have negative shadow prices. Let’s consider a modified objective function in the same Parsnip-Kale example, where the profit of Kale is increased to 0.53. \\[ \\text{Profit} = 0.15 X_1 + \\color{red}{0.53} X_2 \\] We saw from our earlier sensitivity analysis that this will change the optimal solution to: \\(X_1 = 0, X_2 = 142\\). Let’s check this by re-running this modified linear optimization problem, saving it as lp.solution2: # defining parameters objective.fn2 &lt;- c(0.15, 0.53) const.mat &lt;- matrix(c(0.20, 0.70, 1, 1) , ncol=2 , byrow=TRUE) const.dir &lt;- c(&quot;&lt;=&quot;, &quot;&lt;=&quot;) const.rhs &lt;- c(100, 200) # solving model lp.solution2 &lt;- lp(&quot;max&quot;, objective.fn2, const.mat, const.dir, const.rhs, compute.sens=TRUE) # check if it was successful; it also prints out the objective fn value lp.solution2 ## Success: the objective function is 75.71429 # Printing it out: cat(&quot;The optimal solution is:&quot;, lp.solution2$solution, &quot;\\nThe optimal objective function value is:&quot;, lp.solution2$objval, &quot;\\nwith the following Shadow Prices&quot;, lp.solution2$duals) ## The optimal solution is: 0 142.8571 ## The optimal objective function value is: 75.71429 ## with the following Shadow Prices 0.7571429 0 -0.001428571 0 As we expected, the solution is \\(X_1 = 0, X_2 = 142\\) (rounded down). Now, if we look at the shadow prices, we notice that the shadow price of the first constraint: \\(0.20X_1 + 0.70X_2 \\leq 100\\), is +0.75, so increasing the budget by $1 will increase profits by $0.75. The shadow price of the second constraint, \\(X_1 + X_2 \\leq 200\\), in this case, is 0. We can also see that because the total amount of land used \\(X_1 + X_2 = 0 + 142 = 142\\) is much less than the available budget, this constraint now becomes non-binding at this solution, and hence the shadow price of this constraint is zero. Getting access to more land will not help improve profits. But wait, the third shadow price returned by lp.solution2$duals is negative! What does that mean? This shadow price corresponds to the non-negativity constraint \\(X_1 \\geq 0\\). Going by the definition of the shadow price, this means that increasing the value of the right hand side of this constraint, to \\(X_1 \\geq \\color{red}{1}\\) will reduce the optimum profit by -$0.0014. This is because if this constraint were to be changed, then now we are forced to produce at least 1 unit of \\(X_1\\), which is now less profitable, and hence total profit should go down. Finally, note that this also implies that \\(X_1 \\geq 0\\) is a binding constraint, because changing the value of this constraint will affect the optimal solution. So binding constraints can have either positive, or negative, shadow prices. Summarizing sensitivity analyses Solving an optimization problem is straight-forward once you have the objective function and the constraints. Getting the optimal solution is not the end of it. As a top-notch business analyst, we can add more value to the business decision by doing and interpreting sensitivity analyses. We can examine the range of coefficients over which this solution is valid, and make recommendations. For example, we saw earlier than if the profit of Kale goes above 0.53 or profit per Parsnip goes below 0.11, then it makes more sense to switch to producing as much Kale as possible. For example, you could provide the following recommendation to Farmer Jean: “If the profit per unit-Kale goes above $0.53 or if the profit per unit-Parsnips goes below $0.11, then please switch your whole production to Kale.” Secondly, we can examine and interpret shadow prices: For example, to Farmer Jean: “If you get one additional plot of land, you can make $0.05 more profit. But if you get $1 more of budget to buy crops, you can increase your profit by $0.50 (by changing your planting allocation to …)” More broadly, as analysts we could run “what-if” analyses to study the impact of getting more resources, of changing our prices, and of many other possible business decisions. These type of analyses require much more experience, but are potentially much more valuable to making better business decisions. "],
["linear-optimization-summary.html", "12.7 Linear Optimization Summary", " 12.7 Linear Optimization Summary In this chapter, we covered the basics of formulating and solving simple linear optimisation problems, which help us “prescribe” the best business choices to make. If you can formulate your real-world problem as a linear optimisation problem, then there are very efficient solvers and algorithms that can solve the problem, and to help you gain further insight into your problem (e.g., via sensitivity analyses). In the models we’ve covered this chapter, we’ve assumed that all the decision variables are continuous, real-valued numbers. In the next chapter, we will discuss linear optimisation problems where some of the decision variables have to be integers, as well as when some of the decision variables are binary yes/no decisions. "],
["not-done-exercises.html", "12.8 [NOT DONE] Exercises", " 12.8 [NOT DONE] Exercises to be completed "],
["optimization-ii-integer-valued-optimization.html", "Chapter 13 Optimization II: Integer-valued Optimization", " Chapter 13 Optimization II: Integer-valued Optimization In this chapter we will continue our discussion of linear optimization, but now with the added complexity of considering problems with integer-valued decision variables. The learning objectives for this chapter are: Readers should understand how and when integer constraints apply to optimisation problems. Readers should know how to use Linear-Program Relaxation (LP Relaxation) to gain insight into integer-constrained problems. Readers should understand how to solve integer-constrained optimisation problems. # Load the libraries we&#39;ll use in this chapter library(ggplot2) # for plotting library(lpSolve) # for solving linear optimization problems (&#39;linear programming&#39;) "],
["integer-valued-decision-variables.html", "13.1 Integer-valued decision variables", " 13.1 Integer-valued decision variables There are many other examples of integer-valued optimisation problems (i.e., with integer variables). For example, how many days to operate a factory; how many cars / products to produce; how many shifts to assign an employee We can also consider the special case of binary-valued decision variables (i.e., when your decision variable \\(X\\) can only take values of 1 or 0). Binary (Yes/No) Decisions: Should I invest or not? \\(X_i\\) = 1 or 0 Is worker Bob assigned to shift \\(j\\)? \\(X_j\\) = 1 or 0 And finally, we will also see how we can represent logical constraints using binary decision variables. This allows us to encode constraints like, “if A, then B”. In an optimisation problem, if some (but not all) of the decision variables are integer-valued, this is called a mixed-integer optimisation problem. "],
["not-done-from-real-valued-to-integer-solutions.html", "13.2 [NOT DONE] From real-valued to integer solutions", " 13.2 [NOT DONE] From real-valued to integer solutions [ need to rework examples from lecture to illustrate this more ] Feasible solutions are now integer-valued solutions that lie within the feasible region bounded by the other constraints Can we just round off the real-valued answer? No. In this case, rounded-answer is not feasible Can we just round down the answer or round off to the nearest feasible answer? Maybe. But the nearest feasible answer is not guaranteed to be optimal! We’ve seen that the optimal integer solution may not necessarily be near the optimal real-valued solution. But that doesn’t mean the real-valued solution is useless. In fact, the most common way to solve the integer optimisation problem is to FIRST solve the problem without the integer constraints. Solving the optimisation without the integer constraints is called Linear Program [LP] Relaxation, because you “relax” the integer constraints. If you solve the LP relaxation, and the solution happens to be integer valued, then you are lucky, and that solution is also the optimal solution to the integer problem. think back to the example of Farmer Jean. In that case the solution was an integer number of plots of crops, (80, 120) But this will not often be the case that the solution to the LP relaxation will be integer-valued. Then, programs use the real-valued solution as a starting point to systematically search for the optimal integer solution (e.g. “branch and bound” algorithm used by lpSolve). "],
["logical-constraints.html", "13.3 Logical Constraints", " 13.3 Logical Constraints Some logical constraints can also be formalised as integer constraints. For example, if I choose \\(X_1\\), then I must choose \\(X_2\\) as well. This can be formalised as the following constraint: \\[X_2 - X_1 \\geq 0\\] If \\(X_1\\) is 1, then this forces \\(X_2\\) to be 1 too. In general, the following table summaries how some logical constraints can be represented using binary decision variables: Logical Condition Linear Constraint if \\(A\\) then \\(B\\) \\(B - A \\geq 0\\) if not \\(A\\) then \\(B\\) \\(B - (1 - A) \\geq 0\\), or \\(A + B \\geq 1\\) At most one of \\(A\\), \\(B\\) \\(A + B \\leq 1\\) At least one of \\(A\\), \\(B\\) \\(A + B \\geq 1\\) if (\\(A\\) or \\(C\\)) then \\(B\\) \\(B - (A+C) \\geq 0\\) if (\\(A\\) and \\(C\\)) then \\(B\\) \\(B - (A+C) \\geq -1\\), or \\((A+C) - B \\leq 1\\) if \\(A\\) then (\\(B\\) or \\(C\\)) \\((B+C) - A \\geq 0\\) if \\(A\\) then (\\(B\\) and \\(C\\)) \\((B+C) - 2A \\geq 0\\) 13.3.1 Logical Constraints Example: Planning university courses Let’s see logical constraints in action in the following example. Natalie has decided to switch her career to data science, as she feels it has more prospects than her previous industry. She is eyeing a “Pico-masters (TM)” program at her local university, where she has to complete 40 units of courses to satisfy the pico-degree. (“Pico-masters” and “nano-masters” are fictional—at least, for now—but some online education platforms and universities like edX are offering “MicroMasters®” and other similar products.) The program offers the following courses, in STatistics, ProGramming, and Data Management, along with their associated costs and pre-requisites. The pre-requisites for each course must be fulfilled before students are allowed to take that course. In order to finish the PicoMasters, she needs to finish a specialization in one of the three tracks, which is fulfilled by completing the “Level 3” version of that course. Natalie has also indicated her personal interest in each course, in the following table. Course Units Pre-requisites Interest ST1 10 - 8 ST2 10 ST1 4 ST3 10 ST2 6 PG1 10 - 7 PG2a 10 PG1 5 PG2b 10 PG1 6 PG3 10 PG2a or PG2b 3 DM1 10 - 4 DM2 10 DM1 6 DM3 10 DM2 7 Imagine that her goal is to maximize her interest and satisfy the requirements of the degree, while also taking exactly 40 units (in order to keep costs low). Which classes should she take? First, let’s define our decision variables, by just using \\(X_1\\) to \\(X_{10}\\) to correspond to taking each of the ten courses, in the order in the table above. That is, \\(X_1\\) = Take ST1, \\(X_2\\) = Take ST2, \\(X_3\\) = Take ST3, \\(X_4\\) = Take PG1, \\(X_5\\) = Take PG2a, \\(X_6\\) = Take PG2b, \\(X_7\\) = Take PG3, \\(X_8\\) = Take DM1, \\(X_9\\) = Take DM2, \\(X_{10}\\) = Take DM3 Now let’s tackle the constraints one-by-one. Consider the following pre-requisite: Natalie needs to take ST1 before being allowed to take ST2. We can represent this using the following constraint: \\[ X_1 - X_2 \\geq 0 \\] You can easily check if this is correct by ‘testing’ it by subtituting values. If Natalie takes ST2 (i.e., \\(X_2 = 1\\)), then we have the following inequality: \\(X_1 - 1 \\geq 0\\). In order for this also to be true, we need \\(X_1\\) to also be 1. This means that Natalie taking ST2 (\\(X_2 = 1\\)) actually forces her to also have taken ST1 (\\(X_1 = 1\\)). Similarly, we can write out the pre-requisite constraints for the rest. Now, let’s consider one more case: Natalie needs to take either PG2a OR PG2b before being allowed to take PG3. One easy thing to try is to replace ST2 (\\(X_2\\)) in the previous inequality with PG3 (\\(X_7\\)), and now we have to replace ST1 (\\(X_1\\)) with (PG2a or PG2b), or (\\((X_5+X_6)\\)). So let’s try: \\[ (X_5 + X_6) - X_7 \\geq 0 \\] And actually this is the correct inequality! Let’s test this out. If Natalie takes PG3, then \\(X_7=1\\), and so we are left with: \\((X_5 + X_6) - 1 \\geq 0\\). This inequality will be satisfied if \\(X_5\\) is 1, or if \\(X_6\\) is 1, that is, if she takes either PG2a or PG2b. (Note that this will also be satisifed if BOTH \\(X_5\\) and \\(X_6\\) is 1 – there is nothing stopping her from taking both PG2a and PG2b!) And finally let’s consider the constraint that she needs to finish a specialization. That is, she needs to take either ST3 (\\(X_3\\)) or PG3 (\\(X_7\\)) or DM3 (\\(X_{10}\\)). This means that at least one of them needs to be 1. So we can just write: \\[ X_3 + X_7 + X_{10} \\geq 0 \\] Please make sure you understand how we got (or how we tested/verified) the above inequalities! Now, we can finally write out our optimization problem in a large table: Decision variables \\(X_1\\) = Take ST1, \\(X_2\\) = Take ST2, \\(X_3\\) = Take ST3, \\(X_4\\) = Take PG1, \\(X_5\\) = Take PG2a, \\(X_6\\) = Take PG2b, \\(X_7\\) = Take PG3, \\(X_8\\) = Take DM1, \\(X_9\\) = Take DM2, \\(X_{10}\\) = Take DM3 Maximize Interest = 8\\(X_1\\) + 4\\(X_2\\) + 6\\(X_3\\) + 7\\(X_4\\) + 5\\(X_5\\) + 6\\(X_6\\) + 3\\(X_7\\) + 4\\(X_8\\) + 6\\(X_9\\) + 7\\(X_{10}\\) Subject to Minimum Course Requirements (= for cost savings) 10 \\(X_1\\) + 10 \\(X_2\\) + 10 \\(X_3\\) + 10 \\(X_4\\) + 10 \\(X_5\\) + 10 \\(X_6\\) + 10 \\(X_7\\) + 10 \\(X_8\\) + 10 \\(X_9\\) + 10 \\(X_{10}\\) \\(=\\) 40 Pre-Requisite for ST2 \\(X_1 - X_2 \\geq 0\\) or 1 \\(X_1\\) + (-1) \\(X_2\\) + 0 \\(X_3\\) + 0 \\(X_4\\) + 0 \\(X_5\\) + 0 \\(X_6\\) + 0 \\(X_7\\) + 0 \\(X_8\\) + 0 \\(X_9\\) + 0 \\(X_{10}\\) \\(\\geq\\) 0 Pre-Requisite for ST3 \\(X_2 - X_3 \\geq 0\\) or 0 \\(X_1\\) + 1 \\(X_2\\) + -1 \\(X_3\\) + 0 \\(X_4\\) + 0 \\(X_5\\) + 0 \\(X_6\\) + 0 \\(X_7\\) + 0 \\(X_8\\) + 0 \\(X_9\\) + 0 \\(X_{10}\\) \\(\\geq\\) 0 Pre-Requisite for PG2a \\(X_4 - X_5 \\geq 0\\) or 0 \\(X_1\\) + 0 \\(X_2\\) + 0 \\(X_3\\) + 1 \\(X_4\\) + -1 \\(X_5\\) + 0 \\(X_6\\) + 0 \\(X_7\\) + 0 \\(X_8\\) + 0 \\(X_9\\) + 0 \\(X_{10}\\) \\(\\geq\\) 0 Pre-Requisite for PG2b \\(X_4 - X_6 \\geq 0\\) or 0 \\(X_1\\) + 0 \\(X_2\\) + 0 \\(X_3\\) + 1 \\(X_4\\) + 0 \\(X_5\\) + -1 \\(X_6\\) + 0 \\(X_7\\) + 0 \\(X_8\\) + 0 \\(X_9\\) + 0 \\(X_{10}\\) \\(\\geq\\) 0 Pre-Requisite for PG3 (PG2a OR PG2b) \\((X_5 + X_6) - X_7 \\geq 0\\) or 0 \\(X_1\\) + 0 \\(X_2\\) + 0 \\(X_3\\) + 0 \\(X_4\\) + 1 \\(X_5\\) + 1 \\(X_6\\) + -1 \\(X_7\\) + 0 \\(X_8\\) + 0 \\(X_9\\) + 0 \\(X_{10}\\) \\(\\geq\\) 0 Pre-Requisite for DM2 \\(X_8 - X_9 \\geq 0\\) or 0 \\(X_1\\) + 0 \\(X_2\\) + 0 \\(X_3\\) + 0 \\(X_4\\) + 0 \\(X_5\\) + 0 \\(X_6\\) + 0 \\(X_7\\) + 1 \\(X_8\\) + -1 \\(X_9\\) + 0 \\(X_{10}\\) \\(\\geq\\) 0 Pre-Requisite for DM3 \\(X_9 - X_{10} \\geq 0\\) or 0 \\(X_1\\) + 0 \\(X_2\\) + 0 \\(X_3\\) + 0 \\(X_4\\) + 0 \\(X_5\\) + 0 \\(X_6\\) + 0 \\(X_7\\) + 0 \\(X_8\\) + 1 \\(X_9\\) + -1 \\(X_{10}\\) \\(\\geq\\) 0 Complete Specialization \\(X_3 + X_7 + X_{10} \\geq 1\\) or 0 \\(X_1\\) + 0 \\(X_2\\) + 1 \\(X_3\\) + 0 \\(X_4\\) + 0 \\(X_5\\) + 0 \\(X_6\\) + 1 \\(X_7\\) + 0 \\(X_8\\) + 0 \\(X_9\\) + 1 \\(X_{10}\\) \\(\\geq\\) 1 Binary, Integer, Non-Negativity Constraints \\(X_1\\), to \\(X_{10}\\) all binary, integers and \\(\\geq 0\\). Notice that I’ve written out the inequalities into longer forms with explicit “0”s, so \\[X_1 - X_2 \\geq 0\\] becomes \\[X_1 + (-1) X_2 + 0 X_3 + 0 X_4 + 0 X_5 + 0 X_6 + 0 X_7 + 0 X_8 + 0 X_9 + 0 X_{10} \\geq 0\\] This makes it much easier to type this into code. This is the hardest part now, transfering this into code without making any mistakes! #defining parameters objective.fn &lt;- c(8, 4, 6, 7, 5, 6, 3, 4, 6, 7) const.mat &lt;- matrix(c(rep(10,10), rep(0,0), 1, -1, rep(0, 8), # ST2 rep(0,1), 1, -1, rep(0, 7), # ST3 rep(0,3), 1, -1, rep(0, 5), # PG2a rep(0,3), 1, 0, -1, rep(0, 4), # PG2b rep(0,4), 1, 1, -1, rep(0, 3), # PG3 rep(0,7), 1, -1, rep(0, 1), # DM2 rep(0,8), 1, -1, rep(0, 0), # DM3 rep(0,2), 1, rep(0,3), 1, rep(0,2), 1 # complete specialization ) , ncol=10 , byrow=TRUE) const.dir &lt;- c(&quot;=&quot;, rep(&quot;&gt;=&quot;, 8)) const.rhs &lt;- c(40, rep(0,7), 1) #solving model lp.solution &lt;- lp(&quot;max&quot;, objective.fn, const.mat, const.dir, const.rhs, binary.vec = c(1:10)) lp.solution$solution #decision variables values ## [1] 1 1 1 1 0 0 0 0 0 0 lp.solution ## Success: the objective function is 25 Thus, the final solution is \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(X_4\\) = 1, with the rest = 0. In order to maximize her interest while completing the degree, Natalie should choose to specialize in Statistics (taking ST1, ST2, ST3), and then also taking the first course in Programming (PG1). "],
["integer-optimization-summary.html", "13.4 Integer Optimization Summary", " 13.4 Integer Optimization Summary In this lecture we’ve covered how to use linear optimisation to solved integer-valued optimisation problems, where some of the decision variables are constrained to be integers, or even constrained to be binary variables (which can be used to model yes/no decisions). We’ve also seen how to transform logical constraints into linear constraints using binary variables. And we’ve also seen worked examples of problems with 20+ decision variables – these problems scale up very fast! The output of these problems can be very useful in helping managers make optimal decisions based on the data, and is the whole objective behind prescriptive analytics. "],
["not-done-exercises-1.html", "13.5 [NOT DONE] Exercises", " 13.5 [NOT DONE] Exercises to be completed Q1) John is interested in buying ads to market his new startup. He sees the following options: Ad Cost per ad Reach Limits Radio Ad $100 500 40 Newspaper Ad $250 2000 10 Social Media Ad $50 300 80 The “limits” in the table above are imposed by each advertiser, so the Newspaper will only run a maximum of 10 Newspaper Ads. Reach is an estimated number of people that the ad will reach, per ad that John buys (e.g. if he buys 1 Radio ad, it will reach 500 people. If he buys 2, it will reach 1000 people.) Q1a) Identify the decision variables, objective function and constraints. Write out the optimization problem in a table. Q1b) Write R code to solve this problem. What is the optimal solution, and what is the value of the objective function that this optimal solution? something similar to the distribution center example? "],
["not-done-book-summary.html", "Chapter 14 [Not Done:] Book Summary ??", " Chapter 14 [Not Done:] Book Summary ?? Summary "],
["references.html", "References", " References "]
]
