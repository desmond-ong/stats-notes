[
["index.html", "Statistics and Analytics for the Social and Computing Sciences Preface", " Statistics and Analytics for the Social and Computing Sciences Desmond C. Ong Draft Dated: 2021-08-09 Preface This is a working collection of notes on statistics and data analytics that I am compiling, with two goals: To serve as a supplement to a course that I teach at the National University of Singapore (BT1101: Introduction to Business Analytics), a statistics course in R targeted at first-year undergraduate students and aspiring data scientists. These sections will cover introductory material, and will be marked with BT1101 . To discuss material that would be useful for graduate-level researchers in the social and computing sciences. This material will build upon the introductory level, and will be marked with Advanced I hope to cover quite broadly several important and useful statistical tools (such as multivariate regression and simulations), as well as discuss issues like data visualization best practices. I also plan to write several chapters on applying statistics in the computing sciences (for example, proper statistics when analyzing machine learning models). And finally, if I have time, I would like to transition to teaching statistics in a more Bayesian tradition. My philosophy in teaching statistics and analytics is to focus on helping students to achieve a conceptual understanding, and develop their own intuition for data. Yes, students will need some mathematical background to appreciate statistics, and yes, students will need to learn some programming (in R) to actually implement modern statistical calculations, but these are means to an end. The end is an appreciation of data, and especially, how data exists in the real world. Real data rarely conforms to the assumptions we make in our analyses. The job of the analyst is to understand the data, which involves “troubleshooting” confusing statistical output, modifying statistical models and their underlying assumptions, and perhaps even inventing new ones. As some background, I am a computational cognitive psychologist, with a little training in econometrics, so I tend to favor regression and simulation approaches, and my examples may default to examples common in the social sciences. Disclaimer: For students taking BT1101, please refer to these notes only if you are taking this course under me. If you are taking the course under a different instructor, that instructor’s lecture notes take precedence as to whether something is in syllabus or not (and hence, testable on assessments/exams). We are always making improvements to the syllabus, and so for different offerings of the course, instructors may cover slightly different material. So if you are taking it under a different instructor, do not assume that concepts covered here will show up on the exam, or assume that concepts not covered here will not show up on the exam. I’ve indicated sections that were covered the last time I taught BT1101 with a BT1101 label). This is a work-in-progress that is inspired by Russ Poldrack’s Psych10 book here: http://statsthinking21.org/, which is another undergraduate Introduction to Statistics course. This set of notes is hosted on GitHub and built using Bookdown. Feedback can be sent to dco (at) comp (dot) nus (dot) edu (dot) sg. This material is shared under a Creative Commons Attribution Share Alike 4.0 International (CC-BY-SA-4.0) License. What this means is that you are free to copy, redistribute, and even adapt the material in this book, in any format or for any purpose, even commercial. Basically, this is a freely-available educational resource that you can share and use. The only conditions are (i) you must give appropriate credit, and if you made any changes, you must indicate so and not in any way that suggests that I endorse your changes, and (ii) if you transform or build upon the material here, you must also distribute this contributions under the same license. About the Author Desmond Ong is an Assistant Professor in the Department of Information Systems and Analytics, in the School of Computing at the National University of Singapore. He also holds a concurrent appointment as a Research Scientist at the Institute of High Performance Computing (IHPC), A*STAR. Professor Ong received his Ph.D. in Psychology and his M.Sc. in Computer Science from Stanford University, and his undergraduate degrees in Economics and Physics from Cornell University. "],
["outline-of-notes.html", "Outline of notes", " Outline of notes Getting Started R, RStudio, R Markdown Coding Best Practices Introduction Handling Data Basics of Data Wrangling BT1101 Wrangling in the tidyverse BT1101 A data cleaning pipeline for research projects Advanced [In Progress] Descriptive Statistics [In Progress] Data Visualization The Linear Model Linear Regression BT1101 Logistic Regression BT1101 [In Progress] Interactions BT1101 [In Progress] Model Selection BT1101 [In Progress] Mixed-effects linear models Advanced Introduction to Time Series Data BT1101 Time Series Basics Smoothing-based models [In Progress] Time-Series Regression [In Progress] Data Mining BT1101 Optimization BT1101 Linear Optimization Integer-valued Optimization … "],
["getting-started.html", "Getting Started", " Getting Started BT1101 This course will be taught in R, which is a free statistical software environment (/ programming language) that makes it very handy to run data analysis. It is a fully functional programming language, and by that I mean two things: In my experience, any operation that you want to do to data, you can do it in R. There are also many user-contributed libraries (called “packages”), which continually expand the capabilities that you can achieve with R. In particular, I would like to highlight innovative tools like Jupyter notebooks and R Markdown documents which allow incredible flexibility in setting up a data analysis pipeline. For programming language enthusiasts, R is also a functional programming language. There are some neat tricks that you can do by exploiting R’s more advanced functional capabilites. In particular, the R community is very active, which is another plus as it allows users to easily find answers online (e.g., by searching and asking on Stack Overflow). How to download R and RStudio R is free, and can be downloaded from the R Project website. We will be using RStudio, which is an excellent development environment that makes it much friendlier to use R. You can download the free Desktop version at the RStudio website. There are many online resources for learning both R and RStudio. For example, the RStudio website has several Primers covering many relevant basic data science topics. There are also freely available books on how to use R, like R for Data Science. How to use R Markdown R Markdown documents are an extremely useful tool that professional data scientists and business analysts use in their day-to-day work. They allow for R code (and the output of R code) to be neatly formatted (using Markdown) into various output types, such as PDFs, HTML, or Microsoft Word Documents (note, for MS Word, you’ll need MS Word installed on your system). This process is called Knitting. If you open a R Markdown document (.rmd) in RStudio, RStudio should automatically detect it and you should see a little command called Knit at the top of your source window. (If this is your first time, you may have to install a few packages like knitr; RStudio will helpfully suggest to install them.) R Markdown is nice because it allows you to embed code and writeup into the same document, and it produces presentable output, so you can use it to generate reports from your homework, and, when you eventually go out to work in a company, for your projects. Here’s how you embed a “chunk” of R code. We use three apostrophes to denote the start and end of a code chunk ``` {r, example-chunk-1, echo=TRUE, eval=FALSE} 1+1 ``` which (after removing the space between the apostrophes and the {…} on the following line) produces: 1+1 After the three apostrophes, you’ll need r, then you can give the chunk a name. Please note that NAMES HAVE TO BE A SINGLE-WORD, NO SPACES ALLOWED. Also, names have to be unique, that is, every chunk needs a different name. You can give chunks names like: chunk1 read-in-data run-regression or, what will help you with homework: q1a-read-in-data q1b-regression These names are for you to help organize your code. (In practice it will be very useful when you have files with thousands of lines of code…). Names are optional. If you do not give a name, it will default to unnamed-chunk-1, etc After the name of the chunk, you can give it certain options, separated by commas. Here are some important options: echo=T / echo=TRUE or echo=F / echo=FALSE: whether or not the code chunk will be copied into the output file. eval=T / eval=TRUE or eval=F / eval=FALSE: whether or not the code chunk will be evaluated. If you set one chunk eval=F, it will be as if you commented all the code in that chunk; they just won’t run. This can be super useful, if e.g. you have some analyses that take up more time, and you don’t want to run them every single time you knit, so you can just comment them out using this one liner. There is a lot to syntax to learn using the R Markdown. And lots of cool stuff too (e.g., interactive HTML documents). In real work environments, one very applicable use-case for the R Markdown document is to generate regular reports using different data. For example, let’s say every week you get some data (weekly sales) from another team. When you get a new data file, you could just edit one line of the R Markdown file (which data file it’s reading in), and you can generate the exact same report that you did last week, just that now it’s on this week’s data. Exact same analysis, exact same graphs, just with new data. And it’s professional enough to show your manager. Note about working directories in R Markdown. If you do not specify your working directory via setwd('...'), and you hit Knit, the document will assume that the working directory is the directory that the .rmd file is in. Thus, if your rmd is in XYZ/folder1/code.rmd and your dataset is XYZ/folder1/data.csv, then you can simply read in a data file using the local path: d0 &lt;- read.csv('data.csv') without running setwd(). "],
["coding-best-practices.html", "Coding Best Practices", " Coding Best Practices Ensuring a Reproducible workflow A very important concept in statistical data analysis (and in programming and science in general) is the concept of a reproducible workflow. What this means is that, given your code, anyone (including your future self) should be able to reproduce your results exactly. To be more specific, imagine this scenario: You are a data analyst at a company, and your manager tells you “hey, I really liked those graphs you made for your presentation last week, can you make these important changes and send it to me by lunchtime? I need to brief the C-Suite at 1pm”. You get yourself a coffee, sit down at your machine, fire up your RStudio, and open up the file that you worked on last week late into the night. You try to run your script again, and to your horror, you find numerous errors. You try to reconstruct your thought processes that night, as you click into this menu and that menu. “Did I click this checkbox or not when I imported the data?”. “Did I import data_version1 or data_version2?”. “How come this function is giving me a different result now? Or is throwing an error now when it didn’t last week?” If you are lucky you are able to retrace every click you did. If not, you’ll have to tell your boss that you can’t reproduce that graph, let alone make the changes. Regardless, there will be a lot of mental and emotional cost that was unnecessary. Now imagine this other scenario: You fire up your RStudio, hit a button, and your code automagically produces a nicely formatted report with the graph you made last week. You go into your code, modify a few lines to make the changes your manager wants. You run your code again, get the new graph out, send it to your manager, and happily move on to your next task. Consider this last scenario: Your manager asks you for some additional analysis on James’ project, but James had left the company last week. James had given you all his code, neatly organized. You plop James’ code into your RStudio, and hit “run”. It magically works! Or, maybe it doesn’t. Which world do you want to be in? By the way, James doesn’t even have to have left the company — he could be a colleague in a remote team, or on leave that week, or otherwise indisposed to do that analysis, and you have to pick up where he left off. These types of situations happen all the time. Isn’t it much better for everyone to have a workflow that works again and again? What is a reproducible workflow What a reproducible workflow means is: You should write every part of your data analysis pipeline into code. This includes setting your working directory. It is very handy to go to the point-and-click interface to set your working directory, since your file may reside deep in many other folders. For example, on Mac, I can click Session -&gt; Set Working Directory -&gt; To Source File Location. After you click this, RStudio will output a setwd(...) command in the console. For example: setwd(\"~/stats-notes\"). Take this line, and copy it into the preamble of your R script, so that when you next open the file, you know which is your working directory and you can just set it by running that line. Note: R Markdown has a different way of handling working directories. When you knit a Rmd file, it automatically assumes the working directory of the source file. This also includes reading in your data. Do NOT use the Import Dataset from the dropdown menu. DO Use read.csv(...) or read.xlsx(...) or equivalent commands to read in your datasets. This also includes loading libraries and packages. Do NOT use the Packages pane of RStudio to click and load the packages you require for your analysis. DO use library(...) to load in packages. For more advanced analysts (including graduate-level researchers preparing publication-quality analyses), you may also want to consider: Putting all your data processing / data cleaning steps into code. So your code should take in the raw datafile (as raw as possible) and manipulating it only in R. (NEVER EXCEL). If data processing takes too long to run every time, you could output a “cleaned” datafile and just read that in everytime you do analysis. But you should still save the data processing code just in case you need to modify it or check what you did. This can be saved either in another R script or in a chunk in an R Markdown file with eval=F so that it won’t run every single time. Putting any exclusion criteria into code. This also allows you to modify your exclusion criteria if you needed to. DO NOT SAVE .RDATA! A pre-requisite for a reproducible workflow is that everytime you fire up RStudio, it should be a clean session, with no data loaded and no libraries loaded. This means that you should NOT save or reload your session. If RStudio ever prompts you to save your session (to a .RData file) upon exit, do not accept. You can customize RStudio to never do this by going into Preferences (see screenshot below for Mac OSX) and making sure: the Restore .RData into workspace at startup is unselected. the Save workspace to .RData on exit dropdown menu is set to Never. Building habits to ensure a reproducible workflow takes time, but you will thank yourself later for this. It’s never too early to start, and in fact, the earlier the better, so you do not have to unlearn bad habits. It’s just like, for programming, learning programming conventions: why do people name their variables like this or structure their code like that? After you become much more proficient in programming, you’ll see why it helps to make your code so much more efficient. This is the same with reproducibility. Other Coding Best Practices (This will be a growing list that I will add to.) Try not to re-use variable names or data frame names. I highly recommend NOT doing something like: d1$score = d1$score * 100. Why? Well, if you accidentally run that line of code again, your score will be 100x higher than you wanted it to be. Try having longer and more informative names. Do NOT write something like: mean = mean(d1$score). Why? Doing this will overwrite mean() (the function that calculates the average) in the namespace with mean (the variable that you just created). This means (haha) that you will not be able to calculate any more means because typing mean would then reference the newly-created variable, not the function you had in mind. Instead, try to be more informative meanScore = mean(d1$score). If ever in doubt, just type the variable name into the console to see if it’s already used. If it is, it’ll print out a function or some data. If not, it’ll say Error: object 'x' not found. Do not use very common letters like x, y, i, j. Especially as counter variables. For example, what’s wrong with the following code? x = mean(d1$score) ... for(x in 1:10) { print(d1$score[x]) } Then later when you want to print out the mean score, you’ll find that x is 10. And not what you expected. I personally recommend using longer variable names, even for counter variables, and again, never re-using them. Analysis Best Practices (This will be a growing list that I will add to.) "],
["intro.html", "Chapter 1 [Not Done:] Introduction", " Chapter 1 [Not Done:] Introduction Introduction intro example Models are always wrong. Detective analogy "],
["handling-data.html", "Chapter 2 Handling Data", " Chapter 2 Handling Data This Chapter will discuss some basic details of handling data. We will go over different types of data representations, and how to go between them using R (especially using the tidyverse set of packages). The learning objectives for this chapter are: Readers should be able to distinguish what is wide-form data and what is long-form data. Readers should be able to convert wide-form data to long-form, and vice versa (e.g., using tidyverse). # Load the libraries we&#39;ll use in this chapter library(pander) # for displaying tables library(tidyverse) # for data munging ## tidyverse encompasses both tidyr and dplyr, as well as ggplot2 and many others ## library(tidyverse) is equivalent to: # library(tidyr) # library(dplyr) # library(ggplot2) "],
["basics-of-data-wrangling.html", "2.1 Basics of Data Wrangling", " 2.1 Basics of Data Wrangling BT1101 Data wrangling, or data munging, is the process of “wrangling” data from one format to another. Let us imagine that you are interested in the heights of every student when they enter school for the first time at Primary 1. You spend time recording the heights and gender of each student. One very obvious format to store the data is to record your results in a three-column table, with name, gender, and height (in centimeters) being the three columns. Name Gender Height Angela Female 116 Brian Male 110 Cathy Female 121 This is called wide-form data, where each observation is a single row, and each variable is in one column. This is a very common way of representing data, and is very useful for plotting variables. For example, when we have data in this format, we can easily call a function to represent the heights of Male vs. Female students using a boxplot, like below (see Data Visualization chapter for more on the boxplot and other visualizations). Now, imagine that the following two years you get a chance to repeat the study with the same set of students. You collect their heights again at Primary 2, and at Primary 3. You decide to add two new columns, as follows: Name Gender Height_Pri1 Height_Pri2 Height_Pri3 Angela Female 116 120 127 Brian Male 110 116 123 Cathy Female 121 125 131 This is still wide-form data, as each student’s data is still only in one row, and you have merely added two new variables as new columns. With this wide-form data, you could still do things like calculate the correlation between Height_Pri1 and Height_Pri2 using cor(df_wide$Height_Pri1, df_wide$Height_Pri2), or plot a scatterplot between their heights in Primary 1 and Primary 3, like so: But now if you wanted to do something like look at the change of height over time, this representation may not be the easiest to do that type of analysis. Ideally we would want a case where each row had Primary School Level and Height, perhaps like: Name Gender School_Level Height Angela Female 1 116 Angela Female 2 120 Angela Female 3 127 Brian Male 1 110 Brian Male 2 116 Brian Male 3 123 This is called long-form data, where each response is in one row. This also means that each student, because they were measured three times, would appear in three rows. If you notice above, the student Angela, appears three times. A more general definition of long-form is one where each row contains a key-value pair. The value is the actual value of the variable (e.g., the value of “Height” in centimeters), while the key gives you the variables that are associated with that value (e.g. the “School Level” that the height was measured at). This data representation makes it easier to do other kinds of modelling, for example to model how Height increases over time with School Level. It also allows easier plotting such as the one below, whereby each student’s height trajectory is plotted (as separate lines) against time on the horizontal axis. (Note that this is an example of longitudinal or repeated measures data, which requires advanced multilevel modelling to properly model.) In the next section we’ll learn a little bit about the functions needed to move between wide and long-form data. "],
["wrangling-in-the-tidyverse.html", "2.2 Wrangling in the tidyverse", " 2.2 Wrangling in the tidyverse BT1101 (Here’s a great reference, or “cheat sheet”, that is very useful to refer to once you have learnt the tidyverse functions and want to refer back to it, although it’s a little outdated: https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf. Do note that pivot_longer(), covered below, is an updated version of gather() from the pdf.) In this section we’ll briefly discuss using functions from the tidyverse package, which is a very useful package for handling data. It is a very rich package (with an accompanying book), with too much content to cover here, so we’ll only introduce some very basic functions. If you would like to follow along, use the following code to generate the toy data that we’ll use in this example. df_wide1 = data.frame( Name = c(&quot;Angela&quot;, &quot;Brian&quot;, &quot;Cathy&quot;, &quot;Daniel&quot;, &quot;Elaine&quot;, &quot;Frank&quot;, &quot;Grace&quot;, &quot;Ben&quot;, &quot;Chloe&quot;, &quot;James&quot;, &quot;Natalie&quot;, &quot;Matthew&quot;, &quot;Samantha&quot;, &quot;David&quot;, &quot;Vivian&quot;, &quot;Joseph&quot;, &quot;Lisa&quot;, &quot;Mark&quot;, &quot;Jane&quot;, &quot;Jason&quot;), Gender = rep(c(&quot;Female&quot;, &quot;Male&quot;), 10), Height = c(116, 110, 121, 117, 111, 114, 127, 116, 121, 116, 126, 113, 122, 113, 116, 115, 123, 122, 115, 118) ) set.seed(1) df_wide = data.frame( Name = df_wide1$Name, Gender = df_wide1$Gender, Height_Pri1 = df_wide1$Height, Height_Pri2 = df_wide1$Height + ceiling(rnorm(20, 5, 2)) ) df_wide$Height_Pri3 = df_wide$Height_Pri2 + ceiling(rnorm(20, 5, 2)) The %&gt;% operator First, we’ll introduce the %&gt;% operator, which I refer to as the pipe operator1. The %&gt;% operator ‘pipes’ the argument before it, into the function after it, as the first argument (by default). A %&gt;% function is equivalent to calling function(A). The variable before %&gt;% is by default the first argument into function(), and you can list other arguments too. A %&gt;% function(B, C) is equivalent to calling function(A,B,C). The nice thing about the %&gt;% operator, is that you can chain many operations together, while maintaining readability of the code. For example, A %&gt;% step1(B,C) %&gt;% step2(D) %&gt;% step3() means: Take A, apply Step1 (with other arguments B, C), and then apply Step2 with additional argument D, and then2 apply Step3. Overall, this is much easier to read as the code reads sequentially from left to right, as the code “happens”. Contrast this with: step3(step2(step1(A,B,C),D)). NOTE: if you like, you can pipe arguments into the function at a specified position, using .. For example, B %&gt;% function(A, ., C) is equivalent to calling function(A, B, C). Wide-to-long: pivot_longer() The function to transform wide data to long data is pivot_longer() (Documentation). Its first argument, like a lot of the tidyverse functions, is the data frame that we want to manipulate, and is handled by the %&gt;% operator. Recall that the wide-form data looks like the following, with the columns we want to modify in bold: Name Gender Height_Pri1 Height_Pri2 Height_Pri3 Angela Female 116 120 127 Brian Male 110 116 123 Cathy Female 121 125 131 The second set of arguments to pivot_longer() are the columns we want to modify. In this case, they are “Height_Pri1”, “Height_Pri2”, and “Height_Pri3”. Note that these need to be concatenated into a single vector, so use c(...) to concatenate them. The last two arguments that we need, which are named arguments, are names_to and values_to, which give the names of the ‘key’ column and the ‘value’ column in the output long-form data frame. Let’s try the following, where I pipe df_wide into a pivot_longer() command, and save the output to a new data frame called df_long_1 df_long_1 = df_wide %&gt;% pivot_longer(c(&quot;Height_Pri1&quot;, &quot;Height_Pri2&quot;, &quot;Height_Pri3&quot;), names_to = &quot;Variable&quot;, values_to = &quot;Height&quot;) Let’s take a look at what this produces: # output several selected rows pander(df_long_1[c(1:6),], emphasize.strong.cols=3:4) Name Gender Variable Height Angela Female Height_Pri1 116 Angela Female Height_Pri2 120 Angela Female Height_Pri3 127 Brian Male Height_Pri1 110 Brian Male Height_Pri2 116 Brian Male Height_Pri3 123 This looks close! Notice how the names_to and values_to arguments became the names of the columns? Note also how “Name” and “Gender” variables get copied automatically? mutate() The last step we want to do to clean this up is to rename the Variable column and the variables in that column. For example, we want to change Height_Pri1 to something more readable, like maybe the number 1. But instead of renaming, I want to introduce the function mutate() which creates new variables. mutate()’s first argument is the data frame, which again is handled by %&gt;% mutate()’s subsequent arguments follow the format new_variable = operation(). You can also stack many such operations to create many variables at the same time. For example, mutate(newVar1 = operation(), newVar2 = operation(), newVar3 = ...) Let’s use the factor() operation to create a new factor using the values in the Variable column. We specify the levels of the factor as the original values of the Variable column, and then we use labels to rename what these values will be called in the new variable. Specifically, let’s use mutate() to make a new variable called School_Level, which will just have values of 1, 2, 3 to refer to Primary 1, 2 or 3: df_long_2 = df_long_1 %&gt;% mutate(School_Level = factor(Variable, levels=c(&quot;Height_Pri1&quot;, &quot;Height_Pri2&quot;, &quot;Height_Pri3&quot;), labels=c(1, 2, 3))) This produces: Name Gender Variable Height School_Level Angela Female Height_Pri1 116 1 Angela Female Height_Pri2 120 2 Angela Female Height_Pri3 127 3 Brian Male Height_Pri1 110 1 Brian Male Height_Pri2 116 2 Brian Male Height_Pri3 123 3 Which is great, we now have School_Level, which is a lot more readable than Variable. Finally, we can use select() to choose the names of the columns that we want to keep. Let’s just keep School_Level instead of Variable, since they have the same information (We can also use this function to re-order the columns; note the order in the following function call). df_long_3 = df_long_2 %&gt;% select(Name, Gender, School_Level, Height) Which finally produces what we want: Name Gender School_Level Height Angela Female 1 116 Angela Female 2 120 Angela Female 3 127 Brian Male 1 110 Brian Male 2 116 Brian Male 3 123 Thus, putting it all together, we can perform all the previous steps using: df_long = df_wide %&gt;% pivot_longer(c(&quot;Height_Pri1&quot;, &quot;Height_Pri2&quot;, &quot;Height_Pri3&quot;), names_to = &quot;Variable&quot;, values_to = &quot;Height&quot;) %&gt;% mutate(School_Level = factor(Variable, levels=c(&quot;Height_Pri1&quot;, &quot;Height_Pri2&quot;, &quot;Height_Pri3&quot;), labels=c(1, 2, 3))) %&gt;% select(Name, Gender, School_Level, Height) ## equivalent to: # # df_long_1 = df_wide %&gt;% pivot_longer(c(&quot;Height_Pri1&quot;, &quot;Height_Pri2&quot;, &quot;Height_Pri3&quot;), names_to = &quot;Variable&quot;, values_to = &quot;Height&quot;) # # df_long_2 = df_long_1 %&gt;% # mutate(School_Level = factor(Variable, # levels=c(&quot;Height_Pri1&quot;, &quot;Height_Pri2&quot;, &quot;Height_Pri3&quot;), # labels=c(1, 2, 3))) # # df_long_3 = df_long_2 %&gt;% select(Name, Gender, School_Level, Height) # Long-to-wide: pivot_wider() Finally, let’s try to go backwards, from a long-form data frame to a wide-form data frame. Let’s assume we start with df_long made from the previous section, and we want to spread it back to a wide format. The relevant function is pivot_wider(). (Documentation) The first argument is the data frame, and is handled by %&gt;%. The second argument is the id columns; these are the variables that identify the observation. In this case, it is the Name and Gender columns, since they stick with each observation. Then, we have names_from and values_from, which specify the names of the columns that we want to take the values from to spread them out. Let’s try: df_wide_test1 = df_long %&gt;% pivot_wider( id_cols = c(&quot;Name&quot;, &quot;Gender&quot;), names_from = &quot;School_Level&quot;, values_from = &quot;Height&quot;) Which gives us: Name Gender 1 2 3 Angela Female 116 120 127 Brian Male 110 116 123 Cathy Female 121 125 131 Great! We got back a wide-form data frame. But notice how the columns are now labelled 1, 2, 3, and this makes it hard to understand what they mean? We’ll leave it as an exercise to the reader to try to convert these back into something more understandable. (There are several possible solutions!) As it behaves similar to other pipe operators in Python and Unix, the creator pronounces %&gt;% as “and then”: Source: https://community.rstudio.com/t/how-is-pronounced/1783/12↩︎ That’s why some people call %&gt;% “and then”↩︎ "],
["a-data-cleaning-pipeline-for-research-projects.html", "2.3 A data cleaning pipeline for research projects", " 2.3 A data cleaning pipeline for research projects Advanced In research, we often collect data, and the “raw” data that we collect usually cannot be analyzed as it is. Firstly, it may contain identifiable information that needs to be removed carefully. Secondly, we may have to do additional calculations to extract our dependent/independent variables from the data. The golden rule is to never touch the raw data. Everything should be done programmatically, and the output of any processing should not overwrite the raw data. This is to allow us to ‘retrace’ our analysis steps. The only exception I can think of to touch the raw data is if for some reason, some of the raw data needs to be deleted. For example, if a participant from a study decides to withdraw, they are entitled to have their raw data deleted. Think of the following pipeline: Raw data. Contains identifiable information. Should be kept under the strictest data protection (e.g., password protection, access control only to limited people). Deidentified data. This is raw data with any identifiers stripped. Deidentified data can then be safely analysed by more people, like research assistants. Usually we replace the identifiers with a “random key”, and then we have some file that allows us to match the original identifiers with the random key. For example, we might decide to replace “John Smith” with “ID001”. In that case we have to create a new file that contains this information “John Smith = ID001”, and we will store this file with the raw data under the same strict restrictions. Nowadays I am of the opinion that we could use a one-way function (like a cryptographic hash function) to solve this issue. This allows us to maintain the security of the deidentified data, while minimizing the risk of the leak of the file that contains the identifiers. Processed data. This is deidentified data that has been processed. There are different types of processing that one can do: Survey items have been scored. This usually means calculating scores for the subscales, taking into account any reverse-coded items. Additional measures have been calculated. Any attention check measures, CAPTCHA, or other inclusion and exclusion criteria. Going from Raw data to Deidentified data: This should be done via a simple script, and should only have to be done once, immediately after data collection. After this is done, the raw data (and any identifier keys) should be safely stored, not to be accessed unless necessary. Going from Deidentified data to Processed data: This file can be iterated on over the course of analyses. For example, we may first be interested in some data, and start processing and analyzing those first, subsequently we may want to process more variables. Again, this should be done programatically (via an R / RMD script) so that every step can be stored, in case one needs to backtrack. Neither of the above steps should be done manually. For example, opening up the raw data file in Excel, deleting the “Name” column, and saving it as another file, is not a reproducible pipeline. Finally, you can start analyzing the processed data. "],
["not-done-descriptive-statistics.html", "Chapter 3 [Not Done:] Descriptive Statistics", " Chapter 3 [Not Done:] Descriptive Statistics Descriptive Statistics Maybe go over summarise? dplyr::summarise(iris, avg = mean(Sepal.Length)) "],
["not-done-data-visualization.html", "Chapter 4 [Not Done:] Data Visualization", " Chapter 4 [Not Done:] Data Visualization Different types of visualizations, when to choose each when bar, when line ggplot jittering "],
["the-linear-model-i-linear-regression.html", "Chapter 5 The Linear Model I: Linear Regression", " Chapter 5 The Linear Model I: Linear Regression BT1101 In the next few Chapters, we will learning about a workhorse tool of analytics: the linear model, which allows us to run linear regression models that we can use to estimate simple trends and look at how some variables in our data may affect other variables. The linear model is a key tool used in many fields, from psychology, economics, linguistics, business, as well as in some physical sciences like ecology. Thus, it is an essential part of the data scientist’s toolkit. In this Chapter, we will be going over linear regression with one or more independent variables, to predict a continuous dependent variable. We will also discuss how to interpret the output of a simple regression model, and discuss the case of categorical independent variables. The learning objectives for this chapter are: Readers should be able to understand and be able to use both simple and multiple regression to estimate simple trends. Readers should be able to interpret the output of a regression model, including regression coefficients, confidence intervals, hypothesis testing about coefficients, and goodness-of-fit statistics. Readers should be able to interpret the meaning of dummy-coded variables used to model categorical independent variables. # Load the libraries we&#39;ll use in this chapter library(ggplot2) # for plotting I’ve also made up some simulated data just for the next few sections that we will use to illustrate linear regression. Below is the code I used to generate df1: \\(X\\) is just a vector from 0 to 10, and \\(Y\\) is an affine transformation of \\(X\\) with some random noise added. set.seed(1) df1 = data.frame(X=c(seq(0,10))) df1$Y = -2 + df1$X*2 + rnorm(n=nrow(df1), mean=0, sd=1) "],
["basics-of-linear-regression.html", "5.1 Basics of Linear Regression", " 5.1 Basics of Linear Regression BT1101 Linear Regression really boils down to this simple equation. \\[ Y = b_0 + b_1 X \\] We want to predict \\(Y\\), our outcome variable or dependent variable (DV), using \\(X\\), which can have several names, like independent variable (IV), predictor or regressor. \\(b_0\\) and \\(b_1\\) are parameters that we have to estimate in the model: \\(b_0\\) is the Constant or Intercept term, and \\(b_1\\) is generally called a regression coefficient, or the slope. Below, we have a simple graphical illustration. Let’s say I have a dataset of \\(X\\) and \\(Y\\) values, and I plot a scatterplot of \\(Y\\) on the vertical axis and \\(X\\) on the horizontal. It’s convention that the dependent variable is always on the vertical axis. On the right I’ve also drawn the “best-fit” line to the data. Graphically, \\(b_0\\) is where the line crosses the vertical axis (i.e., when \\(X\\) is 0), and \\(b_1\\) is the slope of the line. Some readers may have learnt this in high school, where the slope of the line is the “rise” over the “run”, so how many units of “Y” do we increase (‘rise’) as we increase “X”. And that’s the basic idea. Our linear model is one way of trying to explain \\(Y\\) using \\(X\\), which is by multiplying \\(X\\) by the regression coefficient \\(b_1\\) and adding a constant \\(b_0\\). It’s so simple, yet, it is a very powerful and widely used tool, and we shall see more over the rest of this chapter and the next few chapters. Our dependent variable \\(Y\\) is always the quantity that we are interested in and that we are trying to predict. Our independent variables \\(X\\) are variables that we use to predict \\(Y\\). Here are some examples: Dependent Variable Y Independent Variable X Income Years of Education Quartly Revenue Expenditure on Marketing Quantity of Products Sold Month of Year Click-through rate Colour of advertisement (Note that this is an experiment!) Perhaps we are interested in predicting an individual’s income based on their years of education. Or for a company, we might want to predict Quarterly Revenue using the amount we spent on Marketing, or predict sales at in different months of the year. We can also use regression to model the results of experiments. For example, we might be interested in whether changing the colour of an advertisement will affect how effective it is, measured by how many people click on the advertisement (called the click-through rate, measuring how many people click on our ad). So we may show some of our participants the ad in blue, some in green, etc, and then see how that affects our click-through rate. So linear modelling can and is frequently used to model experiments as well. Independent Variables can be continuous (e.g., Years; Expenditure), or they can also be discrete/categorical (e.g., Month, Colour, Ethnicity). And we shall see examples of both in this chapter Similarly, Dependent Variables can either be continuous or categorical. In this Chapter we shall only cover continuous DVs, and we shall learn about categorical DVs in the next Chapter on Logistic Regression. "],
["running-a-regression.html", "5.2 Running a regression", " 5.2 Running a regression BT1101 The first step to running a regression is to be clear about what is your dependent variable of interest, and what are your independent variables. Often, this is clear from the context: As a researcher we have an objective to model or predict a certain variable — that will be the dependent variable, \\(Y\\). And we have variables that we think would predict that, and those will be our \\(X\\)’s. (Later we’ll discuss the differences between predictors, covariates, and confounders, which could all statistically affect the depenent variable.) 5.2.1 Structure your dataset The second step is to structure your data. For most Linear Regression (at least for the examples in this Chapter), we almost always want wide-form data, discussed in the earlier Chapter on data handling, where you have each row of the data frame be one observation, and you have one column for \\(Y\\) and one column for \\(X\\). (In later Chapters we shall see when we may need long-form data for other types of regression.) For example, the mtcars dataset that comes with R is an example of wide-form data. Each row is one observation (i.e., one car), and each column is an attribute/variable associated with that observation (e.g., fuel economy, number of cylinders, horsepower, etc). head(mtcars,3) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 5.2.2 Visualize The third step is to visualize your data, discussed also in the earlier Chapter on Data Visualization. For regression analyses, visualization of your data allows you to see whether there may be linear trends or non-linear trends (or no trends). Linear models assume that there exists a linear trend between \\(Y\\) and \\(X\\). If you have a non-linear trend, like the quadratic and exponential ones shown here, you may want to think about transforming some of your variables to see if you can get a linear trend before running the linear model. If you visually no trend, like the last plot above, you can confirm this lack of trend just by running a linear model. Plotting can also help with troubleshooting. For example, you’ll be able to immediately see if you accidentally have a factor instead of a numeric variable, or if you have possible outliers (like the graph on the left) or possibly missing data (like the graph on the right). Let’s plot the two variables in our toy dataset df1: Looks very linear! So we should expect to see a strong linear relationship between df$X and df$Y. Additional plotting tips If you are using ggplot() to plot your data, you can ask it to plot the best fit linear line using geom_smooth(method=\"lm\", se=FALSE). (If you don’t specify the method, it defaults to a non-linear smoothing, e.g. LOESS smoothing using stats::loess() for N&lt;1000 observations. “se=FALSE” will remove the standard error ribbon – try setting it to true to see what it gives you). For example, if I wanted to plot the best-fitting linear line to the graph above with the exponential trend, I can use the following chunk of code, which produces: ggplot(df0, aes(x=x, y=y_exp)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=FALSE) + xlab(&quot;X \\n Exponential Trend&quot;) + ylab(&quot;Y&quot;) + theme_bw() + theme(panel.grid = element_blank(), panel.background = element_blank(), axis.ticks = element_blank(), axis.text = element_blank()) Now you can see above that the line does have a significant slope (and indeed if you actually run the linear model using the code in the next section, you’ll see that there is a significant linear relationship). However, the linear line is not really the best description of the data, is it? This is why it is helpful to always plot our variables, so that we can see if we’re trying to fit a linear line to data that is better described by a non-linear relationship. Consider this other case, where we now have discrete X variables. It might be hard to see any linear trend. And in fact a lot of the data are overlapping, so it’s hard to see where the data are! This is where we have two more plotting tricks up our sleeve. First, is to jitter each point. Jittering means slightly shifting each datapoint a little to the left/right (jitter in the “width” direction), or up and down (“height”). This is mainly for visualization, we are not actually changing our data values. We do this giving the option position=position_jitter(width= XXX , height= YYY) to geom_point(), where XXX is how much is our maximum jitter on the horizontal/x direction, and YYY is how much we want to jitter on the vertical/y axis. If say we have an integer valued variable (“How happy are you” on a scale from 1-5), then I recommend a jitter of about 0.1 to 0.2. This is big enough to be seen, but small enough to not be confusing. Second tip, is to make your points slightly transparent, using the alpha = ZZZ option in geom_point(). This will help you see dense clusters of points vs. non-dense clusters. And of course, we can use geom_smooth(). So here we have: ggplot(df0, aes(x=x_discrete, y=y_discrete)) + geom_point(position=position_jitter(width=0.2, height= 0.2), alpha=0.3) + geom_smooth(method=&quot;lm&quot;, se=FALSE) + xlab(&quot;X \\n Discrete points, with jitter, alpha, and best-fit&quot;) + ylab(&quot;Y&quot;) + theme_bw() + theme(panel.grid = element_blank(), panel.background = element_blank(), axis.ticks = element_blank(), axis.text = element_blank()) And it’s much easier to see the trend! 5.2.3 Running the linear model Finally, we’re ready to run the model. And in fact, it’s one line of code. lm for linear model, and then you provide an “equation”, y~x, which is R syntax for “Y depends upon X”. The final argument is the dataframe in which the data is stored. # running a Linear Model fit1 &lt;- lm(Y~X, df1) TIP! Best practice tip: If you store all your Y and X variables in your dataframe as wide form data, you can just write lm(y~x, df1), which is very neat syntax. It’s not as clean to write lm(df1$y~df1$x), and I discourage this. After fitting the model, we can just call summary() on our lm object, in order to view the output of the model. # examining the output summary(fit1) ## ## Call: ## lm(formula = Y ~ X, data = df1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0781 -0.5736 0.1260 0.3071 1.5452 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.26117 0.46171 -4.897 0.000851 *** ## X 2.10376 0.07804 26.956 6.44e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8185 on 9 degrees of freedom ## Multiple R-squared: 0.9878, Adjusted R-squared: 0.9864 ## F-statistic: 726.6 on 1 and 9 DF, p-value: 6.442e-10 There’s a lot of information here, which we will break down in the next few sections, after a short digression to discuss how (Ordinary Least Squares) regression is solved. "],
["ordinary-least-squares-regression.html", "5.3 Ordinary Least Squares Regression", " 5.3 Ordinary Least Squares Regression BT1101 In this section we’ll be taking a quick peek behind what the model is doing, and we’ll discuss the formulation of Ordinary Least Squares regression. In linear regression, we assume that the “true” model is such that \\(Y\\) is \\(b_0\\) + \\(b_1\\)*\\(X\\) plus some “errors”, which could be due to other, unmeasured factors (omitted variables), or maybe just random noise. \\[ ``\\text{True&quot; model: } Y = b_0 + b_1 X + \\epsilon \\] In fact, within regression, we make the further assumption that the errors are normally distributed around zero with some variance. (So \\(\\epsilon \\sim \\mathscr{N}(0, \\sigma)\\)). Since we don’t know the “true” \\(b_0\\) and \\(b_1\\), we can only choose \\(\\hat{b_0}\\) and \\(\\hat{b_1}\\); using this we can compute the prediction of our model, \\(\\hat{Y}\\). We want our \\(\\hat{b_0}\\) and \\(\\hat{b_1}\\) to be as close to the “true” \\(b_0\\) and \\(b_1\\), which will also make our predictions \\(\\hat{Y}\\) as close to the actual \\(Y\\). To do this, we define the Residual or Residual Error of our model. For the \\(i\\)-th data point, the residual is the difference between the actual \\(Y_i\\) and our model predicted, \\(\\hat{Y_i}\\). \\[ \\text{Residual Error: } e_i = Y_i - \\hat{Y_i} \\] Here’s an illustration. Let’s say I start off just by drawing a green line through the origin with some upward slope. Here, the red lines illustrate the residual error; the difference between the actual value and our prediction. And to make our model better, we want to minimise the red bars. Some red bars are lower, some are higher, so let’s pivot the slope upwards. Now, we have this yellow line. It looks better, overall the bars are smaller. Now we note that all the red bars are below, so instead of pivoting, let’s move the whole line down. And finally we get the blue line here, which is the best solution to minimising the red bars. We want to minimise the residuals. How is this done? 5.3.1 Ordinary Least Squares Derivation The residuals can be positive or negative, so if we simply add the residuals up we might be cancelling out some of them. So instead of minimising the sum of the residuals, we usually choose to square the residuals and minimise the sum of squares of the residuals. (Mathematically it becomes easier to work with the square than the absolute value). So here, we have the Ordinary Least Squares Regression, where the goal is to choose \\(\\hat{b_0}\\) and \\(\\hat{b_1}\\) to minimise the sum of squares of the residuals \\(\\sum_{i} e_i^2 = \\sum_i \\left( Y_i - \\hat{Y_i} \\right)^2\\). We can do this by taking the partial derivative with respect to \\(\\hat{b_0}\\) and \\(\\hat{b_1}\\), and setting them both to 0. First, we define the following variables to simplify notation: \\[\\begin{align} \\text{Define } \\bar{Y} &amp;\\equiv \\frac{1}{n}\\sum_i^n Y_i \\\\ \\text{Define } \\bar{X} &amp;\\equiv \\frac{1}{n}\\sum_i^n X_i \\\\ \\text{Define } Z &amp;\\equiv \\sum_i \\left( Y_i - \\hat{Y_i} \\right)^2 \\\\ &amp;= \\left( Y_i - \\hat{b_0} - \\hat{b_1} X \\right)^2 \\\\ \\end{align}\\] Then we take the partial derivative with respect to \\(\\hat{b_0}\\), solve for this \\(\\hat{b_0}\\), then substitute it into the partial deriative with respect to \\(\\hat{b_1}\\): \\[\\begin{align} \\text{Partial deriative w.r.t. } \\hat{b_0} : \\; \\; \\frac{\\partial Z}{\\partial \\hat{b_0}} &amp;= \\sum_i^n -2 \\left(Y_i - \\hat{b_0} - \\hat{b_1}X_i \\right) \\\\ \\text{Setting the derivative to 0 and solving, we have: } \\; \\; \\hat{b_0} &amp;= \\frac{1}{n}\\sum_i^n Y_i - \\frac{1}{n}\\sum_i^n\\hat{b_1}X_i \\\\ \\implies \\hat{b_0} &amp;= \\bar{Y} - \\hat{b_1} \\bar{X} \\\\ \\text{Partial deriative w.r.t. } \\hat{b_1} : \\; \\;\\frac{\\partial Z}{\\partial \\hat{b_1}} &amp;= \\sum_i^n -2X_i \\left( Y_i - \\hat{b_0} - \\hat{b_1}X_i \\right) \\end{align}\\] \\[\\begin{align} \\text{Setting the derivative to 0 and substituting $\\hat{b_1}$, we have: } &amp; \\\\ \\sum_i^n X_i Y_i - \\sum_i^n (\\bar{Y}-\\hat{b_1}\\bar{X})X_i - \\sum_i^n\\hat{b_1}X_i^2 &amp;= 0 \\\\ \\sum_i^n X_i Y_i - \\bar{Y} \\sum_i^n X_i + \\hat{b_1} \\left(\\bar{X} \\sum_i^n X_i - \\sum_i^n X_i^2 \\right) &amp;= 0 \\\\ \\hat{b_1} &amp;= \\frac{\\sum_i^n X_i Y_i - \\bar{Y}\\sum_i^n X_i }{ \\sum_i^n X_i^2 - \\bar{X} \\sum_i^n X_i } \\\\ &amp;= \\frac{\\sum_i^n X_i Y_i - n\\bar{X}\\bar{Y}}{ \\sum_i^n X_i^2 - n\\bar{X}^2} \\\\ \\text{simplifying: } \\; \\; \\hat{b_1} &amp;= \\frac{\\sum_i^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{ \\sum_i^n (X_i - \\bar{X})^2 } \\end{align}\\] And we end up with the final OLS solution: \\[\\begin{align} \\hat{b_0} &amp;= \\bar{Y} - \\hat{b_1} \\bar{X} \\\\ \\hat{b_1} &amp;= \\frac{\\sum_i^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{ \\sum_i^n (X_i - \\bar{X})^2 } = \\frac{Cov(X,Y)}{Var(X)} \\end{align}\\] The good news is that \\(R\\) already does this for you. Let’s check this solution with the lm() model we fit on the previous page! Let’s do \\(\\hat{b_1}\\) first: in R, we can calculate the covariance of \\(X\\) and \\(Y\\), and divide that by the variance of \\(X\\), and save that into b1-hat: for this dataset we get 2.10. b1hat = cov(df1$X, df1$Y) / var(df1$X) b1hat ## [1] 2.103757 Following the equation for \\(\\hat{b_0}\\), we can take the mean of \\(Y\\), and subtract \\(\\hat{b_1}\\) times the mean of \\(X\\), and we get -2.26. b0hat = mean(df1$Y) - b1hat * mean(df1$X) b0hat ## [1] -2.261167 Finally let’s go back to our regression output table, which we can summon using summary(...)$coeff. fit1 &lt;- lm(Y~X, df1) # verifying the OLS solution summary(fit1)$coeff ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.261167 0.46170984 -4.897376 8.50721e-04 ## X 2.103757 0.07804321 26.956313 6.44250e-10 We can see that the Estimate of the Intercept, i.e., \\(\\hat{b_0}\\), is -2.26, and the Estimate of the Coefficient on \\(X\\), i.e., \\(\\hat{b_1}\\), is 2.10. They agree exactly! Excellent. So our lm() is really doing OLS regression. Again, since \\(R\\) does all the calculations for you, it’s not necessary to know how to derive the OLS solutions (especially with more than one independent variable \\(X\\)), but it is handy to know the intuition behind it, especially when we get to more complicated regression. "],
["interpreting-the-output-of-a-regression-model.html", "5.4 Interpreting the output of a regression model", " 5.4 Interpreting the output of a regression model BT1101 In this section we’ll be going over the different parts of the linear model output. First, we’ll talk about the coefficient table, then we’ll talk about goodness-of-fit statistics. Let’s re-run the same model from before: fit1 &lt;- lm(Y~X, df1) summary(fit1) ## ## Call: ## lm(formula = Y ~ X, data = df1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0781 -0.5736 0.1260 0.3071 1.5452 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.26117 0.46171 -4.897 0.000851 *** ## X 2.10376 0.07804 26.956 6.44e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8185 on 9 degrees of freedom ## Multiple R-squared: 0.9878, Adjusted R-squared: 0.9864 ## F-statistic: 726.6 on 1 and 9 DF, p-value: 6.442e-10 First, summary() helpfully reiterates the formula that you put in. This is useful to check that it’s running what you thought it ran. Call: lm(formula = Y ~ X, data = df1) It also tells you the minimum, 1st quantile (25%-ile), median, 3rd quantile (75%-ile), and maximum of the residuals (\\(e_i = Y_i - \\hat{Y_i}\\)). That is, the minimum residual error of this model is -1.0781, the median residual error is 0.1260, and the maximum is 1.5452. Residuals: Min 1Q Median 3Q Max -1.0781 -0.5736 0.1260 0.3071 1.5452 5.4.1 The coefficient table Let’s turn next to the coefficient table. summary(fit1)$coeff ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.261167 0.46170984 -4.897376 8.50721e-04 ## X 2.103757 0.07804321 26.956313 6.44250e-10 Let’s focus on the “Estimate” column. These are the point estimate of \\(b_0\\) and \\(b_1\\), for the equation \\[Y= b_0 + b_1 X\\] What do these numbers mean? \\(b_0\\): The mean value of \\(Y\\) when \\(X\\) is zero The meaning of the intercept, \\(b_0\\), is pretty straightforward. It is the average value of the dependent variable \\(Y\\) when the independent variable \\(X\\) is set to 0. (Graphically, it is the vertical intercept: the point at which the line crosses the vertical axis.) \\(b_1\\): According to the model, a one-unit change in \\(X\\) results in a \\(b_1\\)-unit change in \\(Y\\) The coefficient on \\(X\\), \\(b_1\\), captures the magnitude of change in \\(Y\\), per unit-change in \\(X\\). Graphically, this is the slope of the regression line; if \\(b_1\\) is larger, the line will have a steeper slope. Conversely, if \\(b_1\\) is smaller in magnitude, the line will have a more shallow slope. If \\(b_1\\) is positive, the slope will slope upwards /, otherwise if \\(b_1\\) is negative, the slope will go downwards \\. Example: Interpreting Simple Regression Coefficients Let’s go through an example. Let’s say we fit a model to predict our monthly profit given the amount that we spent on advertising. Both Profit and Expenditure are measured in $. \\[\\text{Profit} = -2500 + 3.21* \\text{ExpenditureOnAdvertising}\\] Coefficient Interpretation (\\(b_0\\)) Monthly profit is -$2500 without any money spent on advertising. (\\(b_1\\)) For every dollar spent on Advertising, Profit increases by $3.21 Q: Why could profit be negative here? Negative (or otherwise unusual) intercepts arise all the time in linear regression. In this example, this just means that, if we spent $0 on advertising, we would still incur a negative profit of $2,500, which could be due to omitted variables such as the amount we have to spent on rent, wages, and other upkeep. Note that it is very important to be aware of the units that each of the variables, both \\(Y\\) and \\(X\\), are measured in. This will ensure accurate interpretation of the coefficients! The rest of the coefficient table The estimated value of \\(b_0\\) and \\(b_1\\) are given in the first column (Estimate) of the coefficient table. Next to the estimates, we have the standard error of \\(b_0\\) and \\(b_1\\), which gives us a sense of the error associated with our estimate. In the third column, we have the t value. This is the t-statistic for a one-sample t-test comparing this coefficient to zero. That is, it is the one-sample t-test for the null hypothesis that the coefficient is zero, against the alternative, two-sided hypothesis that it is not zero: \\[ H_0: b_j = 0 \\\\ H_1: b_j \\neq 0 \\] In fact, the t value here, is simply the Estimate divided by the Standard Error. (You can check it yourself!) So with this t value, and the degrees of freedom of the model, we can actually calculate the p value for such a t test. R helpfully does this for you, and this is given in the fourth column, Pr(&gt;|t|). We can see that these numbers in this example are quite small, so both \\(b_0\\) and \\(b_1\\) are statistically different from zero. To the right of the Pr(&gt;|t|) column, R will helpfully print out certain significance codes. If \\(p\\) is between 0.1 and 0.05, R will print a .. If \\(p\\) is less than 0.05 (\\(\\alpha\\)=5% level of significance) but greater than 0.01 (1%), R will print a single *. If \\(p\\) is less than 0.01 but greater than 0.001, R will print out two asteriks, **. Finally, if \\(p\\) is less than 0.001, R will print out three asterisks, ***. 5.4.2 Goodness-of-fit statistics Finally we’ll look at the last part of the summary output. ## Residual standard error: 0.8185 on 9 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.9878, Adjusted R-squared: 0.9864 ## F-statistic: 726.6 on 1 and 9 DF, p-value: 6.442e-10 First, note that R will helpfully print out whether or not there were observations missing in our data. (1 observation deleted due to missingness) If, for any data point, either the \\(X\\) value, or the \\(Y\\) value (or both) are missing, then R will remove that observation from the linear model, and report it in the output. This is always something useful to check: do we have an abnormally large number of missing observations that we not expect? For example, perhaps one of the variables has a large number of missing observations? Or maybe when we were calculating new variables, we did not consider certain situations, and so end up with a lot of missing variables. (Or maybe we made a typo in our code!). This is always a good safety check before proceeding further. (Note that if there are no missing observations, R will omit this line). Next, we’ll discuss a very important statistic, called the coefficient of determination, or \\(R^2\\) (“R-squared”), which is a measure of the proportion of variance explained by the model. \\(R^2\\) is a number that always lies between 0 and 1. An \\(R^2\\) of 1 means it’s a perfect model, it explains all of the variance (all the data points lie on the line. Alternatively, all the residuals are 0). The total amount of variability of the data is captured in something called the Total Sum of Squares, which is the sum of the difference between each data point \\(Y_i\\) and the mean \\(\\bar{Y}\\) (this is also related to the variance of \\(Y\\)): \\[\\begin{align} \\text{Total Sum of Squares} \\equiv \\sum_i \\left(Y_i - \\bar{Y} \\right)^2 \\end{align}\\] The amount of variability that is explained by our model (which predicts \\(\\hat{Y}\\)) is given by the Regression Sum of Squares, which is the sum of the squared error between our model predictions and the mean \\(\\bar{Y}\\): \\[\\begin{align} \\text{Regression Sum of Squares} \\equiv \\sum_i \\left(\\hat{Y_i} - \\bar{Y} \\right)^2 \\end{align}\\] And finally, the leftover amount of variability, also called the Residual Sum of Squares, is basically the difference between our model predictions \\(\\hat{Y}\\) and the actual data points \\(Y\\). This was the term that Ordinary Least Squares regression tries to minimize, which we saw in the last Section. \\[\\begin{align} \\text{Residual Sum of Squares} \\equiv \\sum_i \\left(Y_i - \\hat{Y_i} \\right)^2 \\end{align}\\] As it turns out, the Total Sum of Squares is made up of these two parts: the Regression Sum of Squares (or “Explained” Sum of Squares), and the Residual Sum of Squares (or the “Unexplained” Sum of Squares). \\[\\begin{align} \\text{Total Sum of Squares} \\equiv \\text{Regression Sum of Squares} + \\text{Residual Sum of Squares} \\end{align}\\] \\(R^2\\) basically measures the proportion of explained variance over the total variance. In other words: \\[\\begin{align} R^2 &amp;\\equiv \\frac{\\text{Regression Sum of Squares}}{\\text{Total Sum of Squares}} \\\\ &amp;\\equiv 1 - \\frac{\\text{Residual Sum of Squares}}{\\text{Total Sum of Squares}} \\end{align}\\] You can read off the \\(R^2\\) value from the field indicated by “Multiple R-squared”, i.e., ## Multiple R-squared: 0.9878, … In the output above, the \\(R^2\\) is 0.9878; this means that this model explains 98.8% of the variance. That’s really high! Now, how good is a good \\(R^2\\)? Unfortunately there’s no good answer, because it really depends on contexts. In some fields and in some contexts, even an \\(R^2\\) of .10 to .20 could be really good. In other fields, maybe we would expect \\(R^2\\)s of .80 or .90! "],
["regression-example1.html", "5.5 Examples: Simple Regression", " 5.5 Examples: Simple Regression BT1101 Here’s a simple example to illustrate what we’ve discussed so far, by using a dataset that comes bundled with R, the mtcars dataset. We can load the dataset using data(mtcars). First, let’s see what the data looks like, using head(mtcars), which prints out the first 6 rows of mtcars. data(mtcars) head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 We’re interested in particular in predicting the fuel economy (mtcars$mpg, measured in miles per gallon) of certain vehicles using its horsepower (mtcars$hp). Let’s plot what they look like on a scatterplot. ggplot(mtcars, aes(x=hp, y=mpg)) + geom_point() + theme_bw() It looks like there is a linear trend! We can see visually that as horsepower increases, fuel economy drops; large cars tend to guzzle more fuel than smaller ones. And finally, we can run the linear model using using lm(mpg ~ hp, mtcars): summary(lm(mpg ~ hp, mtcars)) ## ## Call: ## lm(formula = mpg ~ hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7121 -2.1122 -0.8854 1.5819 8.2360 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.09886 1.63392 18.421 &lt; 2e-16 *** ## hp -0.06823 0.01012 -6.742 1.79e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.863 on 30 degrees of freedom ## Multiple R-squared: 0.6024, Adjusted R-squared: 0.5892 ## F-statistic: 45.46 on 1 and 30 DF, p-value: 1.788e-07 Looking at the coefficient table, we can see that \\(b_0\\), the intercept term, suggests suggests that, according to our model, a car with zero horsepower (if such a vehicle exists) would have a mean fuel economy of 30 mpg. We can look at the coefficient on \\(X\\), which is \\(b_1 = -0.068\\). Now this means that every unit-increase in horsepower would be associated with a decrease in fuel efficiency by -0.068 miles per gallon. Now, these are quite strange units to think about: for one, we do not think about horsepower in 1’s and 2’s; we think of horsepower in the tens or hundreds, and typical cars have horsepowers around 120 (the median in our dataset is 123). Secondly, -0.068 miles per gallon sounds pretty little, given that we see in the data that the mean fuel consumption is about 20 mpg. What we can do to help our interpretation is to multiply our coefficient to get more human-understandable values. This means that, according to the model, an increase of horsepower by 100 units, will be associated with a decrease in fuel efficiency by 6.8 miles per gallon. Ahh these numbers make more sense now. Finally, we note that the \\(R^2\\) of this model is 0.6024. This means that the model explains about 60% of the variance in the data. Again, it’s hard to objectively say whether this is amazing or still needs more work, because this really depends on our desired context. One takeaway from this example is that the linear model just calculates the coefficients based upon the numbers that we, as the analyst, provide it. It is up to us to then read the numbers that are output from the model, and make sense of those numbers, and interpret the meaning of those numbers. So this means being aware of units, and also not hesitating to wonder, “hmm this isn’t right, did I code everything correctly?” "],
["multiple-linear-regression.html", "5.6 Multiple Linear Regression", " 5.6 Multiple Linear Regression BT1101 We’ve covered Simple Linear Regression, whereby “Simple” means just one independent variable. Next we’ll talk about Multiple Linear Regression, where “Multiple” just means multiple independent variables. \\[Y = b_0 + b_1X_1 + b_2X_2 + b_3X_3 + \\ldots\\] This is actually quite a straightforward extension, once we get the hang of the interpretation. We can simply extend our linear model to also include \\(X_2\\), \\(X_3\\) and so forth, and now our \\(b_1\\) is called the coefficient on \\(X_1\\) or the partial coefficient on \\(X_1\\). Here’s the most difficult part. When we interpret each partial coefficient, the value is interpreted holding all the other IVs constant. So \\(b_1\\) represents the expected change in Y when \\(X_1\\) increases by one unit, holding constant all the other variables. This is so important that I’ll say it twice more. If you really understand this point, then you know how to do multiple regression. The partial regression coefficients represent the expected change in the dependent variable when the associated independent variable is increased by one unit while the values of all other independent variables are held constant. And a third time, in different words: Each coefficient \\(b_i\\) estimates the mean change in the dependent variable (\\(Y\\)) per unit increase in \\(X_i\\), when all other predictors are held constant. Here’s an example \\[\\text{Profit} = -2000 + 2.5* \\text{ExpenditureOnAdvertising} +32*\\text{NumberOfProductsSold}\\] Coefficient Intepretation: (b0) Monthly profit is -$2000 without any money spent on advertising and with zero products sold. (b1) Holding the number of products sold constant, every dollar spent on advertising increases profit by $2.50 (b2) Keeping advertising expenditure constant, every product sold increases profit by $32 "],
["standardized-coefficients.html", "5.7 Standardized Coefficients", " 5.7 Standardized Coefficients BT1101 Let’s take a short digression to discuss standardised coefficients. In all the examples in this Chapter, we’ve seen that it’s very important to be clear about what the units of measurement are, as this affects how we interpret the numbers. For example, in \\(\\text{Income} = b_0 + b_1 \\text{YearsOfEducation}\\), we can say for a 1 unit increase in \\(X_1\\), so for one additional year of education, there is a \\(b_1\\) unit increase in \\(Y\\); so there is a $\\(b_1\\) increase in income. Unfortunately, sometimes we may have difficulty comparing \\(X\\)’s. Perhaps I have a dataset of American students and their standardized test scores on the SATs, and a corresponding dataset of Singaporean students and their standardized test scores on the O-levels3. I want to compare the datasets to predict how test scores (\\(X\\)) affect income (\\(Y\\)), but my \\(X\\)s here are on different scales. How should we compare them? One way is by using standardised coefficients4. To do so, we “standardize” each variable by subtracting its mean and dividing by its standard deviation. Then we just re-run the regression. Now, notice that I’ve replaced \\(b\\)’s with \\(\\beta\\)s, and now these \\(\\beta\\)s are unit-less. Or, to put it another way, they are in “standardized” units. By convention (although not everyone follows this), \\(b\\) are used to refer to unstandardized regression coefficients while \\(\\beta\\)s are used to refer to standardized regression coefficients. \\[\\left[ \\frac{Y-\\bar{Y}}{\\sigma_{Y}} \\right] = \\beta_0 + \\beta_1 \\left[ \\frac{X_1 - \\bar{X_1}}{\\sigma_{X_1}} \\right] + \\beta_2 \\left[ \\frac{X_2 - \\bar{X_2}}{\\sigma_{X_2}} \\right] + \\ldots\\] Note: We can choose to standardise only the IVs, or only some of the IVs. The usual convention is that all the IVs and sometimes the DV are standardised. Now, the interpretation is: When \\(X_i\\) increases by one standard deviation, there is a change in \\(Y\\) of \\(\\beta_i\\) standard deviations Example: if we run: \\[\\text{Income} = \\beta_0 + \\beta_1 \\text{Education} + \\beta_2 \\text{Working Experience}\\] as a standardized regression, and we find that \\(\\beta_1\\) = 0.5, then the interpretation is: “For every increase in education level by one standard deviation, holding working experience constant, there is an average increase in income by 0.5 standard deviations.” With standardised coefficients, the interpretation changes, everything is now in standard deviations. Sometimes, standardised coefficients makes it easier to interpret when the underlying unit is quite difficult to interpret, for example: IQ, the intelligence quotient, is actually a standardised variable itself such that 100 is the mean of the population and 15 is the standard deviation. It’s hard to come up with units of “absolute” intelligence, and so IQ is actually measured relative to others in the population. Coming back to the example of comparing the results in an American sample with the SAT and a Singaporean sample with the O-levels; even if I cannot compare 1 point on the SAT with 1 point on the O-levels, with standardized coefficients I can still ask: does a 1 standard deviation increase in SAT score have the same effect (on whatever \\(Y\\)), as a 1 standard deviation increase in O-level score? Here is some R code to ‘manually’ standardize variables and use them in a model # unstandardized lm(Y ~ X, df1) # standardized df1$X_standardized = scale(df1$X, center=TRUE, scale=TRUE) df1$Y_standardized = scale(df1$Y, center=TRUE, scale=TRUE) lm(Y_standardized ~ X_standardized, df1) ignore the fact that the SAT and O-levels are at different levels↩︎ Confusingly, when we use standardised coefficients, it is the variables that get standardised, not the coefficients.↩︎ "],
["categorical-independent-variables.html", "5.8 Categorical Independent Variables", " 5.8 Categorical Independent Variables BT1101 So far we have been dealing with continuous independent variables (\\(X\\)), (e.g. Expenditure, Years, Age, Numbers, …). In this section, we consider categorical independent variables (e.g., Gender, Ethnicity, MaritalStatus, Color-Of-Search-Button, …). Let’s consider an example modelling how Umbrella Sales depend upon Weather. \\[\\text{UmbrellaSales} = b_0 + b_1 \\text{Weather}\\] These categorical variables take on one of a small set of fixed values. Let’s assume in this simple world that weather is only Sunny or Rainy. 5.8.1 Dummy Coding Dummy Coding (the default method in R) is a method by which we create and use dummy variables in our regression model5. In this example, we can define a variable: Rainy that is 1 if Weather==Rainy, and 0 if Weather==Sunny. Rainy is called a dummy variable (sometimes called an indicator variable) We can replace Weather with the dummy variable Rainy: \\[\\text{UmbrellaSales} = b_0 + b_1 \\text{Weather} \\; \\rightarrow \\; \\color{brown}{\\text{UmbrellaSales} = b_0 + b_1 \\text{Rainy}}\\] Thus, this breaks down into two equations (technically, a piecewise equation): If Sunny, \\(\\text{UmbrellaSales} = b_0 + b_1(0) = b_0\\) If Rainy, \\(\\text{UmbrellaSales} = b_0 + b_1(1) = b_0 + b_1\\) Now we can interpret the value of these coefficients. Looking at the first equation, we can see that \\(b_0\\) is simply the average umbrella sales when it is sunny. And similarly, from the second equation, we see that \\(b_0\\) PLUS \\(b_1\\) is the average umbrella sales when it is rainy. This means that \\(b_1\\) is the difference between these equations: It is the average difference in umbrella sales when it is rainy, compared to when it is sunny. The following table summarizes these interpretations: Coefficient Intepretation: (\\(b_0\\)) Average Umbrella sales when it is Sunny (\\(b_0+b_1\\)) Average Umbrella sales when it is Rainy (\\(b_1\\)) Average difference in Umbrella sales when it is Rainy, compared to when it is Sunny (Sales when Rainy - Sales when Sunny) Q: Would you predict $b_1$ to be greater than 0 or less than 0? 5.8.2 Dummy Coding with 3 levels Let’s now consider a more complicated world in which Weather can take one of three values: Sunny, Rainy, or Cloudy. Then we can define two dummy variables, Rainy and Cloudy, Rainy = 1 if weather is rainy, 0 otherwise Cloudy = 1 if weather is cloudy, 0 otherwise We say that Sunny is the Reference Group for the categorical variable Weather. \\[\\text{UmbrellaSales} = b_0 + b_1 \\text{Rainy} + b_2\\text{Cloudy}\\] This breaks down into three equations: If Sunny, \\(\\text{UmbrellaSales} = b_0 + b_1(0) + b_2(0) = b_0\\) If Rainy, \\(\\text{UmbrellaSales} = b_0 + b_1(1) + b_2(0) = b_0 + b_1\\) If Cloudy, \\(\\text{UmbrellaSales} = b_0 + b_1(0) + b_2(1) = b_0 + b_2\\) Just like above, we can interpret the meaning of the coefficients in the following table: Coefficient Intepretation: (\\(b_0\\)) Average Umbrella sales when it is Sunny (\\(b_0+b_1\\)) Average Umbrella sales when it is Rainy (\\(b_1\\)) Average difference in Umbrella sales when it is Rainy, compared to when it is Sunny (Sales when Rainy - Sales when Sunny) (\\(b_0+b_2\\)) Average Umbrella sales when it is Cloudy (\\(b_2\\)) Average difference in Umbrella sales when it is Cloudy, compared to when it is Sunny (Sales when Cloudy - Sales when Sunny) Thus, in general, a categorical variable with \\(n\\) levels will have \\((n-1)\\) dummy variables. And the general interpretation of these \\(i\\) dummy variables are: Coefficient Intepretation: (\\(b_0\\)) Average Value of Y for the reference group. (\\(b_i\\)) Average Difference in Y for Dummy Group i compared to the reference group. 5.8.3 The Reference Group Now, when we do dummy coding, one of the groups will automatically be the reference group. The choice of reference group is not fixed. And choosing your reference group well (depending on your goals) will make your analyses more convenient and interpretable. For example, for the Umbrella Sales example (Sunny, Rainy, Cloudy), I think “Sunny” is a good reference group. Why? Or let’s say I want to see how well people react to the color of the button on my webpage. So I run an experiment with the following four buttons6: Current Button Button A Button B Button C Which should I choose to be my reference group? I think that “Current Button” should be the reference group, since that is the status quo and I am interesting in how changing the buttons would affect click-through, relative to my current button. The good news: R handles dummy coding for you using factors. You do not need to create your own dummy variables. Just run: lm(sales ~ weather, df) and if weather is a factor with n levels, R will default to creating n-1 dummy variables The bad news: R does not know your hypotheses, so it uses a heuristic for choosing the reference group. If you do not specify, R defaults to ranking the groups by alphabetical order. Thus, in the weather example, it would choose &quot;Cloudy&quot;, and in the button example, &quot;Button A&quot; as the reference group. If your variable (df$var) is a factor, you can check by using levels(df$var). The first level will be the reference group. Use relevel(df$var, &quot;desiredReferenceLevel&quot;) to adjust the reference group. (if df$var is a character string, levels() will return NULL, but if you put it into a lm(), R will treat it as categorical variable, with the alphabetically smallest string as the reference group) 5.8.4 Interpreting categorical and continuous independent variables Whenever we have categorical independent variables in a model, interpreting the coefficients has to be done with respect to the reference group, even for other continuous independent variables. Let’s add a continuous variable to our 3-weather model. UmbrellaSales is a continuous variable measured in $. Rainy and Cloudy are dummy variables (“Sunny” is the reference group) ExpenditureOnAdvertising is also a continuous variable measured in $. Let’s say we fit the following model and obtain the following coefficients: \\[\\text{UmbrellaSales} = 10 + 50 \\text{Rainy} + 20 \\text{Cloudy} + 2.5\\text{ExpenditureOnAdvertising}\\] Here’s how we interpret each of these coefficients: Coefficient Intepretation: 10 Average umbrella sales when it is sunny and $0 spent on advertising. 50 Average umbrella sales when it is rainy compared to sunny and $0 spent on advertising. 20 Average umbrella sales when it is cloudy compared to sunny and $0 spent on advertising. 2.5 When it is sunny, every dollar spent on advertising increases sales by $2.50 Aside from dummy coding (R’s default), there are other coding schemes which can be used to test more specific hypothesis, e.g. effect coding, difference coding, etc. But here we shall focus on Dummy Coding.↩︎ This may seem frivolous, but Google actually ran a lot of these tests back in the day, with different shades of blue/green/red to settle on their current “Google colors”.↩︎ "],
["assumptions-behind-linear-regression.html", "5.9 Assumptions behind Linear Regression", " 5.9 Assumptions behind Linear Regression BT1101 Linear Regression is a powerful tool, but also makes a lot of assumptions about the data. These assumptions are necessary in order for the mathematical derivations to work out nicely (e.g., we saw the nice solution to the the Ordinary Least Squares minimization problem). Thus, we have to be mindful of these assumptions, as statistical tools become less valid when there are violations of these assumptions. The following table summarizes some of the assumptions behind linear regression, and how we can check that these assumptions are met (or at least, not violated). Note that in some of these cases there are formal ways to check (e.g. Breusch-Pagan test for homoscedasticity), but for right now our goal is to build intuition, so we will explore graphical methods. Assumption How to (informally) check There exists a linear relationship between dependent variable and the independent variable Examine X-Y scatterplot – there should be a linear trend. There are no outliers Examine X-Y scatterplot and the univariate histograms for outliers Errors are normally-distributed about zero Examine the residual plot: Residuals should be normally distributed about 0 There is no heteroscedasticity / there IS homoscedasticity. i.e., Errors are independent of the independent variable Examine the residual plot: Residuals should not depend on X No or little multicollinearity Check correlation between independent variables / calculate variance inflation factor in linear model Errors are independent of each other; they are not correlated. Check for autocorrelation between errors. We already saw the first two assumptions in the previous few sections: that we expect there to be a linear relationship between the DV and the IV(s), without any outliers. The way to visually eyeball this is to simply plot the X-Y scatterplot, also known as a bivariate (“two-variable”) scatterplot. 5.9.1 Residual plots We saw earlier that the objective of Ordinary Least Squares regression is to find the line that minimizes the sum of the (squares of the) residuals \\(e_i = Y_i - \\hat{Y_i}\\). Below, we have the best-fit linear line to a (purposely) non-linear dataset. The residual errors are shown in red, going from the actual data points (\\(Y_i\\)’s) to the predicted line (\\(\\hat{Y_i}\\)’s). Now let’s instead plot the residuals \\(e_i\\) against \\(X\\). That is, instead of plotting \\(Y\\) on the vertical axis, we plot the length of the red bars \\(e_i\\) against X: Ideally, residuals should be normally distributed around 0, and not depend on X. But we clearly see from the red dots above that it is not the case here that the residuals are normally distributed around 0. In fact, there seems to be some non-linearity in the data. One solution might be to do a non-linear transformation to one of the variables, e.g. \\(\\log(...)\\), \\(exp(...)\\). Let’s try taking the logarithm of the dependent variable, e.g. \\(\\log(Y) \\sim X\\) One thing to note here is that if you just write log() in R you’ll get the natural logarithm (log base e); you’ll need to specify log10() if you want a base10 logarithm. They both achieve the same thing in terms of the purpose of the transformation here. The main difference is in interpretability; if your variable is something like GDP, then a base10 log will be more interpretable, because a base10 log of 3 is a thousand, 4 is ten thousand, and so forth. The residuals of the transformed plot look a lot better. But actually here we see a slightly different problem: notice that the errors seem to be larger for smaller values of \\(X\\): This is called heteroscedasticity, which by definition is when the residual error depends on the value of the independent variable. Having heteroscedasticity often implies that your model is missing some variables that can explain this dependence. Linear regression often assumes no or little heteroscedasticity; in other words, linear regression assumes homoscedasticity. While there are formal tests for heteroscedasticity, for now we leave it as something that we might be able to eyeball from the data. In summary, residual plots should look distributed with no discernable pattern, with respect to the independent variables (first graph below). There are many possibilities, such as having non-normal distributions (second graph below) or heteroscedastic errors (third graph below). If you run a linear model, you can easily pull the residuals using residuals(lm_object). For example, let’s take the mtcars example predicting fuel economy (mpg) using horsepower (hp). # Use mtcars, run simple lm() data(mtcars) fit1 &lt;- lm(mpg ~ hp, mtcars) mtcars$predicted &lt;- predict(fit1) # get predictions if needed mtcars$residuals &lt;- residuals(fit1) # get residuals # plotting DV against IV ggplot(mtcars, aes(x=hp, y=mpg)) + geom_point(color=&quot;blue&quot;) + geom_smooth(method=&quot;lm&quot;, se=FALSE) + theme_bw() # plotting the residuals against IV ggplot(mtcars, aes(x=hp, y=residuals)) + geom_point(color=&quot;red&quot;) + geom_hline(yintercept=0, linetype=&quot;dashed&quot;) + theme_bw() We see what looks like maybe one outlier right on the right edge of the graph, but aside from that the residuals look fine. 5.9.2 [Not done:] Multicollinearity 5.9.3 [Not done:] Autocorrelation "],
["exercises-linear-model-i.html", "5.10 Exercises: Linear Model I", " 5.10 Exercises: Linear Model I BT1101 1a) For this simple linear regression, \\[ \\text{Income} = b_0 + b_1 \\text{Years of Education} \\] We find \\(b_0\\)=-4500, \\(b_1\\)=500 (in units of $/year). What does this mean? [Negative income?!] 1b) For this multiple linear regression, \\[\\text{Income} = b_0 + b_1 \\text{Years of Education} + b_2 \\text{Years of Experience}\\] We find \\(b_0\\)=-3700, \\(b_1\\)=450, \\(b_2\\)=350 (in $/year). What do these mean? 1c) For this simple linear regression, \\[\\text{Income} = b_0 + b_1 \\text{Gender}\\] Let’s say Gender = 1 if Female, 0 if Male, and we find that \\(b_0\\)=3500, \\(b_1\\)=5 (in $). What does this mean? "],
["the-linear-model-ii-logistic-regression.html", "Chapter 6 The Linear Model II: Logistic Regression", " Chapter 6 The Linear Model II: Logistic Regression BT1101 In the previous chapter, we introduced the linear model, and showed how we can use it to model continuous dependent variables (\\(Y\\)), using a combination of both continuous and categorical indepdent variables (\\(X\\)). In this chapter we will discuss expanding our toolkit to use a different type of regression, logistic regression, to model categorical dependent variables (\\(Y\\)). For now, we only consider binary dependent variables (such as Yes/No Decisions, True/False classifications, etc), although there are also extensions to categorical dependent variable with multiple levels (e.g., multinomial regression). The learning objectives for this chapter are: Readers should understand and be able to use logistic regression to estimate categorical dependent variables (e.g., to perform classification). # Load the libraries we&#39;ll use in this chapter library(ggplot2) # for plotting "],
["basics-of-logistic-regression.html", "6.1 Basics of Logistic Regression", " 6.1 Basics of Logistic Regression BT1101 Let’s imagine that we have a dataset with individual yes/no decisions, that we want to predict. For example, we might have a dataset of individual consumer purchases on an e-commerce platform, where we want to predict a consumer’s decision to purchase a product based upon other variables such as how much they spend on the platform, what their demographics are7. Here, our dependent variable is whether or not the customer purchased the product, so just “Yes” or “No”, and we can write the model with this Purchased variable on the left hand side. Can we build a linear model to predict purchasing behaviour? \\[\\text{Purchased} \\sim \\text{Spending} + \\text{Demographics} + \\ldots\\] Unfortunately, here we cannot use linear regression, because our response variable is a binary (yes/no) variable, while the linear regression model assumes a variable with normally distributed errors. But we can use the generalized linear model, which, as its name suggests, generalizes the linear model to other types of variables. The idea is that the GLM introduces a link function that maps the actual response variable Y to what the linear model predicts. Here we’ll focus on logistic regression, which uses the logit function as its link function. Let \\(p\\) be the probability that Purchase = 1 (i.e., “Yes”)8. Then the logistic regression equation becomes: \\[\\text{logit}(p) = b_0 + b_1 X_1 + \\ldots\\] Thus, instead of predicting a continuous outcome variable \\(Y\\) directly, we instead predict the log-odds of an event occurring: \\[ \\text{logit}(p) \\equiv \\log \\frac{p}{1-p}\\] This term is called the log-odds, where \\(\\frac{p}{1-p}\\) is called the “odds” or “odds-ratio”. To be clear, the logit is defined using the natural (base-\\(e\\)) logarithm, not the base-10 logarithm. The difference between the equation for linear regression (in the previous chapter) and logistic regression is summarized in the following table: Name Equation Linear Regression \\[Y = b_0 + b_1 X_1 + \\ldots\\] Logistic Regression \\[\\text{logit}(p) = b_0 + b_1 X_1 + \\ldots\\] In fact, linear regression is a special case of the general linear model with the identity function as the link function. Another common example: we might want to predict people’s voting behaviour or willingness to support certain policies, based upon certain characteristics.↩︎ In general, \\(p\\) is the probability that the dependent variable takes on a particular value of interest (e.g., “success”). R treats binary outcome variables as factors, where the first level (e.g. 0, FALSE, No) is the base, comparison group and the second level (e.g., 1, TRUE, Yes) is the “success” group. As with categorical independent variables, you can use levels(df$var) to check which level R will use as the base group by default.↩︎ "],
["running-a-logistic-regression.html", "6.2 Running a Logistic Regression", " 6.2 Running a Logistic Regression BT1101 The syntax for running a logistic regression is almost the same as a linear regression, just that the call is glm() for general linear model, with an additional specification of family = binomial, which tells glm to run logistic regression. (Other family options produce other types of general linear regression, such as probit regression, etc.) Now, the good news is that R handles a lot of this complication for you, when it can. For example, we do not have to manually calculate the odds ourselves. All we have to do is make sure our variable is a binary factor, then we can just call glm(). Note that, just like categorical IVs, when we do logistic regression with a categorical DV, we also have a reference group, so do use the levels() function to check. Let’s generate a simple dataset with two independent variables \\(X_1\\) and \\(X_2\\), and use them to predict \\(\\text{Purchase}\\), a binary yes/no variable. # logistic set.seed(1) df2 = data.frame(x1=rnorm(20,0,5) + seq(20,1), x2=rnorm(20,5,3), Purchase = factor(rep(c(&quot;Yes&quot;, &quot;No&quot;), each=10)), levels=c(&quot;No&quot;, &quot;Yes&quot;)) levels(df2$Purchase) ## [1] &quot;No&quot; &quot;Yes&quot; ## This means that &quot;No&quot; is the `base group`, and `p` is the probability of &quot;Yes&quot;. # next, running a logistic regression via a general linear model fit_log1 &lt;- glm(Purchase ~ x1 + x2, family=&quot;binomial&quot;, df2) summary(fit_log1) ## ## Call: ## glm(formula = Purchase ~ x1 + x2, family = &quot;binomial&quot;, data = df2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.96587 -0.37136 0.00399 0.54011 1.64780 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.0054 2.4320 -1.647 0.0996 . ## x1 0.3954 0.1653 2.393 0.0167 * ## x2 -0.1281 0.3062 -0.418 0.6758 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 27.726 on 19 degrees of freedom ## Residual deviance: 14.597 on 17 degrees of freedom ## AIC: 20.597 ## ## Number of Fisher Scoring iterations: 6 The summary output looks almost the same too as a lm() call. Let’s focus on the coefficient table, and reproduce the regression equation to help us in the interpretation. \\[\\text{logit}(p) \\equiv \\log\\frac{p}{1-p} = b_0 + b_1 X_1 + b_2X_2\\] Just like in linear regression, \\(b_0\\) is the mean value of the left-hand-side when all the \\(X\\) on the right hand side are zero. Hence, \\(b_0\\) is the log-odds of the event occurring when \\(X_1\\) and \\(X_2\\) are both zero (this part is the same as what we’ve covered previously). So this means that when \\(X_1\\) and \\(X_2\\) are both zero, the log-odds of purchasing an item is -4.005. Conversely, we can also say that the odds of purchasing this item is exp(-4.005), or 0.018. This means that \\(\\frac{p}{1-p}\\) is 0.018. Next, let’s move onto \\(b_1\\). \\(b_1\\) is the expected increase in log-odds per unit increase of \\(X_1\\), holding \\(X_2\\) constant. This is the same as linear regression. And similarly, \\(b_2\\) is the expected increase in log-odds per unit increase of \\(X_2\\), holding \\(X_1\\) constant. Note that \\(b_2\\) is negative, so increasing \\(X_2\\) will decrease the log-odds. (But in this case, it’s not significant anyway) Now, let’s take some numbers to build intuition. Every unit-increase of \\(X_1\\) increases the log-odds by 0.3954. Conversely, every unit-increase of \\(X_1\\) multiplies the odds by exp(0.3954) = 1.48. i.e., the odds increase by 48% Check: When \\(X_1\\) and \\(X_2\\) are 0, the odds are exp(-4.005) = 0.0182. If we now increase \\(X_1\\) to 1, the odds are now exp(-4.005 + 0.395), or 0.0271. The odds have increased by (0.0271-0.0182)/0.0182 ~ 48% (if we kept more decimal places) For example, if \\(X_1\\) is “Number of A-list celebrities endorsing your product”, then getting one additional celebrity endorsement would, in expectation, increase each customer’s odds of purchasing your product by 48%. (not increasing probability, but odds) This table summarizes the interpretations: Coefficient Interpretation (\\(b_0\\)) Log-odds when \\(X_1\\) and \\(X_2\\) are both zero. Odds of purchasing = exp(-4.0054) = 0.018 (\\(b_1\\)) Expected increase in log-odds of event per unit-increase of \\(X_1\\), holding \\(X_2\\) constant. (\\(b_2\\)) Expected increase in log-odds of event per unit-increase of \\(X_2\\), holding \\(X_1\\) constant. The rest of the coefficient table is the similar to the lm(). However, instead of these coefficients following a \\(t\\) distribution, they follow a \\(z\\) distribution. The interpretation of the standard error, \\(z\\) values, and \\(p\\) values are similar. Thus, in this table, the coefficient \\(X_1\\) is statistically significant at the \\(\\alpha=0.05\\) confidence level (\\(p&lt;.05\\)) "],
["not-done-exercises-linear-model-ii.html", "6.3 [Not Done:] Exercises: Linear Model II", " 6.3 [Not Done:] Exercises: Linear Model II BT1101 "],
["not-done-the-linear-model-iii-interactions.html", "Chapter 7 [Not Done:] The Linear Model III: Interactions", " Chapter 7 [Not Done:] The Linear Model III: Interactions BT1101 Cover interpretation of interactions. Concept of moderation simple effect vs main effect The learning objectives for this chapter are: Readers should be able to interpret basic interactions in a regression model, including different slopes, simple effects vs. main effects. # Load the libraries we&#39;ll use in this chapter library(ggplot2) # for plotting "],
["not-done-the-linear-model-iv-model-selection.html", "Chapter 8 [Not Done:] The Linear Model IV: Model Selection", " Chapter 8 [Not Done:] The Linear Model IV: Model Selection BT1101 The learning objectives for this chapter are: Readers should be able to discuss best practices for model building and selection, and identify potential practical issues with building regression models, such as multicollinearity. # Load the libraries we&#39;ll use in this chapter library(ggplot2) # for plotting Pairwise model comparisons "],
["not-done-the-linear-model-v-mixed-effects-linear-models.html", "Chapter 9 [Not Done:] The Linear Model V: Mixed Effects Linear Models", " Chapter 9 [Not Done:] The Linear Model V: Mixed Effects Linear Models Mixed-effects linear modelling / Multilevel modelling lmer # Load the libraries we&#39;ll use in this chapter library(ggplot2) # for plotting library(lme4) ## Loading required package: Matrix Deciding which is a fixed effect and which is a random effect Do not put the same grouping factor as both a fixed and a random effect9. (You could put a continuous variable as both a fixed and random effect if you have enough data) https://biologyforfun.wordpress.com/2015/08/31/two-little-annoying-stats-detail/↩︎ "],
["time-series.html", "Chapter 10 Introduction to Time-Series Data", " Chapter 10 Introduction to Time-Series Data BT1101 In this chapter we look at time-series data, which is also another fairly common type of data. Time-series data is data for one or more variables that vary across time, which we can denote as \\(Y_t\\) to indicate the \\(Y\\) variable at time \\(t\\). In principle, we can use many different predictors in a time-series model: \\[ Y_t = b_0 + b_1 Y_{t-1} \\\\ + b_2 X_2 + ... \\] where we can use the outcome variables at previous time-steps (\\(Y_{t-1}, ...\\)), or we can also use regressors at the current (or previous) time-steps (\\(X_t, ...\\)). We’ll be covering smoothing-based time-series models (the former), as well as modelling time-series data using regression models. The learning objectives for this chapter are: Readers should be able to understand and be able to use simple time-series models, such as the Moving Average, Exponential Smoothing, and Holt-Winters models. Readers should be able to understand concepts important to time-series modelling, such as trends and seasonality, lagged analyses. Readers should be able to apply time-series variables in regression analyses. "],
["time-series-basics.html", "10.1 Time Series Basics", " 10.1 Time Series Basics First, let’s introduce some useful terminology that we can use to describe time-series data. Trend: a gradual upward or downwards movement of a time series over time. A Seasonal effect: an effect that occurs/repeats at a fixed interval Could be at any time-scales Examples: Diurnal temperature, temperature and length of day in temperate climates, “holiday season sales” that occur at specific times in the year Cyclical effects: longer-term effects that do not have a fixed interval/length Example: stock market “boom/bust cycles”. The difference between cyclical and seasonal effects is that the latter are predictable, in that they repeat at regular intervals. Stationarity: when statistical properties of the time-series (e.g., mean, variance) do not change over time. So by definition, a stationary time-series has no trend. Autocorrelation: when a time-series is correlated with a past version of itself Auto here means “with itself”, and so autocorrelation means that the signal Y is correlated with previous versions of itself. Here are some examples to motivate how time-series data is interesting: Here we have a dataset, which is in base R and which shows the monthly total of international airline passengers from 1949 to 1960. You can see that there is a gradual increase, but there is also this very stable pattern, going up and down over a year. Thus, this dataset shows a trend, and an obvious seasonal effect. This is a graph showing the historical prices of the S&amp;P500, a stock market index listed on the stock exchanges in the US, from 1990 until 2019. There are a lot of things going on, as is the case in real world data: There is a dramatic increase in price in the past 3 decades (trend), but also these large stock market fluctuations, or cycles. The first of these cycles was the dot-com bubble in the 2000s, when tech companies were getting very hot, the internet was opening up, everyone was investing heavily in dot-com companies. The nature of these bubbles, however, is not that there is no value, but that the value is overestimated and actually doesn’t match the risks or the actual return-on-investment. So when investors started realising this and when companies actually couldn’t deliver as much as they promised, the bubble popped and the stock market crashed. This second cycle here precipitated what the 2008 financial crisis, and was actually due to overvalued mortgage loans in the US. Part of the American dream was to be able to own your own house in the suburbs; banks capitalized on this and started offering generous loans to people who couldn’t really afford them, and who were in turn more likely to default on paying their loans. This resulted in a “housing bubble”. When people actually started defaulting on their loans, the banks actually started losing money and some went bankrupt, and the whole worldwide economy was impacted. Finally, here’s another time-series graph that is relevant given the Covid-19 pandemic. This screenshot, taken from CNN on Dec 30, 2020, contains the number of cases in the US up through Dec 29 of 2020. (I note that the data is publically available, while credits for this particular visualization goes to CNN.) Similar to the stock market example, there is both an increasing trend, as well as several cycles. "],
["smoothing-based-models.html", "10.2 Smoothing-based models", " 10.2 Smoothing-based models Time-series data is often quite noisy, and often we may want to “smooth” out individual variation in the data, in order to be able to see the bigger picture, like trends. First we define a simple moving average window, sometimes called a sliding window: we define a \\(k\\)-period simple moving average window as just the average of the most recent \\(k\\) elements of the time-series: \\[\\text{Window}_t = \\frac{1}{k} \\left( Y_t + Y_{t-1} + \\ldots + Y_{t-(k-1)} \\right)\\] Note that the default for early timepoints (when there’s not enough to fit in the window), is undefined, and in R is left as a missing, or NA value. There are several functions you can use to calculate these moving averages, here we use the SMA function from the TTR package: # There are many different ways to calculate simple moving averages. # Here&#39;s one from the TTR package # install.packages(&#39;TTR&#39;) library(TTR) SMA(df$y, n=3) # taking window size of 3 This graph shows a simple time-series (in black), as well as the corresponding simple moving average windows of size 2 (in red) and size 4 (in brown). Take a minute to look at how the windows “smooth” out the variation in the data. The CNN graph of covid cases on the previous page also has a seven-day moving average window overlaid on the raw data (which you can see in the faded bars). Notice how the red line makes it easier to see trends and cycles. Here’s another graph that is also related to Covid. This is a real graph of body temperature, taken twice a day. Now you might not be able to see much from this data because it’s so noisy. But if we instead took a seven day moving average, we can see that this individual’s temperature data seems to follow this repeated pattern. (And indeed, this individual happens to be a female, and this data does follow a repeated, biological seasonality.) 10.2.1 Simple Moving Average Model Now, a very simple model is to just use the window of recent values, as a prediction of the future. That is, we use the window to predict the value at the next time step: \\[\\hat{Y}_{t+1} = \\text{Window}_t = \\frac{1}{k} \\left( Y_t + Y_{t-1} + \\ldots + Y_{t-(k-1)} \\right)\\] The major assumption in this model is that Y will stay the same, at least for short time-steps into the future. This assumes stationarity. And thus the simple moving average model does not account for trends, cycles, or seasons, and that we can get an estimate of the data just by averaging out some variation in the recent past. It’s a relatively simple model that can be calculated very quickly, even using a pen and paper pad, and can be used for quick, short-range forecasting. For example, if you are running a small store and you are wondering how much of a product to stock up on, a good estimate would be the average of those products you sold over the past few weeks. 10.2.2 Exponential Smoothing Models The next model we’ll consider is the single exponential smoothing model. This model has a parameter called \\(\\alpha\\). Our prediction for time \\(t+1\\) is a weighted sum of the actual value of \\(Y_t\\) and our model’s prediction \\(\\hat{Y}_t\\). We can then subtitute in our prediction for \\(\\hat{Y}_t\\) in terms of \\(Y_{t-1}\\) and \\(\\hat{Y}_{t-1}\\), and so forth. \\[\\hat{Y}_{t+1} = \\alpha Y_t + (1-\\alpha) \\hat{Y}_{t} \\\\ = \\alpha Y_t + (1-\\alpha) \\left( \\alpha Y_{t-1} + (1-\\alpha) \\hat{Y}_{t-1} \\right) \\\\ = \\alpha \\left( Y_t + (1-\\alpha) Y_{t-1} + (1-\\alpha)^2 Y_{t-2} + \\ldots \\right)\\] Thus, we end up that our model’s prediction for the next time point, \\(\\hat{Y}_{t+1}\\), contains a weighted sum of all previous time points \\(Y_t, Y_{t-1}, \\ldots\\). Because \\(\\alpha\\) lies between 0 and 1, as you go further back in time, each value gets less and less weight. So notice that there are two main differences between the simple moving average and this: this model takes into account all past data (as opposed to a finite simple moving average window), and this model gives more weight to more recent values (as opposed to the same, uniform weight in a simple moving average window). Unfortunately, this model still doesn’t take into account trends or seasonality. We can go further and consider a double exponential smoothing model, which can handle trends. \\[\\hat{Y}_{t+k} = a_t + b_t k \\] Here we have just measured time \\(t\\), and now we want to forecast forward to some \\(t+k\\). The main idea here is that we have \\(Y_{t+k}\\) made up of two components, the “intercept” component \\(a_t\\) which “looks like” the single exponential model; and a second component, the “slope” \\(b_t\\) which will help it to account for trends. \\[a_t = \\alpha Y_t + (1-\\alpha) \\left( a_{t-1} + b_{t-1} \\right)\\] For the “slope” term, we have: \\[b_t = \\beta \\left( a_t - a_{t-1} \\right) + (1-\\beta) b_{t-1}\\] where we update the slope estimate using both (\\(a_t - a_{t-1}\\)), the current slope estimate, and \\(b_{t-1}\\), the previous slope estimate. The basic intuition is that each component is “smoothed” using the same linear combination (\\(\\alpha, (1-\\alpha)\\)) equation. The slope variable is also smoothed using a (\\(\\beta, (1-\\beta)\\)) equation. At every timestep we smooth and update our value of \\(a_t\\) and \\(b_t\\), using these parameters \\(\\alpha, \\beta\\) and our estimates at the previous timesteps. There is a further generalization of this to add a third application of exponential smoothing to model seasonality. This is called the Holt-Winters model. Luckily, R’s HoltWinters() function can do all three types of exponential smoothing. The call is: HoltWinters(x, alpha, beta, gamma, ...) The R function will allow you to specify \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\), and whichever you do not specify it’ll try to fit. Method Trend Seasonality Parameters Call Single Exponential Smoothing No No alpha HoltWinters(x, beta=FALSE, gamma=FALSE) Double Exponential Smoothing Yes No alpha, beta (trend) HoltWinters(x, gamma=FALSE) Holt-Winters Yes Yes alpha, beta (trend), gamma (seasonality) HoltWinters(x) "],
["not-done-regression-based-forecasting-models.html", "10.3 [Not Done:] Regression-based forecasting models", " 10.3 [Not Done:] Regression-based forecasting models "],
["not-done-data-mining.html", "Chapter 11 [Not Done:] Data Mining", " Chapter 11 [Not Done:] Data Mining Data Mining "],
["optimization-i-linear-optimization.html", "Chapter 12 Optimization I: Linear Optimization", " Chapter 12 Optimization I: Linear Optimization BT1101 In this chapter we will switch gears slightly to start talking about Prescriptive Analytics: that is, how to use analytics to make better decisions using data. We’ll be discussing how to define certain problems as simple linear optimization problems: optimizing a given objective function, subject to certain linear constraints. We’ll cover how to use graphs to visualise and gain more intuition as to how to solve this problem. And finally, we will cover how to use R to solve the problem, as well as do additional sensitivity analyses. The learning objectives for this chapter are: Given an optimisation problem statement, readers should be able to properly identify an objective function and relevant constraints. Readers should be familiar with linear optimisation concepts like the feasible region and finding optimal solutions as corner points of the feasible region. Readers should be able to solve simple linear optimisation problems. Readers should be comfortable in interpreting the results of the optimisation and of sensitivity analyses, and preparing a recommendation memo with justifications. Readers should understand how and when integer constraints apply to optimisation problems. Readers should know how to use Linear-Program Relaxation (LP Relaxation) to gain insight into integer-constrained problems. Readers should understand how to solve integer-constrained optimisation problems. # Load the libraries we&#39;ll use in this chapter library(ggplot2) # for plotting library(lpSolve) # for solving linear optimization problems (&#39;linear programming&#39;) "],
["what-is-linear-optimization.html", "12.1 What is Linear Optimization", " 12.1 What is Linear Optimization BT1101 Optimisation is used in every aspect of business: from operations, to finance, to HR, to marketing. Let’s imagine that you run a little bakery, and you have to decide how many of each type of product to make. You can, of course, decide your product line by saying “I like making cupcakes”, or “People really like my croissants”. But if you really want to optimize your sales and scale up product, you would have to do your own research and consider a range of factors, such as: what are the demand for each type of product, what are the costs (in terms of time and materials) associated with each type of product For example, from your “market research”, you may discover that mille-crepes are the rage now (in 2019) but they’re really effortful to make compared to normal cupcakes. So is it worth it to start branching out into this new product? When companies have factories that make thousands (or more) of products a day, one really has to use data to optimize the product mix. Note that this optimization isn’t about making the manufacturing processes faster or cheaper (that’s also a type of optimisation). Here we are interested in how much we need to produce. Some common use cases for linear optimization are: Product Mix: Deciding how much of each product to produce and sell. Product Line Planning: Deciding how much of each product to produce now vs later (e.g. targeting holiday sales season, using forecasts of supply/demand) Choosing an Investment Portfolio: we might be interested in choosing a mix of investment options that in this case might maximise our return, or perhaps minimise our risk exposure, subject to certain constraints. Optimising labor allocation: How best to schedule employees’ work shifts? Optimising transportation and supply chain: How to route deliveries to minimize wait time? How to get materials from suppliers to warehouse to distributors—very important especially for perishable goods. And many of us in today’s world use on-demand gig apps for transportation, food delivery, and so forth. These apps require lots of optimisation, and employ very complicated techniques, so the linear optimization techniques in this chapter will provide but a glimpse of this. Here are the basic steps in Linear Optimization, which we’ll go over in the next few Sections. Identify the Objective Function &amp; Decision Variables Identify the Constraints Write down the Optimization model Either: Solve graphically and/or manually Use R to solve Conduct Sensitivity Analysis Interpret results and make recommendations. "],
["objective-functions-decision-variables.html", "12.2 Objective Functions &amp; Decision Variables", " 12.2 Objective Functions &amp; Decision Variables BT1101 Your goal (or objective) in linear optimisation is to optimise an objective function. This will be in the form of maximising or minimising some quantity of interest, by choosing the values of \\(X\\), our Decision Variables. Decision variables (\\(X\\)) are the variables that you get to pick. How many croissants to make? How many stocks of company A should I hold? In the objective function, we choose \\(X\\) to maximize or minimize some quantity of interest. Some common examples are to choose \\(X\\) to: maximise profits. minimise costs / time spent minimise risk exposure Where profits, costs, time spent, risk, etc, are given by some model of the world, that you as an analyst have come up with. I should note that we have already seen similar “optimization” in earlier chapters. For example, in Ordinary Least Squares Regression, we want to choose our regression coefficients (\\(b_0\\), \\(b_1\\), \\(b_2\\)…) to minimise the sum of squared errors. Or, we may want to choose model hyper parameters that maximises our model’s accuracy (e.g., on the Validation Set) Here, the difference is that we often formulate the objective function in terms of a real-world quantity (i.e., relevant to a business context) that we want to maximise or minimise, by choosing our X’s. Jean the Farmer: Objective Function Let’s get introduced to Farmer Jean, who runs a small farm in a little valley10, who will be with us throughout this chapter. Farmer Jean has two types of crops, Parsnips and Kale. Parsnips cost $0.20 per plot to grow, and sell for $0.35 per plot. Kale cost $0.70 per plot to grow, and sell for $1.10 per plot. She has 200 plots of land, and a $100 budget. How many of each crop should she plant to maximise her profits? Here, the decision variables are quite straightforward. We are just choosing how many plots of parsnips and plots of kale to grow. Let’s call them \\(X_1\\) and \\(X_2\\) respectively. Define: X_1 = number of plots of parsnips grown X_2 = number of plots of kale grown The objective function is just maximising profits, which is the selling price - cost price, multiplied by how many of each crop she grows. So Profits \\(= (0.35 - 0.20) X_1 + (1.10 - 0.70) X_2 = 0.15X_1 + 0.40X_2\\) Objective function: Maximize Profits = 0.15 X_1 + 0.40 X_2 Sprites and the costs/profits of Parsnips and Kale are borrowed from Stardew Valley, and this example is an optimization problem that could actually arise in the game.↩︎ "],
["constraints.html", "12.3 Constraints", " 12.3 Constraints BT1101 In Farmer Jean’s case, obviously if we just wanted to maximise profits, we should just grow more and more of the profit maximising plant. But this is often not possible because of limited constraints. For example, you may not have unlimited budget, or unlimited time, or you may also need to fulfil certain criteria as part of your contract with your distributor. Formally, we represent these constraints as mathematical inequalities or equations. The inequalities can be less than, greater than, or strictly equal. On the left hand side we put a function of our decision variables, and on the right hand side we put the value of the constraint. So for example, if you have to “deliver within budget”, this means that your total cost of your decisions must be less than/equal to your budget. If you have to “deliver at least 50 units of product”, this means that number of product produced must be greater than or equal to 50. You may have to allocate working shifts to give exactly 40 hours of work per staff. So basically these constraints can be greater or less than or equal to some number. Examples Translates to: Deliver within Budget Total Cost \\(\\leq\\) Budget Deliver at least 50 units of product Product \\(\\geq\\) 50 Allocation must contain exactly 40 hours of work Working Hours \\(=\\) 40 Implicit: Products, hours of work must be non-negative Product \\(\\geq\\) 0 Hours \\(\\geq\\) 0 We also often have constraint called non-negativity constraints. For many of these decision variables that have real world meaning, they often cannot be negative. So we often specify that each of the decision variables must be greater than or equal to 0. These are important to specify explicitly, and in fact R assumes this. Jean the Farmer: Constraints Coming back to Farmer Jean, she has two types of constraints. First, she doesn’t have unlimited budget, with which she needs to pay the cost of growing each crop. So we can write this as the cost of producing parsnips times the amount of parsnips (\\(0.20X_1\\)), and the cost of producing kale times the amount of kale (\\(0.70X_2\\)), the sum of these two must be less than equal to 100. The second constraint she has is that she only has 200 plots of land. So the total amount of crops she grows has to fit into these plots. So \\(X_1\\) + \\(X_2\\) must be less than or equal to 200. And finally we have the non-negativity constraints. Name Constraints Budget Constraints \\(0.20X_1 + 0.70 X_2 \\leq 100\\) Space Constraints \\(X_1 + X_2 \\leq 200\\) Non-negativity \\(X_1 \\geq 0\\) \\(X_2 \\geq 0\\) "],
["solving-the-optimization-model.html", "12.4 Solving the Optimization Model", " 12.4 Solving the Optimization Model BT1101 Before we start, it is good practice to formally write down the optimisation model that you want to solve. This summarises all the information in the problem into one system of equations to solve. So for the Farming Example, we have: Decision Variables: \\(X_1\\) = number of plots of parsnips grown \\(X_2\\) = number of plots of kale grown Maximize Profits = 0.15 \\(X_1\\) + 0.40 \\(X_2\\) subject to: - Budget Constraints \\(0.20X_1 + 0.70 X_2 \\leq 100\\) Space Constraints \\(X_1 + X_2 \\leq 200\\) Non-negativity \\(X_1 \\geq 0\\) \\(X_2 \\geq 0\\) Any solution that satisfies all the constraints is a feasible solution. In optimization, we want to find the best / optimal feasible solution. Let us understand this problem in more detail, by graphing our constraints as regions on an \\(X_1\\)-\\(X_2\\) graph, with \\(X_1\\) on the horizontal axis and \\(X_2\\) on the vertical axis. Below we’ve plotted the two constraints: \\(0.20X_1 + 0.70 X_2 \\leq 100\\), on the left, in magenta \\(X_1 + X_2 \\leq 200\\), on the right, in orange Since these are linear constraints, we can easily plot them by defining two points for each line, and then connecting them. For example, in the left graph, we need to plot: \\(0.20X_1 + 0.70 X_2 \\leq 100\\). The first point that will be handy is where it crosses the vertical axis; in this case, \\(X_1 = 0\\), and so we can solve \\(X_2 \\leq (100/0.70)\\), or \\(X_2 \\leq 142\\) (rounding down). The second point is where it crosses the horizontal axis. Now, \\(X_2 = 0\\), and so \\(X_1 \\leq (100/0.20)\\) or \\(X_1 \\leq 500\\). Thus, we can connect the point \\(X_1=0, X_2=142\\) on the vertical axis, with \\(X_1=500, X_2=0\\) on the horizontal axis with a straight line. Then we just read the constraint direction and reason that it should include the area down and to the left of this line! Make sure you understand the right graph as well, and understand how to reproduce it! Question: What do the non-negativity constraint regions look like? Feasible Regions The Feasible Region is the intersection of all the constraint regions. So let’s plot those two regions above on the same plot. A feasible solution is an allocation of decision variables (like \\(X_1\\) = 10, \\(X_2\\) = 10), that satisfies all the constraints. It also follows, that every feasible solution will lie in the feasible region. That means that every point within the feasible region (shaded in green above) could be a solution to this problem. If there is no feasible region, then there is no feasible solution, and we say that the problem is infeasible. In other words, there is no way to solve this problem while satisfying all the constraints. Corner Points While any point in the feasible region is a possible solution, it turns out that the optimal solution, if it exists, lies at a “corner” of the feasible region. Indeed, there are mathematical theorems proving this. In the plot below we’ve indicated the four corner points for this problem: Why must an optimal solution lie at a corner? Imagine that you have a solution in the center of the feasible region, not near a boundary. From here, you can still increase or decrease \\(X_1\\); or you can increase or decrease \\(X_2\\). These changes would either increase or decrease your profit (“better” or “worse”). So the best (and worst) solutions are when you cannot increase or decrease your decision variables any more! Let’s check our profits at each of the corner points. For three of them it’s simple; we just set X1 and X2 to 0 (Bottom-Left), or we set X1 to 0 (Top-Left), or we set X2 to 0 (Bottom-Right). \\[\\begin{align} \\text{Bottom-Left }:&amp; \\; X_1 = 0 ; \\;\\;\\;\\; X_2 = 0 ; \\;\\;\\;\\; \\text{Profit} = 0 \\quad [\\textbf{Worst}] \\\\ \\text{Top-Left }:&amp; \\; X_1 = 0 ; \\;\\;\\;\\; X_2 = 142 ; \\; \\text{Profit} = 56.8 \\\\ \\text{Bottom-Right }:&amp; \\; X_1 = 200 ; \\; X_2 = 0 ; \\;\\;\\;\\; \\text{Profit} = 30 \\\\ \\end{align}\\] Note, the WORST solution is also at a corner point. Can you reason out why? In this case, the bottom-left point happens to be the worst! At the fourth corner point (the top-right point), this point satisfies both the inequalities exactly. So they become equalities, which we can solve as a pair of simultaneous equations: \\[\\begin{align} X_1 + X_2 &amp;= 200 \\quad (1) \\\\ 0.20X_1 + 0.70X_2 &amp;= 100 \\quad (2) \\\\ (1) \\implies X_2 &amp;= 200 - X_1 \\\\ \\text{sub into } (2): 0.20X_1 + 0.70 (200 - X_1) &amp;= 100 \\\\ (0.20 - 0.70) X_1 &amp;= 100 - 140 \\\\ X_1 &amp;= 80 \\\\ \\implies X_2 &amp;= 120 \\end{align}\\] Hence, the solution is \\(X_1 = 80\\), \\(X_2 = 120\\), which gives us a profit of \\(0.15X_1 + 0.40X_2 = 60\\). This happens to be the maximum profit, so (80,120) gives us the optimal solution with a maximum profit of $60. Level Sets An alternative way to visualise this solution is to draw the level set of the objective function (i.e., profits). The level set is the set of all the points that give the same profit. We indicate the direction in which we are optimising (i.e., direction of increasing profit), and we “shift” the level set as we increase profit. The optimal solution is the last point before the level set leaves the feasible region as we optimise profits. (If the level set is parallel to a constraint line, we could also get a set of optimal solutions) Solution Types There are 4 possible types of solutions: There exists a unique optimal solution. There exists multiple optimal solutions. Graphically, this occurs when the set of optimal solutions (i.e., the level set of the optimal solution) is parallel to / lies along a constraint. Thus, every point along that constraint will give an optimal solution. The solution is unbounded There exists no feasible solutions Finding corner points in higher dimensions So we’ve seen that the optimal solution must lie on a corner point. This is true even if we have more than two variables and we cannot plot this on a 2D graph. A ten decision-variable problem will have a feasible region be a region in a ten dimensional space, also called a 10-dimensional polytope. The solution will lie on a corner, or vertex, of this polytope. Image of a high-dimensional polytope and a path that Simplex Algorithm might take, from Wikipedia One algorithm to solve these optimization problems is called the Simplex Algorithm, which systematically checks through these corner vertices to find the optimal one. The algorithm starts at a vertex (perhaps chosen randomly or through some initialization) It will then move along an edge (“side of polytope”) to another vertex only if the objective function value along that edge is increasing. It repeats this until it reaches a maximum, i.e., no such edges are found. The algorithm terminates at the optimal solution, or reports that none are found (infeasible, or unbounded). For this chapter, we do not cover how the Simplex algorithm works, but we just discuss the high-level intuition behind it. We will let R implement this algorithm and solve more difficult optimization problems for us. "],
["using-r-to-solve-linear-optimization.html", "12.5 Using R to solve Linear Optimization", " 12.5 Using R to solve Linear Optimization BT1101 The most difficult part about using R to solve a linear optimization problem is to translate the optimization problem into code. Let’s reproduce the table with all the necessary information for the example of Farmer Jean: Decision Variables: \\(X_1\\) = number of plots of parsnips grown \\(X_2\\) = number of plots of kale grown Maximize Profits = 0.15 \\(X_1\\) + 0.40 \\(X_2\\) subject to: - Budget Constraints \\(0.20X_1 + 0.70 X_2 \\leq 100\\) Space Constraints \\(X_1 + X_2 \\leq 200\\) Non-negativity \\(X_1 \\geq 0\\) \\(X_2 \\geq 0\\) Here’s how you translate it into code. First, we define the objective function parameters, which are just the coefficients of \\(X_1\\) and \\(X_2\\) in the object function: Profits = 0.15 \\(X_1\\) + 0.40 \\(X_2\\) objective.fn &lt;- c(0.15, 0.40) Next, we define the constraints, which are broken up into 3 variables: the constraints matrix, the constraint directions, and the constraint values (or constraint RHS for right-hand-side). \\[0.20X_1 + 0.70 X_2 \\leq 100\\] \\[X_1 + X_2 \\leq 200\\] The constraint matrix is simply concatenating all the coefficients here into a matrix. For simplicity, using the convention below, we just read off the matrix from top to bottom, then within each line, from left to right. So we have: (0.20, 0.70, 1, 1). When I construct the matrix, you have to specify the number of columns (ncol) which is simply the number of decision variables we have; in this case it’s 2. The constraint directions are just a vector that corresponds to each constraint (\\(\\leq\\), \\(\\leq\\)), and the constraint right-hand-side values are just (100, 200) const.mat &lt;- matrix(c(0.20, 0.70, 1, 1) , ncol=2 , byrow=TRUE) const.dir &lt;- c(&quot;&lt;=&quot;, &quot;&lt;=&quot;) const.rhs &lt;- c(100, 200) The important thing to note is to get the order of all the constraints correct. In particular, if you have certain constraints that do not include ALL of the decision variables, to include 0s whenever appropriate. TIP! For example, if you have an additional constraint that you can only produce a maximum of 500 \\(X_1\\), this constraint translates to: \\(X_1 \\leq 500\\) but to be even more helpful to yourself, write it out as: \\[ 1 X_1 + 0 X_2 \\leq 500 \\] This will help you remember to put … 1,0 … into the relevant row of the constraints matrix when you are reading the matrix off from your table. Finally, we put all of these into a call to the lp function within the lpSolve package. We specify max for maximizing the objective function, pass in the rest of the parameters we just defined, and finally we also ask it to compute.sens=TRUE: we need this for the sensitivity analysis in the next section. lp.solution &lt;- lp(&quot;max&quot;, objective.fn, const.mat, const.dir, const.rhs, compute.sens=TRUE) Putting it all together, and looking at the solution (lp.solution$solution) and objective function value (lp.solution$objval) # defining parameters objective.fn &lt;- c(0.15, 0.40) const.mat &lt;- matrix(c(0.20, 0.70, 1, 1) , ncol=2 , byrow=TRUE) const.dir &lt;- c(&quot;&lt;=&quot;, &quot;&lt;=&quot;) const.rhs &lt;- c(100, 200) # solving model lp.solution &lt;- lp(&quot;max&quot;, objective.fn, const.mat, const.dir, const.rhs, compute.sens=TRUE) # check if it was successful; it also prints out the objective fn value lp.solution ## Success: the objective function is 60 # optimal solution (decision variables values) lp.solution$solution ## [1] 80 120 # Printing it out: cat(&quot;The optimal solution is:&quot;, lp.solution$solution, &quot;\\nAnd the optimal objective function value is:&quot;, lp.solution$objval) ## The optimal solution is: 80 120 ## And the optimal objective function value is: 60 Thus, the optimal solution is \\(X_1\\) = 80, \\(X_2\\) = 120, and the optimal profit is 60, which is what we found manually in the previous section. "],
["sensitivity-analysis.html", "12.6 Sensitivity Analysis", " 12.6 Sensitivity Analysis BT1101 When we get an optimal solution to a linear optimisation problem, oftentimes we may want to ask: what if we change the objective by a little, or what if we change the constraints by a little, what would happen? i.e., how sensitive is the solution to changes in the problem? Sensitivity analysis allows us to ask this question systematically. Here we shall cover two types of sensitivity analyses – varying objective function coefficients, and varying constraint values. Varying objective function coefficients Recall our farming example: our optimal solution is \\(X_1\\) = 80, \\(X_2\\) = 120, and our objective function is: \\[ \\text{Profit} = 0.15 X_1 + 0.40 X_2 \\] What if Farmer Jean’s customer decides to reduce the amount they are willing to pay for Kale (\\(X_2\\)) to $1.00 (reducing Jean’s profits from 0.40 to 0.30 per unit of \\(X_2\\)). \\[ \\text{Profit} = 0.15 X_1 + \\color{red}{0.30} X_2 \\] Will that change our optimal solution? Turns out, no, it doesn’t. (Try running the code again to verify that the optimal solution is still (80,120)!) But now if the price that Farmer Jean can sell Parsnips (\\(X_1\\)) drops from 0.35 to 0.30, i.e., reducing her profits per unit-parsnips from 0.15 to 0.10, \\[ \\text{Profit} = \\color{red}{0.10} X_1 + 0.40 X_2 \\] Then the optimal solution actually changes (to \\(X_1\\) = 0, \\(X_2\\) = 142)! Basically, Parsnips are now not profitable, and Jean should plant plant all Kale. Graphically the optimal solution now moves from the blue vertex to the green vertex. The optimal solution is to produce as much Kale as she can afford. Indeed in the first case, reducing the price of \\(X_2\\) by 10 cents did not change the solution, but in the second case, reducing the price of \\(X_1\\) by 5 cents did! So this solution is sensitive to some changes but not others. Varying objective function coefficients in R R’s lpSolve::lp() function helps you to calculate the range of coefficient values for which the given solution is still optimal. So again, recall that the original objective function is: \\[ \\text{Profit} = 0.15 X_1 + 0.40 X_2 \\] We use lp.solution$sens.coef.from and lp.solution$sens.coef.to to get the range of coefficients. # sensitivity analysis on coefficients lp.solution$sens.coef.from ## [1] 0.1142857 0.1500000 lp.solution$sens.coef.to ## [1] 0.400 0.525 The way you read this is that “from” is the lower bound of the coefficients (\\(X_1\\) and \\(X_2\\) respectively), while “to” gives the upper bound of the coefficients. This means that as long as the coefficient on \\(X_1\\) lies between [0.1143, 0.400], this solution is still optimal. Or if the coefficient on \\(X_2\\) lies between [0.150, 0.525], this solution is still optimal. If the coefficients shift outside these ranges, then the solution will move to a different vertex. (How will they move?) Note that all of these sensitivity calculations assume you only adjust one coefficient at a time. This means that if you decide to reduce \\(X_1\\) to 0.12 and increase \\(X_2\\) to 0.50, the optimal solution might change. These ranges assume that you only change that one coefficient while holding the rest constant. If you want to change two or more coefficients, you should re-run the model. Now let’s think through and try to predict what happens when these coefficients go out of these bounds. We just saw that if the coefficient on \\(X_1\\) goes to 0.10, which is less than 0.1143, then the solution moves to: produce no \\(X_1\\) and produce maximum \\(X_2\\). What if the coefficient on \\(X_1\\) goes above 0.400? What do you think will happen? Increasing the profit of \\(X_1\\) beyond this range should mean that it has become more profitable to produce \\(X_1\\), and we know that the solution must lie on a vertex, so we move down the edge of the feasible region to the solution on the horizontal axis, \\(X_1 = 200, X_2 = 0\\). Similarly, we can think about what will happen when the coefficient on \\(X_2\\) drops below 0.15: it becomes not profitable to produce Kale, so we’ll also get the point \\(X_1 = 200, X_2 = 0\\). But when the coefficient on \\(X_2\\) rises above 0.525, then it becomes much more profitable to produce Kale, so we’ll come to the solution \\(X_1 = 0, X_2 = 142\\). Varying Constraint Values (Shadow Prices) The next type of sensitivity analysis we shall look at is what happens when we vary the constraint values. We’ll introduce a new term called shadow prices. The Shadow Price of a constraint is the change in the objective function value per unit-increase in the right-hand-side value of that constraint (holding all else equal). Let’s break this down. In the Farming example, we have two constraints. Let’s consider the first constraint. What if we increased the RHS of the first constraint from 100 to 101? \\[0.20X_1 + 0.70X_2 \\leq 100 \\quad \\rightarrow \\quad 0.20X_1 + 0.70X_2 \\leq \\color{green}{101}\\] The feasible region was pushed outwards a little, by adding this green segment. Doing this will move the optimal vertex up and to the left. The new solution is (\\(X_1\\) = 78, \\(X_2\\) = 122), which gives a profit of $60.50 – an increase of $0.50 Thus, increasing the budgetary constraint by 1 unit (100 to 101) increases the profit by $0.50 (from $60 to $60.50). Hence, (by definition), the Shadow Price of the first constraint is $0.50. What this means is that if Farmer Jean increased her operating budget from $100 to $101, then she can increase her profits by $0.50. That’s a pretty good return-on-investment! (Note that the optimal solution also changes, but here the shadow price indicates the effect of changing the constraint value on the PROFIT). Next, let’s consider the second constraint. What if we increased the RHS of the second constraint from 200 to 201? \\[X_1 + X_2 \\leq 200 \\quad \\rightarrow \\quad X_1 + X_2 \\leq \\color{cyan}{201}\\] The feasible region was pushed out a little, by adding this cyan segment, which moved the optimal vertex down and to the right. The new solution is (\\(X_1\\) = 81.4, \\(X_2\\) = 119.6), which gives a profit of $60.05 – an increase by $0.05 Thus, increasing the space constraint by 1 unit (200 to 201) increases the profit by $0.05 (from $60 to $60.05). The Shadow Price of the space constraint is $0.05. What this means is that if Farmer Jean increased her land plot size from 200 to 201, then she can increase her profits by $0.05. That’s not as high as the budgetary constraints, so this translates to a good recommendation: if Jean wants to increase her profit, she gets more bang of the buck if she gets more operating budget than land space. Varying Constraint Values in R Shadow prices are also known as duals in other fields (e.g. computer science), so we use lp.solution$duals to get them. # Shadow prices lp.solution$duals ## [1] 0.50 0.05 0.00 0.00 Notice that there are four values here. The first two are the two constraints we put in, in the order they were specified in the constraints matrix (so \\(0.20X_1 + 0.70X_2 \\leq 100\\) then \\(X_1 + X_2 \\leq 200\\)). So we can see, as per our calculations above, that the shadow price of the first constraint is $0.50, and that of the second constraint is $0.05. The last two are the shadow prices of the non-negativity constraints \\(X_1 \\geq 0\\), and \\(X_2 \\geq 0\\). This means that the change in the optimal profit, if we were to increase the right hand side values from 0 to 1, are both 0. Note that in some fields, the shadow prices of the non-negativity constraints have a special name, reduced costs. But the interpretation is the same. Why would we have a shadow price of zero for these non-negativity constraints? In this particular case, in the optimal solution we are already producing non-zero quantities of \\(X_1\\) and \\(X_2\\). Thus, increasing the right-hand-side value (i.e., forcing us to produce at least 1 \\(X_1\\) or at least 1 \\(X_2\\)) will not change our optimal solution. The shadow prices are zero. Binding vs non-binding constraints In general, constraints can either be binding or non-binding for the optimal solution. Constraints that are binding ‘restrict’ the optimal solution; so in the Parsnips/Kale example, both the Budget and Space constraints are binding; if we increase the right-hand-side of the constraints, we can do better and increase our profit. Hence, they have non-zero shadow prices. Conversely, non-binding constraints do not restrict or bind the optimal solution. Thus, even if we change the right-hand-side value of the constraint by 1, we will not affect the optimal solution. Thus, shadow prices are zero for non-binding constraints. In certain cases, we might even have negative shadow prices. Let’s consider a modified objective function in the same Parsnip-Kale example, where the profit of Kale is increased to 0.53. \\[ \\text{Profit} = 0.15 X_1 + \\color{red}{0.53} X_2 \\] We saw from our earlier sensitivity analysis that this will change the optimal solution to: \\(X_1 = 0, X_2 = 142\\). Let’s check this by re-running this modified linear optimization problem, saving it as lp.solution2: # defining parameters objective.fn2 &lt;- c(0.15, 0.53) const.mat &lt;- matrix(c(0.20, 0.70, 1, 1) , ncol=2 , byrow=TRUE) const.dir &lt;- c(&quot;&lt;=&quot;, &quot;&lt;=&quot;) const.rhs &lt;- c(100, 200) # solving model lp.solution2 &lt;- lp(&quot;max&quot;, objective.fn2, const.mat, const.dir, const.rhs, compute.sens=TRUE) # check if it was successful; it also prints out the objective fn value lp.solution2 ## Success: the objective function is 75.71429 # Printing it out: cat(&quot;The optimal solution is:&quot;, lp.solution2$solution, &quot;\\nThe optimal objective function value is:&quot;, lp.solution2$objval, &quot;\\nwith the following Shadow Prices&quot;, lp.solution2$duals) ## The optimal solution is: 0 142.8571 ## The optimal objective function value is: 75.71429 ## with the following Shadow Prices 0.7571429 0 -0.001428571 0 As we expected, the solution is \\(X_1 = 0, X_2 = 142\\) (rounded down). Now, if we look at the shadow prices, we notice that the shadow price of the first constraint: \\(0.20X_1 + 0.70X_2 \\leq 100\\), is +0.75, so increasing the budget by $1 will increase profits by $0.75. The shadow price of the second constraint, \\(X_1 + X_2 \\leq 200\\), in this case, is 0. We can also see that because the total amount of land used \\(X_1 + X_2 = 0 + 142 = 142\\) is much less than the available budget, this constraint now becomes non-binding at this solution, and hence the shadow price of this constraint is zero. Getting access to more land will not help improve profits. But wait, the third shadow price returned by lp.solution2$duals is negative! What does that mean? This shadow price corresponds to the non-negativity constraint \\(X_1 \\geq 0\\). Going by the definition of the shadow price, this means that increasing the value of the right hand side of this constraint, to \\(X_1 \\geq \\color{red}{1}\\) will reduce the optimum profit by -$0.0014. This is because if this constraint were to be changed, then now we are forced to produce at least 1 unit of \\(X_1\\), which is now less profitable, and hence total profit should go down. Finally, note that this also implies that \\(X_1 \\geq 0\\) is a binding constraint, because changing the value of this constraint will affect the optimal solution. So binding constraints can have either positive, or negative, shadow prices. Summarizing sensitivity analyses Solving an optimization problem is straight-forward once you have the objective function and the constraints. Getting the optimal solution is not the end of it. As a top-notch business analyst, we can add more value to the business decision by doing and interpreting sensitivity analyses. We can examine the range of coefficients over which this solution is valid, and make recommendations. For example, we saw earlier than if the profit of Kale goes above 0.53 or profit per Parsnip goes below 0.11, then it makes more sense to switch to producing as much Kale as possible. For example, you could provide the following recommendation to Farmer Jean: “If the profit per unit-Kale goes above $0.53 or if the profit per unit-Parsnips goes below $0.11, then please switch your whole production to Kale.” Secondly, we can examine and interpret shadow prices: For example, to Farmer Jean: “If you get one additional plot of land, you can make $0.05 more profit. But if you get $1 more of budget to buy crops, you can increase your profit by $0.50 (by changing your planting allocation to …)” More broadly, as analysts we could run “what-if” analyses to study the impact of getting more resources, of changing our prices, and of many other possible business decisions. These type of analyses require much more experience, but are potentially much more valuable to making better business decisions. "],
["examples-linear-optimization.html", "12.7 Examples: Linear Optimization", " 12.7 Examples: Linear Optimization In this example, imagine that you operate a furniture company, with the following three products: Tables: Each table makes a profit of $500, costs 8.4 production hours to make, and 3\\(m^3\\) of storage to store Sofas: Each sofa makes a profit of $450 Beds: Each bed makes a profit of $580, costs 9.6 production hours to make, and 8 The profit earned by selling each type of furniture is listed above. However, each piece of furniture requires some factory-time to make and requires warehouse space to store. Each week you have a budget of only 72 production hours at your factory. Additionally, you have enough warehouse storage to store only 40 m3 (cubic-meters). Furthermore, from past trends, you can only sell a maximum of 8 tables a week. How many of each type of furniture should you make for next week to maximise profit? First, we define our decision variables and objective function: \\[ X_1 = \\text{number of tables made} \\\\ X_2 = \\text{number of sofas made} \\\\ X_3 = \\text{number of beds made} \\\\ \\text{Maximize Profit} = 500 X_1 + 450 X_2 + 580 X_3 \\] Next, we define our four constraints: Production Hour Budget Constraints: \\[ 8.4X_1 + 6X_2 + 9.6X_3 \\leq 72\\] Storage Space Constraints: \\[ 3X_1 + 6X_2 + 8X_3 \\leq 40\\] Demand Constraints: \\[ X_1 + \\qquad + \\qquad \\leq 8\\] Non-negativity Constraints: \\[ X_1 \\geq 0; X_2 \\geq 0; X_3 \\geq 0 \\] The lp code looks like this: objective.fn &lt;- c(500, 450, 580) const.mat &lt;- matrix(c(8.4, 6, 9.6, 3, 6, 8, 1, 0, 0), ncol=3 , byrow=TRUE) const.dir &lt;- c(&quot;&lt;=&quot;, &quot;&lt;=&quot;, &quot;&lt;=&quot;) const.rhs &lt;- c(72, 40, 8) # solving model lp.solution &lt;- lp(&quot;max&quot;, objective.fn, const.mat, const.dir, const.rhs, compute.sens=TRUE) And we can obtain the solution by running: # to print out the optimal objective function value lp.solution ## Success: the objective function is 4629.63 # to print out the values of the decision variables (X1, X2, X3) lp.solution$solution ## [1] 5.925926 3.703704 0.000000 Thus, the optimal solution is: \\[X_1 = 5.93, X_2 = 3.70, X_3 = 0\\] Hence, the optimal solution, given the constraints, is to produce 5.93 tables, 3.70 sofas, and 0 beds. This will yield a maximum profit of $4629.63 Note that: The optimal solution is to not produce any beds at all! How do we produce fractional tables?? (We’ll address this in the next chapter!) Q: Which constraints are binding? Which constraints are non-binding? Now let’s do some sensitivity analyses: lp.solution$sens.coef.from ## [1] 2.25000e+02 3.64375e+02 -1.00000e+30 lp.solution$sens.coef.to ## [1] 630.0000 1000.0000 681.4815 This means that the optimal solution remains the same if: The profit from tables (coefficient on \\(X_1\\)) lies between $225 and $630 The profit from sofas (coefficient on \\(X_2\\)) lies between $364 and $1000 The profit from beds (coefficient on \\(X_3\\)) lies between $ (-Infinity) and $681.48 That is, in order for the beds to be profitable, we would need to increase the profit of beds to at least $682 per bed! Now, let’s examine the Shadow prices of the solution. # Shadow prices lp.solution$duals ## [1] 50.92593 24.07407 0.00000 0.00000 0.00000 -101.48148 This returns 6 values. The first three correspond to the constraints in the order we put them in. The last three are the shadow prices of the non-negativity constraints (\\(X_1 \\geq 0\\), \\(X_2 \\geq 0\\), and \\(X_3 \\geq 0\\), respectively) Q: How would you interpret each of these values? Q: Why is the last (the 6th) shadow price non-zero? And why is it negative? "],
["linear-optimization-summary.html", "12.8 Linear Optimization Summary", " 12.8 Linear Optimization Summary BT1101 In this chapter, we covered the basics of formulating and solving simple linear optimisation problems, which help us “prescribe” the best business choices to make. If you can formulate your real-world problem as a linear optimisation problem, then there are very efficient solvers and algorithms that can solve the problem, and to help you gain further insight into your problem (e.g., via sensitivity analyses). In the models we’ve covered this chapter, we’ve assumed that all the decision variables are continuous, real-valued numbers. In the next chapter, we will discuss linear optimisation problems where some of the decision variables have to be integers, as well as when some of the decision variables are binary yes/no decisions. "],
["exercises-linear-optimization.html", "12.9 Exercises: Linear Optimization", " 12.9 Exercises: Linear Optimization BT1101 Q1 Consider the following 4 constraints Constraint A: \\(X_1 + X_2 \\leq 10\\) Constraint B: \\(0.5X_1 + 2X_2 \\leq 10\\) Constraint C: \\(X_1 - 0.5X_2 \\geq 12\\) Constraint D: \\(X_2 \\geq 7\\) Non-negativity constraints: \\(X_1 \\geq 0\\), \\(X_2 \\geq 0\\) Assume the non-negativity constraints always hold. For each of the following sets of constraints, please indicate whether they are (i) Feasible, (ii) Infeasible, or (iii) Unbounded. A+B A+C A+D B+C B+D C+D Q2 The examples we discussed in the earlier part of this chapter were all maximization problems (specifically, to maximize profit). In this question we shall explore minimisation. FunToys is famous for three types of toys: Cars, Animals, and Robots. Each year, near the holiday season, it receives large bulk orders for these items. To meet these orders, FunToys operates three small toy-making factories, A, B and C. Factory A costs $1000 per day to operate, and can produce 30 cars, 20 animals and 30 robots per day. Factory B costs $1200 per day to operate, and can produce 40 cars, 50 animals and 10 robots per day. Factory C costs $1500 per day to operate, and can produce 50 cars, 40 animals and 15 robots per day. This Christmas, FunToys is required to deliver 5000 cars, 3000 animals and 2500 robots. You are tasked with finding out what is the most cost-efficient way to meet the order. (In this question, please IGNORE integer requirements, i.e., just use fractional answers if/when they come up.) Let us start by trying to formulate this as a optimisation problem. First, write down what you want to minimize. Second, write down your decision variables. What are you actually choosing? Third, write your objective function in terms of your decision variables. Fourth, write down the constraints: what are the contractual requirements you need to fulfill. What other constraints are there? Write them down in terms of your decision variables. Summarize them nicely in a table. Write code to solve this optimisation problem. Report the optimal solution, and the value of the objective function at that solution. Interpret the solution: what do these numbers mean? (again, please ignore any integer requirements and just report fractional answers if they appear) What if we impose an additional constraint that FunToys only has 60 days to complete the order? (Note that we can run all three factories simultaneously). What happens now? Re-produce a new table summarizing the optimisation problem (including the existing and new constraints), and write R code to solve it. What is the new solution, and what is the objective function value? For the solution in (c), which of the constraints are binding, and which are non-binding? Using your solution in (c), print out the Shadow Prices. Interpret these values – make sure you can explain why each shadow price is zero or why it is positive/negative! Your answer from part (d) should also help you explain. (Note again, that we IGNORE integer requirements in this question, so your \\(X\\) variables can be fractional…) "],
["optimization-ii-integer-valued-optimization.html", "Chapter 13 Optimization II: Integer-valued Optimization", " Chapter 13 Optimization II: Integer-valued Optimization BT1101 In this chapter we will continue our discussion of linear optimization, but now with the added complexity of considering problems with integer-valued decision variables. The learning objectives for this chapter are: Readers should understand how and when integer constraints apply to optimisation problems. Readers should know how to use Linear-Program Relaxation (LP Relaxation) to gain insight into integer-constrained problems. Readers should understand how to solve integer-constrained optimisation problems. # Load the libraries we&#39;ll use in this chapter library(ggplot2) # for plotting library(lpSolve) # for solving linear optimization problems (&#39;linear programming&#39;) "],
["integer-valued-decision-variables.html", "13.1 Integer-valued decision variables", " 13.1 Integer-valued decision variables BT1101 In the previous chapter we learnt how to solve optimization problems by finding real-valued solutions to decision problems. Recall that in the Furniture Factory example, we discovered that the optimal solution was to produce 5.93 tables, 3.70 sofas, and 0 beds. But in real-life, we cannot produce fractional tables or sofas, so this becomes a problem, which we will learn to solve in this Chapter. In addition to decision variables like physical products (e.g., how many cars or tables to produce) that must be integer-valued, there are other examples of integer-valued optimization problems, such as: how many days to operate a factory; (perhaps operating costs are charged or rental contracts are executed for the whole day even if you only need some fraction of it) how many shifts to assign an employee; (another indivisible quantity) Note that we can have some decision variables that are real-valued, and others that must be integer-valued. In such a case, we call this a mixed-integer optimisation problem. We can also consider the special case of binary-valued decision variables (i.e., when your decision variable \\(X\\) can only take values of 1 or 0). These can be used to model binary (Yes/No) decisions: Should I invest or not? \\(X_i\\) = 1 or 0 Is worker Bob assigned to shift \\(j\\)? \\(X_j\\) = 1 or 0 And finally, we will also see how we can represent logical constraints using binary decision variables. This allows us to encode constraints like, “if A, then B”. "],
["from-real-valued-to-integer-solutions.html", "13.2 From real-valued to integer solutions", " 13.2 From real-valued to integer solutions BT1101 Let’s revisit the Furniture Factory example from the previous Chapter. Recall that the objective function, constraints, and solution are: \\[ X_1 = \\text{number of tables made} \\\\ X_2 = \\text{number of sofas made} \\\\ X_3 = \\text{number of beds made} \\\\ \\text{Maximize Profit} = 500 X_1 + 450 X_2 + 580 X_3 \\] Production Hour Budget Constraints: \\[ 8.4X_1 + 6X_2 + 9.6X_3 \\leq 72\\] Storage Space Constraints: \\[ 3X_1 + 6X_2 + 8X_3 \\leq 40\\] Demand Constraints: \\[ X_1 + \\qquad + \\qquad \\leq 8\\] Non-negativity Constraints: \\[ X_1 \\geq 0; X_2 \\geq 0; X_3 \\geq 0 \\] And, the optimal solution is: \\[X_1 = 5.93, X_2 = 3.70, X_3 = 0\\] Let us ignore the beds (\\(X_3\\)) for just a second, and plot the 3 constraints onto \\(X_1\\) (tables) on the horizontal axis and \\(X_2\\) (sofas) on the vertical axis (in the plane \\(X_3=0\\)). The optimal solution (\\(X_1 = 5.93, X_2 = 3.70, X_3 = 0\\)) is given in pink. Now, if we want to impose an integer constraint, such that all our decision variables (\\(X_1\\), \\(X_2\\), \\(X_3\\)) must be integer valued, then our feasible solutions change. They cannot take on just any value, but only values indicated by the yellow dots below: The first question that many people will ask is: Well, can we just round off the real-valued answer to the nearest solution? The answer is No, because in this case, the rounded off answer (\\(X_1=6, X_2=4\\)) is not a feasible solution (it lies outside the other constraints). Then a natural response would be: Can we then round down the answer, or round off the answer to the nearest feasible answer? Maybe. But the nearest feasible answer is not guaranteed to be optimal! Have a look at the following plot, where the profit at several chosen yellow dots (near the constraint boundaries) are shown: We can actually see that the nearest feasible answers (e.g., (5,4) and (6,3)) may not produce the optimal solution. In fact, the optimal solution in this plane (keeping \\(X_3 = 0\\)) is actually quite far away from (\\(X_1 = 5.93\\), \\(X_2 = 3.70\\)). It is (\\(X_1 = 7\\), \\(X_2 = 2\\))! And actually if we do solve the integer-valued optimization problem (see below), we will eventually discover that the proper integer solution to this problem is \\[ X_1 = 6, X_2 = 2, X_3 = 1 \\] which has a profit of $4480. Recall that in the real-valued version of this problem, we concluded that beds were not profitable enough to make (\\(X_3=0\\)). But adding the integer constraints may also change which constraints become binding, and which variables become relevant to the problem! Thus, suddenly the “not-so-profitable” Beds become profitable to make! In reality, solving integer-constrained optimization problems is a hard problem. In fact, in computer science jargon, this is one of a class of NP-Complete problems, which just means there exists no efficient (polynomial-time) method to find solutions to this problem. (Unlike in the simple linear optimization case where such methods do exist). This means that one approach to get the optimal solution is to brute-force search through every solution (the time-complexity of which grows very rapidly with the problem size); another popular approach to use heuristics or approximation algorithms to offer a best-guess at an answer. 13.2.1 LP-Relaxation We’ve seen that the optimal integer solution may not necessarily be near the optimal real-valued solution. But that doesn’t mean the real-valued solution is useless. In fact, the most common way to solve the integer optimisation problem is to FIRST solve the problem without the integer constraints. Solving the optimisation without the integer constraints is called Linear Program [LP] Relaxation, because you “relax” the integer constraints. Now, if you solve the LP relaxation, and the solution happens to be integer valued, then you can thank your lucky stars, because that solution is also the optimal solution to the integer problem. Think back to the example of Farmer Jean in the previous Chapter. In her case, the solution to her optimization problem was an integer number of plots of crops, (80, 120). Thus, even if we had to consider integer constraints, this solution is still valid. And in fact, it still remains optimal. Another collorary is that the objective of the optimal solution to the LP-relaxed problem (e.g., the maximum profit) is always better than or equal to the objective of the optimal integer-valued solution. But this will not often be the case that the solution to the LP relaxation will be integer-valued. Then, programs use the real-valued solution as a starting point to systematically search for the optimal integer solution (such as the “branch and bound” algorithm used by lpSolve). 13.2.2 Specifying Integer Constraints In order to tell lpSolve that we want to impose integer constraints, we simply add a new option int.vec = c(...), where we give a vector of integers. For example, if we want to specify in the furnitude example that \\(X_1, X_2, X_3\\) are all integers, we specify int.vec = c(1,2,3). (conversely, if only \\(X_2\\) was an integer, we would write int.vec=c(2).) lp.solution &lt;- lp(&quot;max&quot;, objective.fn, const.mat, const.dir, const.rhs, int.vec = c(1,2,3), compute.sens=FALSE) # note: the sensitivity analysis calculated by lpSolve::lp() # is inaccurate for integer-valued problems, so we won&#39;t use them. And that’s it. We can use the same functions lp.solution and lp.solution$solution to read out the optimal objective function value and the optimal values of the decision variables. Thus, for the furniture example above, we have: objective.fn &lt;- c(500, 450, 580) const.mat &lt;- matrix(c(8.4, 6, 9.6, 3, 6, 8, 1, 0, 0), ncol=3 , byrow=TRUE) const.dir &lt;- c(&quot;&lt;=&quot;, &quot;&lt;=&quot;, &quot;&lt;=&quot;) const.rhs &lt;- c(72, 40, 8) # solving model lp.solution &lt;- lp(&quot;max&quot;, objective.fn, const.mat, const.dir, const.rhs, int.vec = c(1,2,3), compute.sens=FALSE) # to print out the optimal objective function value lp.solution ## Success: the objective function is 4480 # to print out the values of the decision variables (X1, X2, X3) lp.solution$solution ## [1] 6 2 1 Lastly, one final point to note is that the sensitivity analyses calculated by lpSolve may not be accurate for integer-valued problems (i.e., the effect of varying the objective function coefficients and shadow prices may not be accurate), so we choose not to have complete.sens in the function call. Of course, since we are already experts in this from last Chapter, we could always change the objective function coefficients or constraint values manually and try to solve it to estimate a sensitivity analysis, but the inbuilt convenience function doesn’t do that for us. "],
["logical-constraints.html", "13.3 Logical Constraints", " 13.3 Logical Constraints BT1101 Logical constraints can also be formalised as integer constraints. For example, let’s say that \\(X_1\\) is whether I decide to buy a new house, and \\(X_2\\) is whether I decide to renovate the new house. We restrict both \\(X_1\\), \\(X_2\\) to be either \\(0\\) or \\(1\\). Obviously, I need to purchase the house before I can renovate it, so \\(X_1\\) is a pre-requisite for \\(X_2\\). Using logic, I can write that: \\[ \\text{if I choose } X_2, \\text{then I must choose } X_1 \\text{ as well.} \\] This logical constraint can be formalised as the following integer-valued constraint: \\[X_1 - X_2 \\geq 0\\] (In fact, this is not just an integer-valued constraint, but it is even stronger: the two variables are binary-valued). The intuition is that if \\(X_2\\) is 1, then this forces \\(X_1\\) to be 1 too. Let us check: \\(X_1 = 0\\), \\(X_2 = 0\\): Constraint: \\(0 - 0 = 0 \\geq 0\\) is satisfied. This is fine since I don’t purchase the house and I don’t renovate the house. \\(X_1 = 1\\), \\(X_2 = 0\\): Constraint: \\(1 - 0 = 1 \\geq 0\\) is satisfied. This is fine since I can choose to purchase the house without renovating it. \\(X_1 = 1\\), \\(X_2 = 1\\): Constraint: \\(1 - 1 = 0 \\geq 0\\) is satisfied. Again, this is fine since I can choose to both purchase and renovate the house. Most importantly: \\(X_1 = 0\\), \\(X_2 = 1\\): Constraint: \\(0 - 1 = -1 \\not \\geq 0\\). The constraint is not satisfied. This makes sense, because I cannot not purchase the house (\\(X_1=0\\)) but still choose to renovate it (\\(X_2=1\\)). Thus, we can check and see that this integer constraint correctly captures the intuition behind the logical constraint. In general, the following table summaries how some logical constraints can be represented using binary decision variables: Logical Condition Linear Constraint if \\(A\\) then \\(B\\) \\(B - A \\geq 0\\) if not \\(A\\) then \\(B\\) \\(B - (1 - A) \\geq 0\\), or \\(A + B \\geq 1\\) At most one of \\(A\\), \\(B\\) \\(A + B \\leq 1\\) At least one of \\(A\\), \\(B\\) \\(A + B \\geq 1\\) if (\\(A\\) or \\(C\\)) then \\(B\\) \\(B - (A+C) \\geq 0\\) if (\\(A\\) and \\(C\\)) then \\(B\\) \\(B - (A+C) \\geq -1\\), or \\((A+C) - B \\leq 1\\) if \\(A\\) then (\\(B\\) or \\(C\\)) \\((B+C) - A \\geq 0\\) if \\(A\\) then (\\(B\\) and \\(C\\)) \\((B+C) - 2A \\geq 0\\) 13.3.1 How to specify logical constraints Earlier we saw that to tell lpSolve that certain decision variables are integer-valued, we used an option int.vec = c(...). If we want to tell lpSolve that instead some decision variables are binary-valued, then we use: binary.vec = c(...). And that’s it! lpSolve does everything else for us. (In fact, it’s harder to ensure that you have written down the constraints correctly!) 13.3.2 Logical Constraints Example: Planning university courses Let’s see logical constraints in action in the following example. Natalie has decided to switch her career to data science, as she feels it has more prospects than her previous industry. She is eyeing a “Pico-masters (TM)” program at her local university, where she has to complete 40 units of courses to satisfy the pico-degree. (“Pico-masters” and “nano-masters” are fictional—at least, for now—but some online education platforms and universities like edX are offering “MicroMasters®” and other similar products.) The program offers the following courses, in STatistics, ProGramming, and Data Management, along with their associated costs and pre-requisites. The pre-requisites for each course must be fulfilled before students are allowed to take that course. In order to finish the PicoMasters, she needs to finish a specialization in one of the three tracks, which is fulfilled by completing the “Level 3” version of that course. Natalie has also indicated her personal interest in each course, in the following table. Course Units Pre-requisites Interest ST1 10 - 8 ST2 10 ST1 4 ST3 10 ST2 6 PG1 10 - 7 PG2a 10 PG1 5 PG2b 10 PG1 6 PG3 10 PG2a or PG2b 3 DM1 10 - 4 DM2 10 DM1 6 DM3 10 DM2 7 Imagine that her goal is to maximize her interest and satisfy the requirements of the degree, while also taking exactly 40 units (in order to keep costs low). Which classes should she take? First, let’s define our decision variables, by just using \\(X_1\\) to \\(X_{10}\\) to correspond to taking each of the ten courses, in the order in the table above. That is, \\(X_1\\) = Take ST1, \\(X_2\\) = Take ST2, \\(X_3\\) = Take ST3, \\(X_4\\) = Take PG1, \\(X_5\\) = Take PG2a, \\(X_6\\) = Take PG2b, \\(X_7\\) = Take PG3, \\(X_8\\) = Take DM1, \\(X_9\\) = Take DM2, \\(X_{10}\\) = Take DM3 Now let’s tackle the constraints one-by-one. Consider the following pre-requisite: Natalie needs to take ST1 before being allowed to take ST2. We can represent this using the following constraint: \\[ X_1 - X_2 \\geq 0 \\] You can easily check if this is correct by ‘testing’ it by subtituting values. If Natalie takes ST2 (i.e., \\(X_2 = 1\\)), then we have the following inequality: \\(X_1 - 1 \\geq 0\\). In order for this also to be true, we need \\(X_1\\) to also be 1. This means that Natalie taking ST2 (\\(X_2 = 1\\)) actually forces her to also have taken ST1 (\\(X_1 = 1\\)). Similarly, we can write out the pre-requisite constraints for the rest. Now, let’s consider one more case: Natalie needs to take either PG2a OR PG2b before being allowed to take PG3. One easy thing to try is to replace ST2 (\\(X_2\\)) in the previous inequality with PG3 (\\(X_7\\)), and replace ST1 (\\(X_1\\)) with (PG2a or PG2b), or (\\((X_5+X_6)\\)). So let’s try: \\[ (X_5 + X_6) - X_7 \\geq 0 \\] And actually this is the correct inequality! Let’s test this out. If Natalie takes PG3, then \\(X_7=1\\), and so we are left with: \\((X_5 + X_6) - 1 \\geq 0\\). This inequality will be satisfied if \\(X_5\\) is 1, or if \\(X_6\\) is 1, that is, if she takes either PG2a or PG2b. (Note that this will also be satisifed if BOTH \\(X_5\\) and \\(X_6\\) is 1 – there is nothing stopping her from taking both PG2a and PG2b!) And finally let’s consider the constraint that she needs to finish a specialization. That is, she needs to take either ST3 (\\(X_3\\)) or PG3 (\\(X_7\\)) or DM3 (\\(X_{10}\\)). This means that at least one of them needs to be 1. So we can just write: \\[ X_3 + X_7 + X_{10} \\geq 0 \\] Please make sure you understand how we got (or how we tested/verified) the above inequalities! Now, we can finally write out our optimization problem in a large table: Decision variables \\(X_1\\) = Take ST1, \\(X_2\\) = Take ST2, \\(X_3\\) = Take ST3, \\(X_4\\) = Take PG1, \\(X_5\\) = Take PG2a, \\(X_6\\) = Take PG2b, \\(X_7\\) = Take PG3, \\(X_8\\) = Take DM1, \\(X_9\\) = Take DM2, \\(X_{10}\\) = Take DM3 Maximize Interest = 8\\(X_1\\) + 4\\(X_2\\) + 6\\(X_3\\) + 7\\(X_4\\) + 5\\(X_5\\) + 6\\(X_6\\) + 3\\(X_7\\) + 4\\(X_8\\) + 6\\(X_9\\) + 7\\(X_{10}\\) Subject to Minimum Course Requirements (= for cost savings) 10 \\(X_1\\) + 10 \\(X_2\\) + 10 \\(X_3\\) + 10 \\(X_4\\) + 10 \\(X_5\\) + 10 \\(X_6\\) + 10 \\(X_7\\) + 10 \\(X_8\\) + 10 \\(X_9\\) + 10 \\(X_{10}\\) \\(=\\) 40 Pre-Requisite for ST2 \\(X_1 - X_2 \\geq 0\\) or 1 \\(X_1\\) + (-1) \\(X_2\\) + 0 \\(X_3\\) + 0 \\(X_4\\) + 0 \\(X_5\\) + 0 \\(X_6\\) + 0 \\(X_7\\) + 0 \\(X_8\\) + 0 \\(X_9\\) + 0 \\(X_{10}\\) \\(\\geq\\) 0 Pre-Requisite for ST3 \\(X_2 - X_3 \\geq 0\\) or 0 \\(X_1\\) + 1 \\(X_2\\) + -1 \\(X_3\\) + 0 \\(X_4\\) + 0 \\(X_5\\) + 0 \\(X_6\\) + 0 \\(X_7\\) + 0 \\(X_8\\) + 0 \\(X_9\\) + 0 \\(X_{10}\\) \\(\\geq\\) 0 Pre-Requisite for PG2a \\(X_4 - X_5 \\geq 0\\) or 0 \\(X_1\\) + 0 \\(X_2\\) + 0 \\(X_3\\) + 1 \\(X_4\\) + -1 \\(X_5\\) + 0 \\(X_6\\) + 0 \\(X_7\\) + 0 \\(X_8\\) + 0 \\(X_9\\) + 0 \\(X_{10}\\) \\(\\geq\\) 0 Pre-Requisite for PG2b \\(X_4 - X_6 \\geq 0\\) or 0 \\(X_1\\) + 0 \\(X_2\\) + 0 \\(X_3\\) + 1 \\(X_4\\) + 0 \\(X_5\\) + -1 \\(X_6\\) + 0 \\(X_7\\) + 0 \\(X_8\\) + 0 \\(X_9\\) + 0 \\(X_{10}\\) \\(\\geq\\) 0 Pre-Requisite for PG3 (PG2a OR PG2b) \\((X_5 + X_6) - X_7 \\geq 0\\) or 0 \\(X_1\\) + 0 \\(X_2\\) + 0 \\(X_3\\) + 0 \\(X_4\\) + 1 \\(X_5\\) + 1 \\(X_6\\) + -1 \\(X_7\\) + 0 \\(X_8\\) + 0 \\(X_9\\) + 0 \\(X_{10}\\) \\(\\geq\\) 0 Pre-Requisite for DM2 \\(X_8 - X_9 \\geq 0\\) or 0 \\(X_1\\) + 0 \\(X_2\\) + 0 \\(X_3\\) + 0 \\(X_4\\) + 0 \\(X_5\\) + 0 \\(X_6\\) + 0 \\(X_7\\) + 1 \\(X_8\\) + -1 \\(X_9\\) + 0 \\(X_{10}\\) \\(\\geq\\) 0 Pre-Requisite for DM3 \\(X_9 - X_{10} \\geq 0\\) or 0 \\(X_1\\) + 0 \\(X_2\\) + 0 \\(X_3\\) + 0 \\(X_4\\) + 0 \\(X_5\\) + 0 \\(X_6\\) + 0 \\(X_7\\) + 0 \\(X_8\\) + 1 \\(X_9\\) + -1 \\(X_{10}\\) \\(\\geq\\) 0 Complete Specialization \\(X_3 + X_7 + X_{10} \\geq 1\\) or 0 \\(X_1\\) + 0 \\(X_2\\) + 1 \\(X_3\\) + 0 \\(X_4\\) + 0 \\(X_5\\) + 0 \\(X_6\\) + 1 \\(X_7\\) + 0 \\(X_8\\) + 0 \\(X_9\\) + 1 \\(X_{10}\\) \\(\\geq\\) 1 Binary, Integer, Non-Negativity Constraints \\(X_1\\), to \\(X_{10}\\) all binary, integers and \\(\\geq 0\\). Notice that I’ve written out the inequalities into longer forms with explicit “0”s, so \\[X_1 - X_2 \\geq 0\\] becomes \\[X_1 + (-1) X_2 + 0 X_3 + 0 X_4 + 0 X_5 + 0 X_6 + 0 X_7 + 0 X_8 + 0 X_9 + 0 X_{10} \\geq 0\\] This makes it much easier to type this into code. This is the hardest part now, transfering this into code without making any mistakes! #defining parameters objective.fn &lt;- c(8, 4, 6, 7, 5, 6, 3, 4, 6, 7) const.mat &lt;- matrix(c(rep(10,10), rep(0,0), 1, -1, rep(0, 8), # ST2 rep(0,1), 1, -1, rep(0, 7), # ST3 rep(0,3), 1, -1, rep(0, 5), # PG2a rep(0,3), 1, 0, -1, rep(0, 4), # PG2b rep(0,4), 1, 1, -1, rep(0, 3), # PG3 rep(0,7), 1, -1, rep(0, 1), # DM2 rep(0,8), 1, -1, rep(0, 0), # DM3 rep(0,2), 1, rep(0,3), 1, rep(0,2), 1 # complete specialization ) , ncol=10 , byrow=TRUE) const.dir &lt;- c(&quot;=&quot;, rep(&quot;&gt;=&quot;, 8)) const.rhs &lt;- c(40, rep(0,7), 1) #solving model lp.solution &lt;- lp(&quot;max&quot;, objective.fn, const.mat, const.dir, const.rhs, binary.vec = c(1:10)) lp.solution$solution #decision variables values ## [1] 1 1 1 1 0 0 0 0 0 0 lp.solution ## Success: the objective function is 25 Thus, the final solution is \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(X_4\\) = 1, with the rest = 0. In order to maximize her interest while completing the degree, Natalie should choose to specialize in Statistics (taking ST1, ST2, ST3), and then also taking the first course in Programming (PG1). "],
["integer-optimization-summary.html", "13.4 Integer Optimization Summary", " 13.4 Integer Optimization Summary BT1101 In this lecture we’ve covered how to use linear optimisation to solved integer-valued optimisation problems, where some of the decision variables are constrained to be integers, or even constrained to be binary variables (which can be used to model yes/no decisions). We’ve also seen how to transform logical constraints into linear constraints using binary variables. And we’ve also seen worked examples of problems with 10+ decision variables – these problems scale up very fast! The output of these problems can be very useful in helping managers make optimal decisions based on the data, and is the whole objective behind prescriptive analytics. "],
["exercises-integer-optimization.html", "13.5 Exercises: Integer Optimization", " 13.5 Exercises: Integer Optimization BT1101 Q1 John is interested in buying ads to market his new startup. He sees the following options: Ad Cost per ad Reach Limits Radio Ad $100 500 40 Newspaper Ad $250 2000 10 Social Media Ad $50 300 80 The “limits” in the table above are imposed by each advertiser, so the Newspaper will only run a maximum of 10 Newspaper Ads. Reach is an estimated number of people that the ad will reach, per ad that John buys (e.g. if he buys 1 Radio ad, it will reach 500 people. If he buys 2, it will reach 1000 people.) Q1a) Identify the decision variables, objective function and constraints. Write out the optimization problem in a table. Q1b) Write R code to solve this problem. What is the optimal solution, and what is the value of the objective function that this optimal solution? "]
]
