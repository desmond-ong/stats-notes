<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.4 Ordinary Least Squares Regression | Some notes on Statistics and Data Analytics</title>
  <meta name="description" content="A collection of statistics notes useful for data analytics." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="5.4 Ordinary Least Squares Regression | Some notes on Statistics and Data Analytics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A collection of statistics notes useful for data analytics." />
  <meta name="github-repo" content="desmond-ong/stats-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.4 Ordinary Least Squares Regression | Some notes on Statistics and Data Analytics" />
  
  <meta name="twitter:description" content="A collection of statistics notes useful for data analytics." />
  

<meta name="author" content="Desmond Ong" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="running-a-regression.html"/>
<link rel="next" href="not-done-interpreting-the-output-of-a-regression-model.html"/>
<script src="assets/jquery-2.2.3/jquery.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">My Statistics Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="outline-of-notes.html"><a href="outline-of-notes.html"><i class="fa fa-check"></i>Outline of notes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="not-done-getting-started.html"><a href="not-done-getting-started.html"><i class="fa fa-check"></i>[Not Done:] Getting Started</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> [Not Done:] Introduction</a></li>
<li class="chapter" data-level="2" data-path="not-done-descriptive-statistics.html"><a href="not-done-descriptive-statistics.html"><i class="fa fa-check"></i><b>2</b> [Not Done:] Descriptive Statistics</a></li>
<li class="chapter" data-level="3" data-path="not-done-handling-data.html"><a href="not-done-handling-data.html"><i class="fa fa-check"></i><b>3</b> [Not Done:] Handling Data</a></li>
<li class="chapter" data-level="4" data-path="not-done-data-visualization.html"><a href="not-done-data-visualization.html"><i class="fa fa-check"></i><b>4</b> [Not Done:] Data Visualization</a></li>
<li class="chapter" data-level="5" data-path="the-linear-model-i-linear-regression.html"><a href="the-linear-model-i-linear-regression.html"><i class="fa fa-check"></i><b>5</b> The Linear Model I: Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="not-done-yet-introduction-to-the-linear-model.html"><a href="not-done-yet-introduction-to-the-linear-model.html"><i class="fa fa-check"></i><b>5.1</b> [Not Done Yet] Introduction to the Linear Model</a></li>
<li class="chapter" data-level="5.2" data-path="basics-of-linear-regression.html"><a href="basics-of-linear-regression.html"><i class="fa fa-check"></i><b>5.2</b> Basics of Linear Regression</a></li>
<li class="chapter" data-level="5.3" data-path="running-a-regression.html"><a href="running-a-regression.html"><i class="fa fa-check"></i><b>5.3</b> Running a regression</a></li>
<li class="chapter" data-level="5.4" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html"><i class="fa fa-check"></i><b>5.4</b> Ordinary Least Squares Regression</a><ul>
<li class="chapter" data-level="5.4.1" data-path="ordinary-least-squares-regression.html"><a href="ordinary-least-squares-regression.html#ordinary-least-squares-derivation"><i class="fa fa-check"></i><b>5.4.1</b> Ordinary Least Squares Derivation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="not-done-interpreting-the-output-of-a-regression-model.html"><a href="not-done-interpreting-the-output-of-a-regression-model.html"><i class="fa fa-check"></i><b>5.5</b> [Not Done:] Interpreting the output of a regression model</a><ul>
<li class="chapter" data-level="5.5.1" data-path="not-done-interpreting-the-output-of-a-regression-model.html"><a href="not-done-interpreting-the-output-of-a-regression-model.html#not-done-the-coefficient-table"><i class="fa fa-check"></i><b>5.5.1</b> [Not Done:] The coefficient table</a></li>
<li class="chapter" data-level="5.5.2" data-path="not-done-interpreting-the-output-of-a-regression-model.html"><a href="not-done-interpreting-the-output-of-a-regression-model.html#not-done-goodness-of-fit-statistics"><i class="fa fa-check"></i><b>5.5.2</b> [Not Done:] Goodness-of-fit statistics</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="not-done-assumptions-behind-linear-regression.html"><a href="not-done-assumptions-behind-linear-regression.html"><i class="fa fa-check"></i><b>5.6</b> [Not Done:] Assumptions behind Linear Regression</a></li>
<li class="chapter" data-level="5.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>5.7</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="5.8" data-path="standardized-coefficients.html"><a href="standardized-coefficients.html"><i class="fa fa-check"></i><b>5.8</b> Standardized Coefficients</a></li>
<li class="chapter" data-level="5.9" data-path="categorical-independent-variables.html"><a href="categorical-independent-variables.html"><i class="fa fa-check"></i><b>5.9</b> Categorical Independent Variables</a><ul>
<li class="chapter" data-level="5.9.1" data-path="categorical-independent-variables.html"><a href="categorical-independent-variables.html#dummy-coding"><i class="fa fa-check"></i><b>5.9.1</b> Dummy Coding</a></li>
<li class="chapter" data-level="5.9.2" data-path="categorical-independent-variables.html"><a href="categorical-independent-variables.html#dummy-coding-with-3-levels"><i class="fa fa-check"></i><b>5.9.2</b> Dummy Coding with 3 levels</a></li>
<li class="chapter" data-level="5.9.3" data-path="categorical-independent-variables.html"><a href="categorical-independent-variables.html#the-reference-group"><i class="fa fa-check"></i><b>5.9.3</b> The Reference Group</a></li>
<li class="chapter" data-level="5.9.4" data-path="categorical-independent-variables.html"><a href="categorical-independent-variables.html#interpreting-categorical-and-continuous-independent-variables"><i class="fa fa-check"></i><b>5.9.4</b> Interpreting categorical and continuous independent variables</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="todo-discuss-transformations.html"><a href="todo-discuss-transformations.html"><i class="fa fa-check"></i><b>5.10</b> [todo:] Discuss Transformations?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="not-done-the-linear-model-ii-logistic-regression.html"><a href="not-done-the-linear-model-ii-logistic-regression.html"><i class="fa fa-check"></i><b>6</b> [Not Done:] The Linear Model II: Logistic Regression</a></li>
<li class="chapter" data-level="7" data-path="not-done-the-linear-model-iii-interactions.html"><a href="not-done-the-linear-model-iii-interactions.html"><i class="fa fa-check"></i><b>7</b> [Not Done:] The Linear Model III: Interactions</a></li>
<li class="chapter" data-level="8" data-path="not-done-the-linear-model-iv-model-selection.html"><a href="not-done-the-linear-model-iv-model-selection.html"><i class="fa fa-check"></i><b>8</b> [Not Done:] The Linear Model IV: Model Selection</a></li>
<li class="chapter" data-level="9" data-path="not-done-the-linear-model-v-mixed-effects-linear-models.html"><a href="not-done-the-linear-model-v-mixed-effects-linear-models.html"><i class="fa fa-check"></i><b>9</b> [Not Done:] The Linear Model V: Mixed Effects Linear Models</a></li>
<li class="chapter" data-level="10" data-path="not-done-data-mining.html"><a href="not-done-data-mining.html"><i class="fa fa-check"></i><b>10</b> [Not Done:] Data Mining</a></li>
<li class="chapter" data-level="11" data-path="not-done-simulations.html"><a href="not-done-simulations.html"><i class="fa fa-check"></i><b>11</b> [Not Done:] Simulations</a></li>
<li class="chapter" data-level="12" data-path="optimization-i-linear-optimization.html"><a href="optimization-i-linear-optimization.html"><i class="fa fa-check"></i><b>12</b> Optimization I: Linear Optimization</a><ul>
<li class="chapter" data-level="12.1" data-path="what-is-linear-optimization.html"><a href="what-is-linear-optimization.html"><i class="fa fa-check"></i><b>12.1</b> What is Linear Optimization</a></li>
<li class="chapter" data-level="12.2" data-path="objective-functions-decision-variables.html"><a href="objective-functions-decision-variables.html"><i class="fa fa-check"></i><b>12.2</b> Objective Functions &amp; Decision Variables</a><ul>
<li class="chapter" data-level="" data-path="objective-functions-decision-variables.html"><a href="objective-functions-decision-variables.html#jean-the-farmer-objective-function"><i class="fa fa-check"></i>Jean the Farmer: Objective Function</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="constraints.html"><a href="constraints.html"><i class="fa fa-check"></i><b>12.3</b> Constraints</a><ul>
<li class="chapter" data-level="" data-path="constraints.html"><a href="constraints.html#jean-the-farmer-constraints"><i class="fa fa-check"></i>Jean the Farmer: Constraints</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="solving-the-optimization-model.html"><a href="solving-the-optimization-model.html"><i class="fa fa-check"></i><b>12.4</b> Solving the Optimization Model</a><ul>
<li class="chapter" data-level="12.4.1" data-path="solving-the-optimization-model.html"><a href="solving-the-optimization-model.html#writing-out-the-optimization-model"><i class="fa fa-check"></i><b>12.4.1</b> Writing out the Optimization model</a></li>
<li class="chapter" data-level="12.4.2" data-path="solving-the-optimization-model.html"><a href="solving-the-optimization-model.html#solving-an-optimization-problem-graphically"><i class="fa fa-check"></i><b>12.4.2</b> Solving an optimization problem graphically</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="using-r-to-solve-linear-optimization.html"><a href="using-r-to-solve-linear-optimization.html"><i class="fa fa-check"></i><b>12.5</b> Using R to solve Linear Optimization</a></li>
<li class="chapter" data-level="12.6" data-path="sensitivity-analysis.html"><a href="sensitivity-analysis.html"><i class="fa fa-check"></i><b>12.6</b> Sensitivity Analysis</a><ul>
<li class="chapter" data-level="" data-path="sensitivity-analysis.html"><a href="sensitivity-analysis.html#varying-objective-function-coefficients"><i class="fa fa-check"></i>Varying objective function coefficients</a></li>
<li class="chapter" data-level="" data-path="sensitivity-analysis.html"><a href="sensitivity-analysis.html#varying-constraint-values-shadow-prices"><i class="fa fa-check"></i>Varying Constraint Values (Shadow Prices)</a></li>
<li class="chapter" data-level="" data-path="sensitivity-analysis.html"><a href="sensitivity-analysis.html#binding-vs-non-binding-constraints"><i class="fa fa-check"></i>Binding vs non-binding constraints</a></li>
<li class="chapter" data-level="" data-path="sensitivity-analysis.html"><a href="sensitivity-analysis.html#summarizing-sensitivity-analyses"><i class="fa fa-check"></i>Summarizing sensitivity analyses</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>12.7</b> Summary</a></li>
<li class="chapter" data-level="12.8" data-path="not-done-exercises.html"><a href="not-done-exercises.html"><i class="fa fa-check"></i><b>12.8</b> [NOT DONE] Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="optimization-ii-integer-valued-optimization.html"><a href="optimization-ii-integer-valued-optimization.html"><i class="fa fa-check"></i><b>13</b> Optimization II: Integer-valued Optimization</a><ul>
<li class="chapter" data-level="13.1" data-path="integer-valued-decision-variables.html"><a href="integer-valued-decision-variables.html"><i class="fa fa-check"></i><b>13.1</b> Integer-valued decision variables</a></li>
<li class="chapter" data-level="13.2" data-path="not-done-from-real-valued-to-integer-solutions.html"><a href="not-done-from-real-valued-to-integer-solutions.html"><i class="fa fa-check"></i><b>13.2</b> [NOT DONE] From real-valued to integer solutions</a></li>
<li class="chapter" data-level="13.3" data-path="logical-constraints.html"><a href="logical-constraints.html"><i class="fa fa-check"></i><b>13.3</b> Logical Constraints</a><ul>
<li class="chapter" data-level="13.3.1" data-path="logical-constraints.html"><a href="logical-constraints.html#logical-constraints-example-planning-university-courses"><i class="fa fa-check"></i><b>13.3.1</b> Logical Constraints Example: Planning university courses</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>13.4</b> Summary</a></li>
<li class="chapter" data-level="13.5" data-path="not-done-exercises-1.html"><a href="not-done-exercises-1.html"><i class="fa fa-check"></i><b>13.5</b> [NOT DONE] Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="not-done-book-summary.html"><a href="not-done-book-summary.html"><i class="fa fa-check"></i><b>14</b> [Not Done:] Book Summary ??</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Some notes on Statistics and Data Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ordinary-least-squares-regression" class="section level2">
<h2><span class="header-section-number">5.4</span> Ordinary Least Squares Regression</h2>
<p>Let’s take a quick peek behind what the model is doing, and we’ll discuss the formulation of <strong>Ordinary Least Squares</strong> regression.</p>
<p>Let’s assume that the “true” model is such that <span class="math inline">\(Y\)</span> is <span class="math inline">\(b_0\)</span> + <span class="math inline">\(b_1\)</span>*<span class="math inline">\(X\)</span> plus some “errors”, which could be due to other, unmeasured factors, or maybe just random noise.</p>
<p><span class="math display">\[ \text{``True&quot; model: } Y = b_0 + b_1 X + \epsilon \]</span></p>
<p>Within a linear regression, we are making the assumption that the errors are Normally distributed around zero with some variance. (So <span class="math inline">\(\epsilon \sim \mathscr{N}(0, \sigma)\)</span>).</p>
<p>Since we don’t know the “true” <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, we can only choose <span class="math inline">\(\hat{b_0}\)</span> and <span class="math inline">\(\hat{b_1}\)</span>; using this we can compute the prediction of our model, <span class="math inline">\(\hat{Y}\)</span>. We want our <span class="math inline">\(\hat{b_0}\)</span> and <span class="math inline">\(\hat{b_1}\)</span> to be as close to the “true” <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, which will also make our predictions <span class="math inline">\(\hat{Y}\)</span> as close to the actual <span class="math inline">\(Y\)</span>.</p>
<p>To do this, we define the Residual or <strong>Residual Error</strong> of our model. For the <span class="math inline">\(i\)</span>-th data point, the residual is the difference between the actual <span class="math inline">\(Y_i\)</span> and our model predicted, <span class="math inline">\(\hat{Y_i}\)</span>.</p>
<p><span class="math display">\[ \text{Residual Error: } e_i = Y_i - \hat{Y_i} \]</span></p>
<p>Here’s an illustration. Let’s say I start off just by drawing a green line through the origin with some upward slope.</p>
<p><img src="06-a-regression_files/figure-html/lmi-ols1-1.png" width="336" /></p>
<p>Here, the red lines illustrate the residual error; the difference between the actual value and our prediction. And to make our model better, we want to minimise the red bars.
Some red bars are lower, some are higher, so let’s pivot the slope upwards.</p>
<p><img src="06-a-regression_files/figure-html/lmi-ols2-1.png" width="336" /></p>
<p>Now, we have this yellow line. It looks better, overall the bars are smaller. Now we note that all the red bars are below, so instead of pivoting, let’s move the whole line down.</p>
<p><img src="06-a-regression_files/figure-html/lmi-ols3-1.png" width="336" /></p>
<p>And finally we get the blue line here, which is the best solution to minimising the red bars. We want to minimise the residuals. How is this done?</p>
<div id="ordinary-least-squares-derivation" class="section level3">
<h3><span class="header-section-number">5.4.1</span> Ordinary Least Squares Derivation</h3>
<p>The residuals can be positive or negative, so if we simply add the residuals up we might be cancelling out some of them. So instead of minimising the sum of the residuals, we usually choose to square the residuals and minimise the sum of squares of the residuals. (Mathematically it becomes easier to work with the square than the absolute value).</p>
<p>So here, we have the Ordinary Least Squares Regression, where the goal is to choose <span class="math inline">\(\hat{b_0}\)</span> and <span class="math inline">\(\hat{b_1}\)</span> to minimise the sum of squares of the residuals <span class="math inline">\(\sum_{i} e_i^2 = \sum_i \left( Y_i - \hat{Y_i} \right)^2\)</span>.</p>
<p>We can do this by taking the partial derivative with respect to <span class="math inline">\(\hat{b_0}\)</span> and <span class="math inline">\(\hat{b_1}\)</span>, and setting them both to 0. First, we define the following variables to simplify notation:</p>
<p><span class="math display">\[\begin{align}
\text{Define } \bar{Y} &amp;\equiv \frac{1}{n}\sum_i^n Y_i \\
\text{Define } \bar{X} &amp;\equiv \frac{1}{n}\sum_i^n X_i \\
\text{Define } Z &amp;\equiv \sum_i \left( Y_i - \hat{Y_i} \right)^2 \\
&amp;= \left( Y_i - \hat{b_0} - \hat{b_1} X \right)^2 \\
\end{align}\]</span></p>
<p>Then we take the partial derivative with respect to <span class="math inline">\(\hat{b_0}\)</span>, solve for this <span class="math inline">\(\hat{b_0}\)</span>, then substitute it into the partial deriative with respect to <span class="math inline">\(\hat{b_1}\)</span>:
<span class="math display">\[\begin{align}
\text{Partial deriative w.r.t. } \hat{b_0} : \; \; 
\frac{\partial Z}{\partial \hat{b_0}} &amp;= \sum_i^n -2 \left(Y_i - \hat{b_0} - \hat{b_1}X_i \right) \\
\text{Setting the derivative to 0 and solving, we have: } \; \;
\hat{b_0} &amp;= \frac{1}{n}\sum_i^n Y_i - \frac{1}{n}\sum_i^n\hat{b_1}X_i \\
\implies \hat{b_0} &amp;= \bar{Y} - \hat{b_1} \bar{X} \\
\text{Partial deriative w.r.t. } \hat{b_1} : \; \;\frac{\partial Z}{\partial \hat{b_1}} &amp;= \sum_i^n  -2X_i \left( Y_i - \hat{b_0} - \hat{b_1}X_i \right) 
\end{align}\]</span>
<span class="math display">\[\begin{align}
\text{Setting the derivative to 0 and substituting $\hat{b_1}$, we have: } &amp; \\
\sum_i^n X_i Y_i - \sum_i^n (\bar{Y}-\hat{b_1}\bar{X})X_i - \sum_i^n\hat{b_1}X_i^2 &amp;= 0 \\
\sum_i^n X_i Y_i - \bar{Y} \sum_i^n X_i + \hat{b_1} \left(\bar{X} \sum_i^n X_i - \sum_i^n X_i^2 \right) &amp;= 0 \\
\hat{b_1} &amp;= \frac{\sum_i^n X_i Y_i - \bar{Y}\sum_i^n X_i }{ \sum_i^n X_i^2 - \bar{X} \sum_i^n X_i } \\ &amp;= \frac{\sum_i^n X_i Y_i - n\bar{X}\bar{Y}}{ \sum_i^n X_i^2 - n\bar{X}^2} \\
\text{simplifying: } \; \; \hat{b_1} &amp;= \frac{\sum_i^n (X_i - \bar{X})(Y_i - \bar{Y})}{ \sum_i^n (X_i - \bar{X})^2 }
\end{align}\]</span></p>
<p>And we end up with the final OLS solution:</p>
<p><span class="math display">\[\begin{align}
\hat{b_0} &amp;= \bar{Y} - \hat{b_1} \bar{X} \\
\hat{b_1} &amp;= \frac{\sum_i^n (X_i - \bar{X})(Y_i - \bar{Y})}{ \sum_i^n (X_i - \bar{X})^2 } = \frac{Cov(X,Y)}{Var(X)}
\end{align}\]</span></p>
<p>The good news is that <span class="math inline">\(R\)</span> already does this for you. Let’s check this solution with the <code>lm()</code> model we fit on the previous page!</p>
<p>Let’s do <span class="math inline">\(\hat{b_1}\)</span> first: in R, we can calculate the covariance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and divide that by the variance of <span class="math inline">\(X\)</span>, and save that into b1-hat: for this dataset we get 2.10.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1">b1hat =<span class="st"> </span><span class="kw">cov</span>(df1<span class="op">$</span>x, df1<span class="op">$</span>y) <span class="op">/</span><span class="st"> </span><span class="kw">var</span>(df1<span class="op">$</span>x)</a>
<a class="sourceLine" id="cb7-2" data-line-number="2">b1hat</a></code></pre></div>
<pre><code>## [1] 2.103757</code></pre>
<p>Following the equation for <span class="math inline">\(\hat{b_0}\)</span>, we can take the mean of <span class="math inline">\(Y\)</span>, and subtract <span class="math inline">\(\hat{b_1}\)</span> times the mean of <span class="math inline">\(X\)</span>, and we get -2.26.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1">b0hat =<span class="st"> </span><span class="kw">mean</span>(df1<span class="op">$</span>y) <span class="op">-</span><span class="st"> </span>b1hat <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(df1<span class="op">$</span>x)</a>
<a class="sourceLine" id="cb9-2" data-line-number="2">b0hat</a></code></pre></div>
<pre><code>## [1] -2.261167</code></pre>
<p>Finally let’s go back to our regression output table, which we can summon using <code>summary(...)$coeff</code>.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1">fit1 &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>x, df1)</a>
<a class="sourceLine" id="cb11-2" data-line-number="2"><span class="co"># verifying the OLS solution</span></a>
<a class="sourceLine" id="cb11-3" data-line-number="3"><span class="kw">summary</span>(fit1)<span class="op">$</span>coeff</a></code></pre></div>
<pre><code>##              Estimate Std. Error   t value    Pr(&gt;|t|)
## (Intercept) -2.261167 0.46170984 -4.897376 8.50721e-04
## x            2.103757 0.07804321 26.956313 6.44250e-10</code></pre>
<p>We can see that the Estimate of the Intercept, i.e., <span class="math inline">\(\hat{b_0}\)</span>, is -2.26, and the Estimate of the Coefficient on <span class="math inline">\(X\)</span>, i.e., <span class="math inline">\(\hat{b_1}\)</span>, is 2.10. They agree exactly! Excellent. So our <code>lm()</code> is really doing OLS regression.</p>
<p>Again, since <span class="math inline">\(R\)</span> does all the calculations for you, it’s not necessary to know how to derive the OLS solutions (especially with more than 1 <span class="math inline">\(X\)</span>), but it is handy to know the intuition behind it, especially when we get to more complicated regression.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="running-a-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="not-done-interpreting-the-output-of-a-regression-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
